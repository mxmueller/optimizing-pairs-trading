{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.analysis.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "    \n",
    "    symbols = price_matrix.columns.tolist()\n",
    "    \n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "    \n",
    "    return price_matrix, symbols"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_pairs(cointegrated_pairs):\n",
    "    return cointegrated_pairs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import toml\n",
    "\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "price_matrix, symbols = load_and_prepare_data(config['data']['raw_data_path'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_returns_and_spreads(price_matrix, cointegrated_pairs):\n",
    "    returns = price_matrix.pct_change().dropna()\n",
    "    \n",
    "    pairs = generate_pairs(cointegrated_pairs)\n",
    "    \n",
    "    spreads = pd.DataFrame(index=returns.index)\n",
    "    for s1, s2 in pairs:\n",
    "        spreads[f'{s1}_{s2}_spread'] = returns[s1] - returns[s2]\n",
    "        \n",
    "    return returns, spreads"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_ml_data(returns, spreads, train_period, test_period, lookback=3):\n",
    "    ml_datasets = {}\n",
    "    \n",
    "    for spread_col in spreads.columns:\n",
    "        sym1, sym2 = spread_col.replace('_spread', '').split('_')\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            f'{sym1}_return': returns[sym1],\n",
    "            f'{sym2}_return': returns[sym2]\n",
    "        })\n",
    "        \n",
    "        for t in range(1, lookback+1):\n",
    "            df[f'{sym1}_return_t-{t}'] = df[f'{sym1}_return'].shift(t)\n",
    "            df[f'{sym2}_return_t-{t}'] = df[f'{sym2}_return'].shift(t)\n",
    "        \n",
    "        spread_next_day = (df[f'{sym1}_return'] - df[f'{sym2}_return']).shift(-1)\n",
    "        df['target'] = np.where(spread_next_day > 0, 1, 0)\n",
    "        \n",
    "        feature_cols = [col for col in df.columns if 't-' in col]\n",
    "        features = df[feature_cols].copy()\n",
    "        \n",
    "        clean_idx = features.dropna().index\n",
    "        features = features.loc[clean_idx]\n",
    "        target = df.loc[clean_idx, 'target']\n",
    "    \n",
    "        train_mask = (features.index >= train_period['start']) & (features.index < train_period['end'])\n",
    "        test_mask = (features.index >= test_period['start']) & (features.index < test_period['end'])\n",
    "        \n",
    "        ml_datasets[f'{sym1}_{sym2}'] = {\n",
    "            'X_train': features[train_mask],\n",
    "            'X_test': features[test_mask],\n",
    "            'y_train': target[train_mask],\n",
    "            'y_test': target[test_mask]\n",
    "        }\n",
    "        \n",
    "    return ml_datasets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_evaluate_models(ml_datasets, coint_results):\n",
    "   results = {}\n",
    "   \n",
    "   for pair, data in tqdm(ml_datasets.items(), desc=\"Training models\"):\n",
    "       sym1, sym2 = pair.split('_')\n",
    "       \n",
    "       # Debug: Klassen-Balance\n",
    "       print(f\"\\nAnalyzing pair: {pair}\")\n",
    "       print(\"Class distribution (train):\", np.bincount(data['y_train']) / len(data['y_train']))\n",
    "       print(\"Class distribution (test):\", np.bincount(data['y_test']) / len(data['y_test']))\n",
    "       \n",
    "       p_value = coint_results[\n",
    "           ((coint_results['symbol1'] == sym1) & (coint_results['symbol2'] == sym2)) |\n",
    "           ((coint_results['symbol1'] == sym2) & (coint_results['symbol2'] == sym1))\n",
    "       ]['p_value'].iloc[0]\n",
    "       \n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(data['X_train'])\n",
    "       X_test_scaled = scaler.transform(data['X_test'])\n",
    "       \n",
    "       # Angepasste Parameter\n",
    "       svm = SVC(\n",
    "           kernel='rbf',\n",
    "           C=0.1,  # Reduzierter C-Wert\n",
    "           class_weight='balanced',\n",
    "           random_state=42\n",
    "       )\n",
    "       svm.fit(X_train_scaled, data['y_train'])\n",
    "       predictions = svm.predict(X_test_scaled)\n",
    "       \n",
    "       # Debug: Predictions Verteilung\n",
    "       print(\"\\nPrediction distribution:\", np.bincount(predictions) / len(predictions))\n",
    "       \n",
    "       f1 = f1_score(data['y_test'], predictions)\n",
    "       weighted_score = 1 * f1 + 0 * (1 - p_value)\n",
    "       \n",
    "       results[pair] = {\n",
    "           'model': svm,\n",
    "           'scaler': scaler,\n",
    "           'accuracy': accuracy_score(data['y_test'], predictions),\n",
    "           'precision': precision_score(data['y_test'], predictions),\n",
    "           'recall': recall_score(data['y_test'], predictions),\n",
    "           'f1': f1,\n",
    "           'p_value': p_value,\n",
    "           'weighted_score': weighted_score\n",
    "       }\n",
    "   \n",
    "   return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_matrix, pvalue_matrix, cointegrated_pairs, coint_results = analyze_pairs(price_matrix)\n",
    "\n",
    "returns, spreads = calculate_returns_and_spreads(price_matrix, cointegrated_pairs)\n",
    "\n",
    "train_period = get_training_period()\n",
    "test_period = get_test_period()\n",
    "\n",
    "ml_datasets = prepare_ml_data(returns, spreads, train_period, test_period)\n",
    "\n",
    "model_results = train_evaluate_models(ml_datasets, coint_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T14:21:34.829773Z",
     "start_time": "2025-02-07T14:21:34.810010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_df = pd.DataFrame([\n",
    "   {\n",
    "       'pair': pair,\n",
    "       'accuracy': metrics['accuracy'],\n",
    "       'precision': metrics['precision'],\n",
    "       'recall': metrics['recall'],\n",
    "       'f1': metrics['f1'],\n",
    "       'p_value': metrics['p_value'],\n",
    "       'weighted_score': metrics['weighted_score']\n",
    "   }\n",
    "   for pair, metrics in model_results.items()\n",
    "])\n",
    "\n",
    "top_20 = results_df.sort_values('weighted_score', ascending=False).head(20)\n",
    "print(top_20[['pair', 'f1', 'p_value', 'weighted_score', 'accuracy', 'precision', 'recall']])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pair        f1   p_value  weighted_score  accuracy  precision  \\\n",
      "22     AMAT_MU  0.697051  0.039682        0.697051  0.551587   0.555556   \n",
      "220   NFLX_TTD  0.684211  0.030740        0.684211  0.523810   0.528455   \n",
      "14     ADP_KHC  0.664820  0.013673        0.664820  0.519841   0.519481   \n",
      "167   KDP_LRCX  0.661017  0.039539        0.661017  0.523810   0.522321   \n",
      "176   KDP_SNPS  0.660920  0.024321        0.660920  0.531746   0.529954   \n",
      "223   NXPI_XEL  0.659218  0.048490        0.659218  0.515873   0.531532   \n",
      "165   ISRG_TTD  0.657143  0.003670        0.657143  0.523810   0.511111   \n",
      "199  LRCX_NXPI  0.656977  0.001418        0.656977  0.531746   0.520737   \n",
      "196   LIN_PANW  0.656000  0.027741        0.656000  0.488095   0.488095   \n",
      "65    CSX_DXCM  0.655914  0.043234        0.655914  0.492063   0.489960   \n",
      "11    ADI_PANW  0.652291  0.009833        0.652291  0.488095   0.485944   \n",
      "80    CSX_LRCX  0.648352  0.014721        0.648352  0.492063   0.489627   \n",
      "112   EA_GOOGL  0.647230  0.009857        0.647230  0.519841   0.497758   \n",
      "113     EA_HON  0.644578  0.004449        0.644578  0.531746   0.529703   \n",
      "19   ADSK_TTWO  0.640000  0.026662        0.640000  0.535714   0.544503   \n",
      "59   CSGP_MCHP  0.637838  0.048716        0.637838  0.468254   0.470120   \n",
      "183   KHC_MSTR  0.636364  0.042634        0.636364  0.492063   0.500000   \n",
      "100    CSX_TTD  0.634286  0.025944        0.634286  0.492063   0.482609   \n",
      "230  PAYX_TSLA  0.634146  0.033774        0.634146  0.523810   0.522613   \n",
      "23   AMAT_NXPI  0.632836  0.000091        0.632836  0.511905   0.519608   \n",
      "\n",
      "       recall  \n",
      "22   0.935252  \n",
      "220  0.970149  \n",
      "14   0.923077  \n",
      "167  0.900000  \n",
      "176  0.877863  \n",
      "223  0.867647  \n",
      "165  0.920000  \n",
      "199  0.889764  \n",
      "196  1.000000  \n",
      "65   0.991870  \n",
      "11   0.991803  \n",
      "80   0.959350  \n",
      "112  0.925000  \n",
      "113  0.823077  \n",
      "19   0.776119  \n",
      "59   0.991597  \n",
      "183  0.875000  \n",
      "100  0.925000  \n",
      "230  0.806202  \n",
      "23   0.809160  \n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

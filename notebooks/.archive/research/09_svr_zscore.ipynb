{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:59:26.189854Z",
     "start_time": "2025-02-09T13:59:26.185402Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.analysis.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:59:26.198788Z",
     "start_time": "2025-02-09T13:59:26.196074Z"
    }
   },
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "    volume_matrix = df.pivot(index='date', columns='symbol', values='volume')\n",
    "    \n",
    "    symbols = price_matrix.columns.tolist()\n",
    "    \n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "    \n",
    "    return price_matrix, volume_matrix, symbols"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:59:26.217747Z",
     "start_time": "2025-02-09T13:59:26.216061Z"
    }
   },
   "source": [
    "def generate_pairs(cointegrated_pairs):\n",
    "    return cointegrated_pairs"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:59:26.281214Z",
     "start_time": "2025-02-09T13:59:26.235068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import toml\n",
    "\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "price_matrix, volume_matrix, symbols = load_and_prepare_data(config['data']['raw_data_path'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 2021-02-02 00:00:00 to 2025-01-01 00:00:00\n",
      "Total symbols: 94\n",
      "Total trading days: 985\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:59:26.301957Z",
     "start_time": "2025-02-09T13:59:26.299616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_returns_and_spreads(price_matrix, cointegrated_pairs):\n",
    "    returns = price_matrix.pct_change().dropna()\n",
    "    \n",
    "    pairs = generate_pairs(cointegrated_pairs)\n",
    "    \n",
    "    spreads = pd.DataFrame(index=returns.index)\n",
    "    for s1, s2 in pairs:\n",
    "        spreads[f'{s1}_{s2}_spread'] = returns[s1] - returns[s2]\n",
    "        \n",
    "    return returns, spreads"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:59:26.323907Z",
     "start_time": "2025-02-09T13:59:26.319596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_ml_data(price_matrix, volume_matrix, spreads, train_period, test_period, lookback=5):\n",
    "   ml_datasets = {}\n",
    "   returns = price_matrix.pct_change().dropna()\n",
    "   \n",
    "   for spread_col in spreads.columns:\n",
    "       sym1, sym2 = spread_col.replace('_spread', '').split('_')\n",
    "       \n",
    "       df = pd.DataFrame({\n",
    "           f'{sym1}_volume': volume_matrix[sym1],\n",
    "           f'{sym2}_volume': volume_matrix[sym2],\n",
    "           f'{sym1}_return': returns[sym1],\n",
    "           f'{sym2}_return': returns[sym2]\n",
    "       })\n",
    "       \n",
    "       for t in range(1, lookback+1):\n",
    "           df[f'{sym1}_volume_t-{t}'] = df[f'{sym1}_volume'].shift(t)\n",
    "           df[f'{sym2}_volume_t-{t}'] = df[f'{sym2}_volume'].shift(t)\n",
    "           df[f'{sym1}_return_t-{t}'] = df[f'{sym1}_return'].shift(t)\n",
    "           df[f'{sym2}_return_t-{t}'] = df[f'{sym2}_return'].shift(t)\n",
    "       \n",
    "       spread_next_day = (returns[sym1] - returns[sym2]).shift(-1)\n",
    "       df['target'] = spread_next_day\n",
    "       \n",
    "       feature_cols = [col for col in df.columns if 't-' in col]\n",
    "       features = df[feature_cols].copy()\n",
    "       \n",
    "       clean_idx = features.dropna().index\n",
    "       features = features.loc[clean_idx]\n",
    "       target = df.loc[clean_idx, 'target']\n",
    "   \n",
    "       train_mask = (features.index >= train_period['start']) & (features.index < train_period['end'])\n",
    "       test_mask = (features.index >= test_period['start']) & (features.index < pd.Timestamp('2024-12-31'))\n",
    "       \n",
    "       ml_datasets[f'{sym1}_{sym2}'] = {\n",
    "           'X_train': features[train_mask],\n",
    "           'X_test': features[test_mask],\n",
    "           'y_train': target[train_mask],\n",
    "           'y_test': target[test_mask]\n",
    "       }\n",
    "       \n",
    "   return ml_datasets"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:59:26.343048Z",
     "start_time": "2025-02-09T13:59:26.340922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:59:26.363679Z",
     "start_time": "2025-02-09T13:59:26.359907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_evaluate_models(ml_datasets, coint_results):\n",
    "    results = {}\n",
    "    \n",
    "    param_grid = {\n",
    "        'kernel': ['rbf', 'linear', 'poly'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'epsilon': [0.001, 0.01, 0.1, 1],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "        'degree': [2, 3, 4]\n",
    "    }\n",
    "    \n",
    "    for pair, data in tqdm(ml_datasets.items(), desc=\"Training models\"):\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        \n",
    "        p_value = coint_results[\n",
    "            ((coint_results['symbol1'] == sym1) & (coint_results['symbol2'] == sym2)) |\n",
    "            ((coint_results['symbol1'] == sym2) & (coint_results['symbol2'] == sym1))\n",
    "        ]['p_value'].iloc[0]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(data['X_train'])\n",
    "        X_test_scaled = scaler.transform(data['X_test'])\n",
    "        \n",
    "        svr = SVR()\n",
    "        grid_search = GridSearchCV(\n",
    "            svr, \n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_scaled, data['y_train'])\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        predictions = best_model.predict(X_test_scaled)\n",
    "        \n",
    "        r2 = r2_score(data['y_test'], predictions)\n",
    "        mse = mean_squared_error(data['y_test'], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        weighted_score = 0.7 * r2 + 0.3 * (1 - p_value)\n",
    "        \n",
    "        results[pair] = {\n",
    "            'model': best_model,\n",
    "            'scaler': scaler,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'p_value': p_value,\n",
    "            'weighted_score': weighted_score,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "    \n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T14:09:51.998725Z",
     "start_time": "2025-02-09T13:59:26.381055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "score_matrix, pvalue_matrix, cointegrated_pairs, coint_results = analyze_pairs(price_matrix)\n",
    "\n",
    "# Sortiere nach p-value und nimm die Top 5\n",
    "top_5_pairs = coint_results.sort_values('p_value').head(1)\n",
    "top_5_pairs_list = [(row['symbol1'], row['symbol2']) for _, row in top_5_pairs.iterrows()]\n",
    "\n",
    "# Berechne returns und spreads nur für diese 5 Paare\n",
    "returns, spreads = calculate_returns_and_spreads(price_matrix, top_5_pairs_list)\n",
    "\n",
    "train_period = get_training_period()\n",
    "test_period = get_test_period()\n",
    "\n",
    "# Bereite ML Daten nur für diese 5 Paare vor\n",
    "ml_datasets = prepare_ml_data(price_matrix, volume_matrix, spreads, train_period, test_period)\n",
    "\n",
    "# Training nur für diese 5 Paare\n",
    "model_results = train_evaluate_models(ml_datasets, top_5_pairs)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing pairs: 100%|██████████| 4371/4371 [01:00<00:00, 72.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete!\n",
      "Found 244 cointegrated pairs\n",
      "Total pairs analyzed: 4371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 1/1 [09:25<00:00, 565.57s/it]\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T14:09:52.027140Z",
     "start_time": "2025-02-09T14:09:52.023815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for pair, res in model_results.items():\n",
    "    print(f\"{pair}: {res['best_params']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAT_NXPI: {'C': 0.1, 'degree': 2, 'epsilon': 0.001, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T14:09:52.046198Z",
     "start_time": "2025-02-09T14:09:52.041778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_df = pd.DataFrame([\n",
    "   {\n",
    "       'pair': pair,\n",
    "       'r2': metrics['r2'],\n",
    "       'rmse': metrics['rmse'],\n",
    "       'p_value': metrics['p_value'],\n",
    "       'weighted_score': metrics['weighted_score']\n",
    "   }\n",
    "   for pair, metrics in model_results.items()\n",
    "])\n",
    "\n",
    "top_20 = results_df.sort_values('weighted_score', ascending=False).head(20)\n",
    "print(top_20[['pair', 'r2', 'rmse', 'p_value', 'weighted_score']])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        pair        r2     rmse   p_value  weighted_score\n",
      "0  AMAT_NXPI -0.030879  0.01933  0.000091        0.278357\n"
     ]
    }
   ],
   "execution_count": 87
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib tqdm clickhouse-driver pandas numpy statsmodels seaborn matplotlib pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from clickhouse_driver import Client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': datetime(2015, 2, 2).date(),\n",
    "    'TRAIN_END': datetime(2020, 2, 2).date(),\n",
    "    'TEST_END': datetime(2025, 1, 1).date(),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "SELECTED_SECTOR = 'Information Technology'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hauptfunktion\n",
    "Die Hauptfunktion find_cointegrated_pairs():\n",
    "Diese Funktion ist das Herzstück der Analyse. Sie erstellt zwei Matrizen:\n",
    "- Eine für Statistik-Scores (score_matrix)\n",
    "- Eine für p-Werte (pvalue_matrix)\n",
    "\n",
    "Der Code durchläuft dann jedes mögliche Aktienpaar und führt einen Kointegrationstest durch.\n",
    "\n",
    "**Zeitbereich:**\n",
    "Im Code wird der Zeitbereich so festgelegt:\n",
    "```python\n",
    "start_date = client.execute('SELECT MIN(date) FROM stock_data')[0][0]\n",
    "end_date = start_date + timedelta(days=48*30)\n",
    "```\n",
    "Das bedeutet, der Code nimmt das früheste verfügbare Datum in der Datenbank und analysiert von dort aus die nächsten 48 Monate (also 4 Jahre). Die Analyse läuft also über einen 4-Jahres-Zeitraum, beginnend mit dem ersten verfügbaren Datenpunkt.\n",
    "\n",
    "1. Teststatistik:\n",
    "Stellen Sie sich die Teststatistik wie ein Thermometer vor. Sie misst, wie stark zwei Aktien miteinander verbunden sind. Je negativer der Wert ist, desto stärker ist die Verbindung. Das ist wie bei einem Thermometer, das unter Null fällt - je weiter unter Null, desto \"kälter\" oder in unserem Fall, desto stärker verbunden sind die Aktien.\n",
    "\n",
    "2. P-Wert:\n",
    "Der P-Wert ist wie eine Wahrscheinlichkeitsangabe auf einer Skala von 0 bis 1. Er sagt uns, wie verlässlich unsere Beobachtung ist:\n",
    "- Ein p-Wert von 0.05 bedeutet eine 5% Chance, dass wir uns irren\n",
    "- Je kleiner der p-Wert, desto sicherer können wir sein\n",
    "- Im Code werden Aktienpaare mit p-Wert < 0.05 als bedeutsam eingestuft\n",
    "Das ist wie bei einer Wettervorhersage: Wenn die Regenwahrscheinlichkeit bei 5% liegt, sind wir ziemlich sicher, dass es nicht regnen wird.\n",
    "\n",
    "3. Kritische Werte:\n",
    "Die kritischen Werte sind wie Grenzlinien. Sie helfen uns zu entscheiden, ob die Teststatistik bedeutsam ist:\n",
    "- Es gibt typischerweise drei kritische Werte (1%, 5% und 10% Niveau)\n",
    "- Wenn unsere Teststatistik kleiner (negativer) ist als diese Werte, haben wir einen signifikanten Fund\n",
    "Das ist wie beim Hochsprung: Die kritischen Werte sind wie verschiedene Höhen der Latte. Wenn ein Springer darüber kommt, ist es eine bedeutende Leistung.\n",
    "\n",
    "Ein praktisches Beispiel:\n",
    "Nehmen wir an, wir analysieren zwei Energieaktien:\n",
    "- Teststatistik: -3.5\n",
    "- P-Wert: 0.02\n",
    "- Kritische Werte: [-3.4, -2.9, -2.6]\n",
    "\n",
    "Das würde bedeuten:\n",
    "- Die Teststatistik (-3.5) ist kleiner als der strengste kritische Wert (-3.4)\n",
    "- Der p-Wert (0.02 oder 2%) ist kleiner als 0.05 (5%)\n",
    "- Schlussfolgerung: Diese Aktien haben eine sehr starke, statistisch signifikante Verbindung\n",
    "\n",
    "Diese Analyse ist besonders wichtig für Händler, die Pairs-Trading-Strategien entwickeln wollen, wo sie auf die Annäherung von zeitweise auseinandergelaufenen, aber grundsätzlich verbundenen Aktien setzen.\n",
    "\n",
    "Möchten Sie, dass ich einen dieser Aspekte noch genauer erkläre oder sollen wir uns ansehen, wie diese Werte praktisch interpretiert werden können?\n",
    "\n",
    "----\n",
    "\n",
    "#### Visualisierung:\n",
    "```python\n",
    "plt.figure(figsize=(12, 8))\n",
    "mask = (pvalues >= 0.98)\n",
    "sns.heatmap(pvalues, \n",
    "            xticklabels=data.columns, \n",
    "            yticklabels=data.columns, \n",
    "            cmap='RdYlGn_r',\n",
    "            mask=mask)\n",
    "```\n",
    "Die Heatmap zeigt p-Werte für alle Aktienpaare:\n",
    "- Die Maske blendet sehr hohe p-Werte (≥ 0.98) aus\n",
    "- Die Farbskala geht von Rot (hohe p-Werte, keine Kointegration) zu Grün (niedrige p-Werte, starke Kointegration)\n",
    "- Die Achsen zeigen die Aktiensymbole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cointegrated_pairs(data):\n",
    "    n = data.shape[1]\n",
    "    score_matrix = np.zeros((n, n))\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    keys = data.keys()\n",
    "    pairs = []\n",
    "    results = []  # Für alle Paare\n",
    "\n",
    "    total_iterations = sum(range(n))\n",
    "    \n",
    "    with tqdm(total=total_iterations, desc=\"Analyzing pairs\") as pbar:\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                S1 = data[keys[i]]\n",
    "                S2 = data[keys[j]]\n",
    "                result = coint(S1, S2)\n",
    "                score = result[0]\n",
    "                pvalue = result[1]\n",
    "                score_matrix[i, j] = score\n",
    "                pvalue_matrix[i, j] = pvalue\n",
    "                \n",
    "                results.append({\n",
    "                    'symbol1': keys[i],\n",
    "                    'symbol2': keys[j],\n",
    "                    'p_value': pvalue,\n",
    "                    'score': score\n",
    "                })\n",
    "                \n",
    "                if pvalue <= 0.01:\n",
    "                    pairs.append((keys[i], keys[j]))\n",
    "                pbar.update(1)\n",
    "\n",
    "    return score_matrix, pvalue_matrix, pairs, results\n",
    "\n",
    "def update_pairs_stats(client, results):\n",
    "    for result in tqdm(results, desc=\"Updating database\"):\n",
    "        sector_query = f\"\"\"\n",
    "        SELECT sector \n",
    "        FROM stock_pairs \n",
    "        WHERE symbol1 = '{result['symbol1']}' AND symbol2 = '{result['symbol2']}'\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        sector = client.execute(sector_query)[0][0]\n",
    "        sector_clean = sector.replace(' ', '_').upper()\n",
    "        pair_key = f\"{sector_clean}_{result['symbol1']}_{result['symbol2']}\"\n",
    "        \n",
    "        update_query = f\"\"\"\n",
    "        ALTER TABLE stock_pairs \n",
    "        UPDATE p_value = {result['p_value']},\n",
    "               cointegration_score = {result['score']}\n",
    "        WHERE pair_key = '{pair_key}'\n",
    "        \"\"\"\n",
    "        client.execute(update_query)\n",
    "\n",
    "scores, pvalues, pairs, results = find_cointegrated_pairs(data)\n",
    "update_pairs_stats(client, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sector_pairs():\n",
    "    print(f\"Fetching pairs for sector: {SELECTED_SECTOR if SELECTED_SECTOR else 'ALL'}\")\n",
    "    query = '''\n",
    "    SELECT pair_key, sector, symbol1, symbol2 \n",
    "    FROM stock_pairs\n",
    "    '''\n",
    "    if SELECTED_SECTOR:\n",
    "        query += f\" WHERE sector = '{SELECTED_SECTOR}'\"\n",
    "    pairs_df = pd.DataFrame(client.execute(query), \n",
    "                          columns=['pair_key', 'sector', 'symbol1', 'symbol2'])\n",
    "    print(f\"Found {len(pairs_df)} pairs\")\n",
    "    return pairs_df\n",
    "\n",
    "def get_stock_data(symbols):\n",
    "    placeholders = ', '.join(f\"'{s}'\" for s in symbols)\n",
    "    query = f'''\n",
    "    SELECT symbol, date, close \n",
    "    FROM stock_data\n",
    "    WHERE symbol IN ({placeholders})\n",
    "    AND date BETWEEN '{DATE_CONFIG['TRAIN_START']}' AND '{DATE_CONFIG['TEST_END']}'\n",
    "    ORDER BY symbol, date\n",
    "    '''\n",
    "    print(\"Loading stock data...\")\n",
    "    df = pd.DataFrame(\n",
    "        client.execute(query), \n",
    "        columns=['symbol', 'date', 'close']\n",
    "    )\n",
    "    print(f\"Loaded data for {len(symbols)} symbols\")\n",
    "    return df.pivot(columns='symbol', values='close', index='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting analysis...\")\n",
    "pairs_df = get_sector_pairs()\n",
    "unique_symbols = pd.concat([pairs_df['symbol1'], pairs_df['symbol2']]).unique()\n",
    "data = get_stock_data(unique_symbols)\n",
    "\n",
    "print(\"Cleaning data...\")\n",
    "min_periods = len(data) * 0.95\n",
    "\n",
    "print(f\"Analysis will be performed on {data.shape[1]} symbols\")\n",
    "\n",
    "print(\"Starting cointegration analysis...\")\n",
    "scores, pvalues, pairs = find_cointegrated_pairs(data)\n",
    "\n",
    "# %%\n",
    "print(\"Creating visualization...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "mask = (pvalues >= 0.98)\n",
    "sns.heatmap(pvalues, \n",
    "            xticklabels=data.columns, \n",
    "            yticklabels=data.columns, \n",
    "            cmap='RdYlGn_r',\n",
    "            mask=mask)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Cointegration p-values heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCointegrated Pairs (p-value < 0.01):\")\n",
    "for pair in pairs:\n",
    "    print(f\"{pair[0]} - {pair[1]}\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairs(data, pairs):\n",
    "    for s1, s2 in pairs:\n",
    "        S1 = data[s1]\n",
    "        S2 = data[s2]\n",
    "        score, pvalue, _ = coint(S1, S2)\n",
    "        ratios = S1 / S2\n",
    "        \n",
    "        plt.figure(figsize=(15,7))\n",
    "        ratios.plot()\n",
    "        plt.axhline(ratios.mean(), color='r')\n",
    "        plt.title(f'{s1} / {s2} Price Ratio (p-value: {pvalue:.4f})')\n",
    "        plt.legend(['Price Ratio', 'Mean'])\n",
    "        plt.show()\n",
    "        print(f\"\\n{s1} - {s2} p-value: {pvalue}\")\n",
    "\n",
    "plot_pairs(data, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(series):\n",
    "    return (series - series.mean()) / np.std(series)\n",
    "\n",
    "for pair in pairs:\n",
    "    symbol1, symbol2 = pair\n",
    "    \n",
    "    ratios = data[symbol1] / data[symbol2]\n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    zscore(ratios).plot()\n",
    "    plt.axhline(zscore(ratios).mean(), color='black')\n",
    "    plt.axhline(1.0, color='red', linestyle='--')\n",
    "    plt.axhline(-1.0, color='green', linestyle='--')\n",
    "    plt.title(f'Z-Score: {symbol1} vs {symbol2}')\n",
    "    plt.legend(['Ratio z-score', 'Mean', '+1', '-1'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:\n",
    "    symbol1, symbol2 = pair\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH pair_data AS (\n",
    "        SELECT symbol, date, close \n",
    "        FROM stock_data\n",
    "        WHERE symbol IN ('{symbol1}', '{symbol2}')\n",
    "        AND date BETWEEN '{DATE_CONFIG['TRAIN_START']}' AND '{DATE_CONFIG['TEST_END']}'\n",
    "        ORDER BY symbol, date\n",
    "    )\n",
    "    SELECT * FROM pair_data\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        client.execute(query),\n",
    "        columns=['symbol', 'date', 'close']\n",
    "    )\n",
    "    \n",
    "    pair_data = df.pivot(columns='symbol', values='close', index='date')\n",
    "    ratios = pair_data[symbol1] / pair_data[symbol2]\n",
    "    training_days = get_training_days()\n",
    "    train = ratios[:training_days]\n",
    "    test = ratios[training_days:]\n",
    "    \n",
    "    ratios_mavg5 = train.rolling(window=5, center=False).mean()\n",
    "    ratios_mavg60 = train.rolling(window=60, center=False).mean()\n",
    "    std_60 = train.rolling(window=60, center=False).std()\n",
    "    zscore_60_5 = (ratios_mavg5 - ratios_mavg60)/std_60\n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(train.index, train.values)\n",
    "    plt.plot(ratios_mavg5.index, ratios_mavg5.values)\n",
    "    plt.plot(ratios_mavg60.index, ratios_mavg60.values)\n",
    "    \n",
    "    plt.title(f'Moving Averages: {symbol1} vs {symbol2}')\n",
    "    plt.legend(['Ratio','5d Ratio MA', '60d Ratio MA'])\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:\n",
    "    symbol1, symbol2 = pair\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH pair_data AS (\n",
    "        SELECT symbol, date, close \n",
    "        FROM stock_data\n",
    "        WHERE symbol IN ('{symbol1}', '{symbol2}')\n",
    "        AND date BETWEEN '{DATE_CONFIG['TRAIN_START']}' AND '{DATE_CONFIG['TEST_END']}'\n",
    "        ORDER BY symbol, date\n",
    "    )\n",
    "    SELECT * FROM pair_data\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        client.execute(query),\n",
    "        columns=['symbol', 'date', 'close']\n",
    "    )\n",
    "    \n",
    "    pair_data = df.pivot(columns='symbol', values='close', index='date')\n",
    "    ratios = pair_data[symbol1] / pair_data[symbol2]\n",
    "    training_days = get_training_days()\n",
    "    train = ratios[:training_days]\n",
    "    \n",
    "    ratios_mavg5 = train.rolling(window=5, center=False).mean()\n",
    "    ratios_mavg60 = train.rolling(window=60, center=False).mean()\n",
    "    std_60 = train.rolling(window=60, center=False).std()\n",
    "    zscore_60_5 = (ratios_mavg5 - ratios_mavg60)/std_60\n",
    "    \n",
    "    buy = train.copy()\n",
    "    sell = train.copy()\n",
    "    buy[zscore_60_5>-1] = 0\n",
    "    sell[zscore_60_5<1] = 0\n",
    "    \n",
    "    plt.figure(figsize=(18,9))\n",
    "    S1 = pair_data[symbol1].iloc[:training_days]\n",
    "    S2 = pair_data[symbol2].iloc[:training_days]\n",
    "    \n",
    "    S1[60:].plot(color='b')\n",
    "    S2[60:].plot(color='c')\n",
    "    \n",
    "    buyR = 0*S1.copy()\n",
    "    sellR = 0*S1.copy()\n",
    "    \n",
    "    buyR[buy!=0] = S1[buy!=0]\n",
    "    sellR[buy!=0] = S2[buy!=0]\n",
    "    buyR[sell!=0] = S2[sell!=0]\n",
    "    sellR[sell!=0] = S1[sell!=0]\n",
    "    \n",
    "    buyR[60:].plot(color='g', linestyle='None', marker='^')\n",
    "    sellR[60:].plot(color='r', linestyle='None', marker='^')\n",
    "    \n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,min(S1.min(),S2.min()),max(S1.max(),S2.max())))\n",
    "    \n",
    "    plt.title(f'Price Action: {symbol1} vs {symbol2}')\n",
    "    plt.legend([symbol1, symbol2, 'Buy Signal', 'Sell Signal'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Ratios (ratios_train = S1_train/S2_train)**\n",
    "- **Bedeutung**: Zeigt die relative Preisbeziehung zwischen zwei Aktien\n",
    "- **Eigenschaft**: Sollte theoretisch um einen Mittelwert schwanken \n",
    "- **Nutzen**: Identifiziert, wenn Aktien aus ihrer historischen Preisbeziehung ausbrechen\n",
    "- **Beispiel**: Wenn zwei Banken normalerweise im Verhältnis 2:1 handeln und plötzlich auf 2.5:1 steigen, könnte dies eine Handelsmöglichkeit sein\n",
    "\n",
    "2. **Moving Averages (ma1_train, ma2_train)**\n",
    "- **Kurzer MA (5 Tage)**:\n",
    "  - **Eigenschaft**: Reagiert schnell auf Preisänderungen\n",
    "  - **Nutzen**: Zeigt kurzfristige Trends\n",
    "  - **Aussage**: Hilft kurzfristige Abweichungen zu erkennen\n",
    "\n",
    "- **Langer MA (60 Tage)**:\n",
    "  - **Eigenschaft**: Glättet Preisschwankungen stark\n",
    "  - **Nutzen**: Bestimmt den langfristigen \"normalen\" Zustand\n",
    "  - **Aussage**: Dient als Referenzpunkt für \"faire\" Bewertung\n",
    "\n",
    "3. **Standardabweichung (std_train)**\n",
    "- **Bedeutung**: Misst die \"normale\" Schwankungsbreite der Ratios\n",
    "- **Eigenschaft**: Höhere Werte bedeuten mehr Volatilität\n",
    "- **Nutzen**: Hilft zu bestimmen, ob eine Abweichung signifikant ist\n",
    "- **Beispiel**: Wenn Ratio normalerweise ±5% schwankt, ist eine 15% Abweichung bedeutend\n",
    "\n",
    "4. **Z-Score**\n",
    "- **Bedeutung**: Standardisierte Abweichung vom Durchschnitt\n",
    "- **Eigenschaften**:\n",
    "  - \\>1: Stark überdurchschnittlich\n",
    "  - <-1: Stark unterdurchschnittlich\n",
    "  - Zwischen -0.5 und 0.5: \"Normal\"\n",
    "- **Nutzen**: Automatisierte Handelsentscheidungen\n",
    "- **Beispiel**: \n",
    "  - Z-Score = 2 bedeutet: Ratio ist 2 Standardabweichungen über normal\n",
    "  - Deutet auf überbewertet/unterbewertet hin\n",
    "\n",
    "5. **Position Tracking (countS1, countS2)**\n",
    "- **Bedeutung**: Aktuelle Handelsposition\n",
    "- **Eigenschaften**: \n",
    "  - Positiv: Long-Position\n",
    "  - Negativ: Short-Position\n",
    "- **Nutzen**: Verfolgt offene Positionen und deren Größe\n",
    "- **Beispiel**: \n",
    "  - countS1 = -1, countS2 = +2 bedeutet:\n",
    "  - Short 1 Einheit von S1\n",
    "  - Long 2 Einheiten von S2\n",
    "\n",
    "6. **Money (Gewinn/Verlust)**\n",
    "- **Bedeutung**: Kumulierter Handelserfolg\n",
    "- **Eigenschaft**: Summe aller realisierten Gewinne/Verluste\n",
    "- **Nutzen**: Misst Strategie-Performance\n",
    "- **Beispiel**: \n",
    "  - Positive Werte: Profitable Strategie\n",
    "  - Negative Werte: Verlustbringende Strategie\n",
    "\n",
    "Die Strategie basiert auf der Annahme, dass extreme Abweichungen (gemessen durch Z-Score) sich wieder normalisieren werden. Wenn der Z-Score extrem wird (>1 oder <-1), wird eine Position aufgebaut, und wenn sich das Verhältnis normalisiert (Z-Score zwischen -0.5 und 0.5), wird die Position geschlossen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade(S1_train, S2_train, S1_test, S2_test, window1, window2, symbol1, symbol2):\n",
    "    # Original ratios und MAs\n",
    "    ratios_train = S1_train/S2_train\n",
    "    ma1_train = ratios_train.rolling(window=window1, center=False).mean()\n",
    "    ma2_train = ratios_train.rolling(window=window2, center=False).mean()\n",
    "    std_train = ratios_train.rolling(window=window2, center=False).std()\n",
    "    \n",
    "    ratios_test = S1_test/S2_test\n",
    "    \n",
    "    last_ma1 = ma1_train.iloc[-1]\n",
    "    last_ma2 = ma2_train.iloc[-1]\n",
    "    last_std = std_train.iloc[-1]\n",
    "    \n",
    "    trades = []\n",
    "    \n",
    "    # Feature-Berechnung für ML\n",
    "    for i in range(len(ratios_test)):\n",
    "        current_ratio = ratios_test.iloc[i]\n",
    "        zscore = (current_ratio - last_ma2)/last_std\n",
    "        \n",
    "        # Erweiterte Features für ML\n",
    "        trade_info = {\n",
    "            'date': ratios_test.index[i],\n",
    "            'zscore': zscore,\n",
    "            'ratio': current_ratio,\n",
    "            'ma1': last_ma1,\n",
    "            'ma2': last_ma2,\n",
    "            'S1_price': S1_test.iloc[i],\n",
    "            'S2_price': S2_test.iloc[i],\n",
    "            # Neue Features für ML:\n",
    "            'std_ratio': std_train.iloc[-1],          # Volatilität des Ratios\n",
    "            'ratio_trend': current_ratio/last_ma2,    # Trend des Ratios\n",
    "            'ma_spread': (last_ma1 - last_ma2),       # Spread zwischen MAs\n",
    "            'price_momentum_s1': S1_test.iloc[i]/S1_test.iloc[max(0,i-5)], # 5-Tage Momentum\n",
    "            'price_momentum_s2': S2_test.iloc[i]/S2_test.iloc[max(0,i-5)]  # 5-Tage Momentum\n",
    "        }\n",
    "        \n",
    "        # Original Trading-Logik bleibt gleich\n",
    "        if zscore > 1.5:\n",
    "            trade_info['action'] = 'SHORT'\n",
    "            trades.append(trade_info)\n",
    "            \n",
    "        elif zscore < -1.5:\n",
    "            trade_info['action'] = 'LONG'\n",
    "            trades.append(trade_info)\n",
    "            \n",
    "        elif abs(zscore) < 0.5:\n",
    "            trade_info['action'] = 'EXIT'\n",
    "            trades.append(trade_info)\n",
    "            \n",
    "        last_ma1 = 0.8 * last_ma1 + 0.2 * current_ratio\n",
    "        last_ma2 = 0.983 * last_ma2 + 0.017 * current_ratio\n",
    "    \n",
    "    trades_df = pd.DataFrame(trades)\n",
    "    trades_df['pair'] = f\"{symbol1}-{symbol2}\"\n",
    "    \n",
    "    return trades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trades = []\n",
    "\n",
    "for pair in pairs:\n",
    "    symbol1, symbol2 = pair\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH pair_data AS (\n",
    "        SELECT \n",
    "            symbol, \n",
    "            date, \n",
    "            close,\n",
    "            volume,\n",
    "            high,\n",
    "            low\n",
    "        FROM stock_data\n",
    "        WHERE symbol IN ('{symbol1}', '{symbol2}')\n",
    "        AND date BETWEEN '{DATE_CONFIG['TRAIN_START']}' AND '{DATE_CONFIG['TEST_END']}'\n",
    "        ORDER BY symbol, date\n",
    "    )\n",
    "    SELECT * FROM pair_data\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        client.execute(query),\n",
    "        columns=['symbol', 'date', 'close', 'volume', 'high', 'low']\n",
    "    )\n",
    "    \n",
    "    # Da wir nur close für die pairs brauchen:\n",
    "    pair_data = df.pivot(columns='symbol', values='close', index='date')\n",
    "    \n",
    "    # Zusätzliche features als separate DataFrames wenn nötig  \n",
    "    volume_data = df.pivot(columns='symbol', values='volume', index='date')\n",
    "    \n",
    "    training_mask = pair_data.index < DATE_CONFIG['TRAIN_END']\n",
    "    \n",
    "    S1_train = pair_data[symbol1][training_mask]\n",
    "    S2_train = pair_data[symbol2][training_mask]\n",
    "    S1_test = pair_data[symbol1][~training_mask]\n",
    "    S2_test = pair_data[symbol2][~training_mask]\n",
    "    \n",
    "    trades_df = trade(S1_train, S2_train, S1_test, S2_test, 5, 60, symbol1, symbol2)\n",
    "    all_trades.append(trades_df)\n",
    "\n",
    "final_trades = pd.concat(all_trades)\n",
    "final_trades.to_parquet('../data/processed/01_static_cointegraton_trading_results.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def simulate_trading(trades_df, initial_capital=10000, commission_per_share=0.005, variable_fee=0.00018, \n",
    "                    bid_ask_spread=0.0002, price_impact_coef=0.002):\n",
    "    capital = initial_capital\n",
    "    positions = {}\n",
    "    trades_history = []\n",
    "    daily_returns = []\n",
    "    last_date = None\n",
    "    daily_capital = initial_capital\n",
    "    \n",
    "    for _, trade in trades_df.iterrows():\n",
    "        if last_date is None:\n",
    "            last_date = trade['date']\n",
    "        elif trade['date'] != last_date:\n",
    "            daily_returns.append({\n",
    "                'date': last_date,\n",
    "                'return': (capital - daily_capital) / daily_capital\n",
    "            })\n",
    "            daily_capital = capital\n",
    "            last_date = trade['date']\n",
    "            \n",
    "        risk_amount = capital * 0.01\n",
    "        s1_symbol, s2_symbol = trade['pair'].split('-')\n",
    "        \n",
    "        if trade['pair'] in positions:\n",
    "            old_pos = positions[trade['pair']]\n",
    "            pnl = 0\n",
    "            hold_time = trade['date'] - old_pos['entry_date']\n",
    "            \n",
    "            if old_pos['type'] == 'LONG':\n",
    "                s1_pnl = old_pos['s1_shares'] * (trade['S1_price'] - old_pos['s1_entry'])\n",
    "                s2_pnl = old_pos['s2_shares'] * (old_pos['s2_entry'] - trade['S2_price'])\n",
    "                pnl = s1_pnl + s2_pnl\n",
    "            else:\n",
    "                s1_pnl = old_pos['s1_shares'] * (old_pos['s1_entry'] - trade['S1_price'])\n",
    "                s2_pnl = old_pos['s2_shares'] * (trade['S2_price'] - old_pos['s2_entry'])\n",
    "                pnl = s1_pnl + s2_pnl\n",
    "            \n",
    "            s1_trade_value = old_pos['s1_shares'] * trade['S1_price']\n",
    "            s2_trade_value = old_pos['s2_shares'] * trade['S2_price']\n",
    "            total_value = s1_trade_value + s2_trade_value\n",
    "            \n",
    "            commission_cost = (old_pos['s1_shares'] + old_pos['s2_shares']) * commission_per_share\n",
    "            variable_cost = total_value * variable_fee\n",
    "            spread_cost = total_value * bid_ask_spread\n",
    "            price_impact = total_value * price_impact_coef * np.sqrt(total_value/1000000)\n",
    "            total_costs = commission_cost + variable_cost + spread_cost + price_impact\n",
    "            \n",
    "            capital += pnl - total_costs\n",
    "            del positions[trade['pair']]\n",
    "            \n",
    "            trades_history.append({\n",
    "                'date': trade['date'],\n",
    "                'pair': trade['pair'],\n",
    "                'type': 'CLOSE',\n",
    "                'pnl': pnl,\n",
    "                'capital': capital,\n",
    "                'transaction_costs': total_costs,\n",
    "                'hold_time': hold_time.total_seconds() / 3600,\n",
    "                'return': pnl / risk_amount\n",
    "            })\n",
    "        \n",
    "        if trade['action'] in ['LONG', 'SHORT']:\n",
    "            s1_shares = risk_amount / trade['S1_price']\n",
    "            s2_shares = s1_shares * trade['ratio']\n",
    "            \n",
    "            s1_trade_value = s1_shares * trade['S1_price']\n",
    "            s2_trade_value = s2_shares * trade['S2_price']\n",
    "            total_value = s1_trade_value + s2_trade_value\n",
    "            \n",
    "            commission_cost = (s1_shares + s2_shares) * commission_per_share\n",
    "            variable_cost = total_value * variable_fee\n",
    "            spread_cost = total_value * bid_ask_spread\n",
    "            price_impact = total_value * price_impact_coef * np.sqrt(total_value/1000000)\n",
    "            total_costs = commission_cost + variable_cost + spread_cost + price_impact\n",
    "            \n",
    "            cost_adjustment = 1 - (total_costs / risk_amount)\n",
    "            s1_shares *= cost_adjustment\n",
    "            s2_shares *= cost_adjustment\n",
    "            \n",
    "            positions[trade['pair']] = {\n",
    "                'type': trade['action'],\n",
    "                's1_shares': s1_shares,\n",
    "                's2_shares': s2_shares,\n",
    "                's1_entry': trade['S1_price'],\n",
    "                's2_entry': trade['S2_price'],\n",
    "                'entry_date': trade['date']\n",
    "            }\n",
    "            \n",
    "            trades_history.append({\n",
    "                'date': trade['date'],\n",
    "                'pair': trade['pair'],\n",
    "                'type': 'OPEN',\n",
    "                'action': trade['action'],\n",
    "                'capital': capital,\n",
    "                'transaction_costs': total_costs\n",
    "            })\n",
    "    \n",
    "    trades_df = pd.DataFrame(trades_history)\n",
    "    daily_returns_df = pd.DataFrame(daily_returns)\n",
    "    \n",
    "    closed_trades = trades_df[trades_df['type'] == 'CLOSE']\n",
    "    metrics = calculate_metrics(trades_df, daily_returns_df, initial_capital)\n",
    "    \n",
    "    return trades_df, metrics\n",
    "\n",
    "def calculate_metrics(trades_df, daily_returns_df, initial_capital):\n",
    "    closed_trades = trades_df[trades_df['type'] == 'CLOSE']\n",
    "    \n",
    "    metrics = {\n",
    "        'total_trades': len(trades_df[trades_df['type'] == 'OPEN']),\n",
    "        'total_pnl': closed_trades['pnl'].sum(),\n",
    "        'total_return': (trades_df['capital'].iloc[-1] / initial_capital - 1) * 100,\n",
    "        'win_rate': (closed_trades['pnl'] > 0).mean() * 100,\n",
    "        'avg_profit_per_trade': closed_trades['pnl'].mean(),\n",
    "        'avg_win': closed_trades[closed_trades['pnl'] > 0]['pnl'].mean(),\n",
    "        'avg_loss': closed_trades[closed_trades['pnl'] < 0]['pnl'].mean(),\n",
    "        'largest_win': closed_trades['pnl'].max(),\n",
    "        'largest_loss': closed_trades['pnl'].min(),\n",
    "        'profit_factor': abs(closed_trades[closed_trades['pnl'] > 0]['pnl'].sum() / \n",
    "                           closed_trades[closed_trades['pnl'] < 0]['pnl'].sum()),\n",
    "        'avg_hold_time': closed_trades['hold_time'].mean(),\n",
    "        'total_transaction_costs': trades_df['transaction_costs'].sum(),\n",
    "        'avg_transaction_costs': trades_df['transaction_costs'].mean()\n",
    "    }\n",
    "    \n",
    "    daily_returns = daily_returns_df['return']\n",
    "    metrics.update({\n",
    "        'sharpe_ratio': np.sqrt(252) * (daily_returns.mean() / daily_returns.std()),\n",
    "        'volatility_annual': daily_returns.std() * np.sqrt(252) * 100,\n",
    "        'max_drawdown': calculate_max_drawdown(trades_df['capital']),\n",
    "        'skewness': stats.skew(daily_returns),\n",
    "        'kurtosis': stats.kurtosis(daily_returns)\n",
    "    })\n",
    "    \n",
    "    pair_metrics = calculate_pair_metrics(closed_trades)\n",
    "    metrics['pair_metrics'] = pair_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_max_drawdown(capital_series):\n",
    "    peak = capital_series.expanding(min_periods=1).max()\n",
    "    drawdown = (capital_series - peak) / peak\n",
    "    return drawdown.min() * 100\n",
    "\n",
    "def calculate_pair_metrics(closed_trades):\n",
    "    pair_metrics = {}\n",
    "    for pair in closed_trades['pair'].unique():\n",
    "        pair_trades = closed_trades[closed_trades['pair'] == pair]\n",
    "        pair_metrics[pair] = {\n",
    "            'total_trades': len(pair_trades),\n",
    "            'win_rate': (pair_trades['pnl'] > 0).mean() * 100,\n",
    "            'total_pnl': pair_trades['pnl'].sum(),\n",
    "            'avg_profit': pair_trades['pnl'].mean(),\n",
    "            'avg_hold_time': pair_trades['hold_time'].mean()\n",
    "        }\n",
    "    return pair_metrics\n",
    "\n",
    "trades_df = pd.read_parquet('../data/processed/01_static_cointegraton_trading_results.parquet')\n",
    "results, metrics = simulate_trading(trades_df)\n",
    "\n",
    "print(f\"=== Grundlegende Metriken ===\")\n",
    "print(f\"Endkapital: ${results['capital'].iloc[-1]:.2f}\")\n",
    "print(f\"Gesamtrendite: {metrics['total_return']:.2f}%\")\n",
    "print(f\"Anzahl Trades: {metrics['total_trades']}\")\n",
    "print(f\"\\n=== Performance Metriken ===\")\n",
    "print(f\"Win Rate: {metrics['win_rate']:.2f}%\")\n",
    "print(f\"Profit Faktor: {metrics['profit_factor']:.2f}\")\n",
    "print(f\"Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n",
    "print(f\"Max Drawdown: {metrics['max_drawdown']:.2f}%\")\n",
    "print(f\"Jährliche Volatilität: {metrics['volatility_annual']:.2f}%\")\n",
    "print(f\"\\n=== Trade Statistiken ===\")\n",
    "print(f\"Durchschn. Gewinn: ${metrics['avg_win']:.2f}\")\n",
    "print(f\"Durchschn. Verlust: ${metrics['avg_loss']:.2f}\")\n",
    "print(f\"Größter Gewinn: ${metrics['largest_win']:.2f}\")\n",
    "print(f\"Größter Verlust: ${metrics['largest_loss']:.2f}\")\n",
    "print(f\"Durchschn. Haltezeit: {metrics['avg_hold_time']:.2f} Stunden\")\n",
    "print(f\"\\n=== Kosten ===\")\n",
    "print(f\"Gesamte Transaktionskosten: ${metrics['total_transaction_costs']:.2f}\")\n",
    "print(f\"Durchschn. Kosten/Trade: ${metrics['avg_transaction_costs']:.2f}\")\n",
    "print(f\"\\n=== Verteilungsmetriken ===\")\n",
    "print(f\"Schiefe: {metrics['skewness']:.2f}\")\n",
    "print(f\"Kurtosis: {metrics['kurtosis']:.2f}\")\n",
    "\n",
    "print(\"\\n=== Performance nach Pairs ===\")\n",
    "for pair, pair_metric in metrics['pair_metrics'].items():\n",
    "    print(f\"\\n{pair}:\")\n",
    "    print(f\"Trades: {pair_metric['total_trades']}\")\n",
    "    print(f\"Win Rate: {pair_metric['win_rate']:.2f}%\")\n",
    "    print(f\"Gesamt P&L: ${pair_metric['total_pnl']:.2f}\")\n",
    "    print(f\"Durchschn. Profit: ${pair_metric['avg_profit']:.2f}\")\n",
    "    print(f\"Durchschn. Haltezeit: {pair_metric['avg_hold_time']:.2f} Stunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maschine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /Users/mumaxi2/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /Users/mumaxi2/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /Users/mumaxi2/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /Users/mumaxi2/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /Users/mumaxi2/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_features(features_df):\n",
    "   df = features_df.copy()\n",
    "   \n",
    "   # Core Features mit verschiedenen Zeitfenstern\n",
    "   df['zscore_abs'] = np.abs(df['zscore'])\n",
    "   df['zscore_trend_short'] = df.groupby('pair')['zscore'].ewm(span=5).mean().reset_index(0, drop=True)\n",
    "   df['zscore_trend'] = df.groupby('pair')['zscore'].ewm(span=10).mean().reset_index(0, drop=True)\n",
    "   df['zscore_trend_long'] = df.groupby('pair')['zscore'].ewm(span=20).mean().reset_index(0, drop=True)\n",
    "   \n",
    "   # Reversion Strength in verschiedenen Zeitfenstern\n",
    "   df['ratio_momentum_short'] = df.groupby('pair')['ratio'].pct_change(3)\n",
    "   df['ratio_momentum'] = df.groupby('pair')['ratio'].pct_change(5)\n",
    "   df['ratio_momentum_long'] = df.groupby('pair')['ratio'].pct_change(10)\n",
    "   \n",
    "   df['reversion_strength_short'] = df['zscore'] * df['ratio_momentum_short']\n",
    "   df['reversion_strength'] = df['zscore'] * df['ratio_momentum']\n",
    "   df['reversion_strength_long'] = df['zscore'] * df['ratio_momentum_long']\n",
    "   \n",
    "   # Signal Quality\n",
    "   df['signal_consistency'] = df.groupby('pair')['zscore'].rolling(10).std().reset_index(0, drop=True)\n",
    "   \n",
    "   df = df.fillna(method='ffill').fillna(0)\n",
    "   \n",
    "   feature_columns = [\n",
    "       'zscore', 'zscore_abs',\n",
    "       'zscore_trend_short', 'zscore_trend', 'zscore_trend_long',\n",
    "       'reversion_strength_short', 'reversion_strength', 'reversion_strength_long',\n",
    "       'signal_consistency'\n",
    "   ]\n",
    "   \n",
    "   return df, feature_columns\n",
    "def prepare_ml_data(trades_df, simulation_results):\n",
    "    print(\"Initial columns in trades_df:\", trades_df.columns.tolist())\n",
    "    merged_data = pd.merge(\n",
    "        trades_df,\n",
    "        simulation_results[['date', 'pair', 'pnl', 'return', 'hold_time']],\n",
    "        on=['date', 'pair'],\n",
    "        how='inner'\n",
    "    )\n",
    "    print(\"Columns after merge:\", merged_data.columns.tolist())\n",
    "    \n",
    "    features_df = merged_data.copy()\n",
    "    features_df['profitable'] = features_df['pnl'] > 0\n",
    "    \n",
    "    # Generate enhanced features\n",
    "    features_df, feature_columns = enhance_features(features_df)\n",
    "    \n",
    "    return features_df, feature_columns, 'profitable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X, y):\n",
    "    over = SMOTE(sampling_strategy=0.8, random_state=42)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.9, random_state=42)\n",
    "    \n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    return pipeline.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ml_model(features_df, feature_columns, target_column, test_size=0.2):\n",
    "    X = features_df[feature_columns]\n",
    "    y = features_df[target_column]\n",
    "    \n",
    "    split_idx = int(len(features_df) * (1-test_size))\n",
    "    X_train = X[:split_idx]\n",
    "    y_train = y[:split_idx]\n",
    "    X_test = X[split_idx:]\n",
    "    y_test = y[split_idx:]\n",
    "    \n",
    "    X_balanced, y_balanced = balance_data(X_train, y_train)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_leaf': [50, 100]\n",
    "    }\n",
    "    \n",
    "    model = GridSearchCV(\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_balanced, y_balanced)\n",
    "    best_model = model.best_estimator_\n",
    "    \n",
    "    print(\"\\nBest Parameters:\", model.best_params_)\n",
    "    print(\"\\nModel Evaluation on Test Set:\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance)\n",
    "    \n",
    "    return best_model, feature_importance, (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(model, X_test, y_test):\n",
    "    predictions = model.predict_proba(X_test)\n",
    "    probs = predictions[:, 1]  # Wahrscheinlichkeit für die positive Klasse\n",
    "    thresholds = np.arange(0.3, 0.9, 0.1)\n",
    "    \n",
    "    print(\"\\nPrediction quality at different confidence levels:\")\n",
    "    for threshold in thresholds:\n",
    "        mask = probs > threshold\n",
    "        if mask.any():\n",
    "            y_filtered = y_test[mask]  # Nur die relevanten y-Werte\n",
    "            preds = np.ones(len(y_filtered))  # Alle über dem Threshold sind 1\n",
    "            acc = (preds == y_filtered).mean()\n",
    "            n_preds = mask.sum()\n",
    "            print(f\"Threshold {threshold:.1f}: Accuracy = {acc:.2%} (on {n_preds} predictions)\")\n",
    "    \n",
    "    prediction_df = pd.DataFrame({\n",
    "        'actual': y_test,\n",
    "        'probability': probs,\n",
    "        'high_conf_pred': probs > 0.7\n",
    "    })\n",
    "    \n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(features_df, feature_columns):\n",
    "    # Kopie erstellen\n",
    "    cleaned_df = features_df.copy()\n",
    "    \n",
    "    # Forward fill für rolling calculations\n",
    "    for col in feature_columns:\n",
    "        if cleaned_df[col].isna().any():\n",
    "            cleaned_df[col] = cleaned_df.groupby('pair')[col].fillna(method='ffill')\n",
    "            cleaned_df[col] = cleaned_df.groupby('pair')[col].fillna(method='bfill')\n",
    "            \n",
    "    # Restliche NaNs mit 0 füllen\n",
    "    cleaned_df = cleaned_df.fillna(0)\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing ML data...\n",
      "Initial columns in trades_df: ['date', 'zscore', 'ratio', 'ma1', 'ma2', 'S1_price', 'S2_price', 'std_ratio', 'ratio_trend', 'ma_spread', 'pair', 'action']\n",
      "Columns after merge: ['date', 'zscore', 'ratio', 'ma1', 'ma2', 'S1_price', 'S2_price', 'std_ratio', 'ratio_trend', 'ma_spread', 'pair', 'action', 'pnl', 'return', 'hold_time']\n",
      "Cleaning data...\n",
      "\n",
      "Training ML model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/w5v6vwl91955r0nc47_mqt88flhqcv/T/ipykernel_66775/573042856.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill').fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 50, 'n_estimators': 200}\n",
      "\n",
      "Model Evaluation on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.79      0.80       261\n",
      "        True       0.44      0.47      0.46        93\n",
      "\n",
      "    accuracy                           0.70       354\n",
      "   macro avg       0.62      0.63      0.63       354\n",
      "weighted avg       0.71      0.70      0.71       354\n",
      "\n",
      "\n",
      "Feature Importance:\n",
      "                feature  importance\n",
      "5    reversion_strength    0.189498\n",
      "2          zscore_trend    0.155599\n",
      "1            zscore_abs    0.148331\n",
      "6    signal_consistency    0.099936\n",
      "3            vol_change    0.089776\n",
      "7            zscore_vol    0.086961\n",
      "0                zscore    0.073864\n",
      "4        ratio_momentum    0.064992\n",
      "9  momentum_consistency    0.048378\n",
      "8        trend_strength    0.042666\n",
      "\n",
      "Analyzing predictions...\n",
      "\n",
      "Prediction quality at different confidence levels:\n",
      "Threshold 0.3: Accuracy = 36.78% (on 174 predictions)\n",
      "Threshold 0.4: Accuracy = 41.04% (on 134 predictions)\n",
      "Threshold 0.5: Accuracy = 44.00% (on 100 predictions)\n",
      "Threshold 0.6: Accuracy = 45.57% (on 79 predictions)\n",
      "Threshold 0.7: Accuracy = 51.79% (on 56 predictions)\n",
      "Threshold 0.8: Accuracy = 69.70% (on 33 predictions)\n",
      "Threshold 0.9: Accuracy = 80.00% (on 10 predictions)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing ML data...\")\n",
    "features_df, feature_columns, target_column = prepare_ml_data(trades_df, simulation_results)\n",
    "\n",
    "print(\"Cleaning data...\")\n",
    "features_df = clean_data(features_df, feature_columns)\n",
    "\n",
    "print(\"\\nTraining ML model...\")\n",
    "model, feature_importance, (X_train, X_test, y_train, y_test) = train_ml_model(\n",
    "    features_df, \n",
    "    feature_columns, \n",
    "    target_column\n",
    ")\n",
    "\n",
    "print(\"\\nAnalyzing predictions...\")\n",
    "prediction_analysis = analyze_predictions(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

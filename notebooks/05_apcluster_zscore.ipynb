{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.analysis.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Basic configuration\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "    \n",
    "    symbols = price_matrix.columns.tolist()\n",
    "    \n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "    \n",
    "    return price_matrix, symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loade Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_matrix, symbols = load_and_prepare_data('../data/raw/nasdaq_daily.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\text{Rendite} = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{P_t - P_{t-1}}{P_{t-1}} \\times \\text{Trading days per year} $$\n",
    "\n",
    "\n",
    "$$ \\text{VolatilitÃ¤t} = \\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} \\left( \\frac{P_t - P_{t-1}}{P_{t-1}} - \\mu \\right)^2} \\times \\sqrt{\\text{Trading days per year}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(price_matrix):\n",
    "    returns = price_matrix.pct_change().mean() * DATE_CONFIG['TRADING_DAYS_PER_YEAR']\n",
    "    metrics = pd.DataFrame(returns, columns=['returns'])\n",
    "    metrics['volatility'] = price_matrix.pct_change().std() * np.sqrt(DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics(price_matrix)\n",
    "print(\"Erste 5 Zeilen der Metriken:\")\n",
    "print(metrics.head())\n",
    "print(\"\\nBeschreibung der Metriken:\")\n",
    "print(metrics.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale Transform\n",
    "The StandardScaler transforms our features (returns and volatility) to have zero mean and unit variance, which eliminates the scale difference between our variables and prevents higher magnitude features from dominating. This standardization is crucial for many machine learning algorithms as it ensures that all features contribute equally to the model and helps prevent numerical instabilities during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_metrics(metrics):\n",
    "    scaler = StandardScaler()\n",
    "    scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(metrics),\n",
    "        columns=metrics.columns,\n",
    "        index=metrics.index\n",
    "    )\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scale_metrics(metrics)\n",
    "print(\"Erste 5 Zeilen der skalierten Daten:\")\n",
    "print(X.head())\n",
    "print(\"\\nBeschreibung der skalierten Daten:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_pairs(X, ap, price_matrix, min_pairs=20, p_threshold=0.05):\n",
    "    scores = []\n",
    "    \n",
    "    for cluster_id in np.unique(ap.labels_):\n",
    "        cluster_mask = ap.labels_ == cluster_id\n",
    "        cluster_symbols = X.index[cluster_mask]\n",
    "        center = X.iloc[ap.cluster_centers_indices_[cluster_id]]\n",
    "        \n",
    "        for i in range(len(cluster_symbols)):\n",
    "            for j in range(i+1, len(cluster_symbols)):\n",
    "                symbol1, symbol2 = cluster_symbols[i], cluster_symbols[j]\n",
    "                \n",
    "                dist1 = np.linalg.norm(X.loc[symbol1] - center)\n",
    "                dist2 = np.linalg.norm(X.loc[symbol2] - center)\n",
    "                center_dist = (dist1 + dist2) / 2\n",
    "                \n",
    "                profile_diff = np.linalg.norm(X.loc[symbol1] - X.loc[symbol2])\n",
    "                \n",
    "                series1 = price_matrix[symbol1]\n",
    "                series2 = price_matrix[symbol2]\n",
    "                score, pvalue, _ = coint(series1, series2)\n",
    "                \n",
    "                if pvalue < p_threshold:\n",
    "                    scores.append({\n",
    "                        'pair': (symbol1, symbol2),\n",
    "                        'center_dist': center_dist,\n",
    "                        'profile_diff': profile_diff,\n",
    "                        'pvalue': pvalue,\n",
    "                        'cluster': cluster_id\n",
    "                    })\n",
    "\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "\n",
    "    scores_df['center_dist_norm'] = (scores_df['center_dist'] - scores_df['center_dist'].min()) / \\\n",
    "                                   (scores_df['center_dist'].max() - scores_df['center_dist'].min())\n",
    "    scores_df['profile_diff_norm'] = (scores_df['profile_diff'] - scores_df['profile_diff'].min()) / \\\n",
    "                                    (scores_df['profile_diff'].max() - scores_df['profile_diff'].min())\n",
    "\n",
    "    scores_df['combined_score'] = 0.6 * scores_df['center_dist_norm'] + \\\n",
    "                                 0.4 * scores_df['profile_diff_norm']\n",
    "    \n",
    "    scores_df = scores_df.sort_values('combined_score')\n",
    "    \n",
    "    while len(scores_df) < min_pairs and p_threshold < 0.1:\n",
    "        p_threshold += 0.05\n",
    "        scores = []\n",
    "        for cluster_id in np.unique(ap.labels_):\n",
    "            cluster_mask = ap.labels_ == cluster_id\n",
    "            cluster_symbols = X.index[cluster_mask]\n",
    "            center = X.iloc[ap.cluster_centers_indices_[cluster_id]]\n",
    "            \n",
    "            for i in range(len(cluster_symbols)):\n",
    "                for j in range(i+1, len(cluster_symbols)):\n",
    "                    symbol1, symbol2 = cluster_symbols[i], cluster_symbols[j]\n",
    "                    dist1 = np.linalg.norm(X.loc[symbol1] - center)\n",
    "                    dist2 = np.linalg.norm(X.loc[symbol2] - center)\n",
    "                    center_dist = (dist1 + dist2) / 2\n",
    "                    profile_diff = np.linalg.norm(X.loc[symbol1] - X.loc[symbol2])\n",
    "                    score, pvalue, _ = coint(series1, series2)\n",
    "                    \n",
    "                    if pvalue < p_threshold:\n",
    "                        scores.append({\n",
    "                            'pair': (symbol1, symbol2),\n",
    "                            'center_dist': center_dist,\n",
    "                            'profile_diff': profile_diff,\n",
    "                            'pvalue': pvalue,\n",
    "                            'cluster': cluster_id\n",
    "                        })\n",
    "        \n",
    "        scores_df = pd.DataFrame(scores)\n",
    "        if len(scores) > 0:\n",
    "            scores_df['center_dist_norm'] = (scores_df['center_dist'] - scores_df['center_dist'].min()) / \\\n",
    "                                          (scores_df['center_dist'].max() - scores_df['center_dist'].min())\n",
    "            scores_df['profile_diff_norm'] = (scores_df['profile_diff'] - scores_df['profile_diff'].min()) / \\\n",
    "                                           (scores_df['profile_diff'].max() - scores_df['profile_diff'].min())\n",
    "            scores_df['combined_score'] = 0.6 * scores_df['center_dist_norm'] + \\\n",
    "                                        0.4 * scores_df['profile_diff_norm']\n",
    "            scores_df = scores_df.sort_values('combined_score')\n",
    "    \n",
    "    print(f\"Found {len(scores_df)} pairs with p-value < {p_threshold}\")\n",
    "    return scores_df['pair'].tolist()[:min_pairs], scores_df[:min_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affinity Propagation Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AffinityPropagation()\n",
    "ap.fit(X)\n",
    "labels1 = ap.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(X.iloc[:,0], X.iloc[:,1], c=labels1, cmap='rainbow')\n",
    "ax.set_title('Affinity Propagation Clustering Results')\n",
    "ax.set_xlabel('Mean Return')\n",
    "ax.set_ylabel('Volatility')\n",
    "plt.colorbar(scatter)\n",
    "plt.show()\n",
    "\n",
    "top_pairs, scores_df = get_top_pairs(X, ap, price_matrix, min_pairs=20)\n",
    "\n",
    "print(\"\\nTop pairs details:\")\n",
    "print(scores_df[['pair', 'center_dist', 'profile_diff', 'pvalue', 'combined_score', 'cluster']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "#Extract the cluster centers and labels\n",
    "cci = ap.cluster_centers_indices_\n",
    "labels2 = ap.labels_\n",
    "\n",
    "#Print their number\n",
    "clusters = len(cci)\n",
    "print('The number of clusters is:',clusters)\n",
    "\n",
    "#Plot the results\n",
    "X_ap = np.asarray(X)\n",
    "plt.close('all')\n",
    "plt.figure(1)\n",
    "plt.clf\n",
    "fig=plt.figure(figsize=(15,10))\n",
    "colors = cycle('cmykrgbcmykrgbcmykrgbcmykrgb')\n",
    "for k, col in zip(range(clusters),colors):\n",
    "    cluster_members = labels2 == k\n",
    "    cluster_center = X_ap[cci[k]]\n",
    "    plt.plot(X_ap[cluster_members, 0], X_ap[cluster_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=12)\n",
    "    for x in X_ap[cluster_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.title('Affinity Propagation Clustering Results with Connections')\n",
    "plt.xlabel('Mean Return')\n",
    "plt.ylabel('Volatility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_series_ap = pd.Series(index=X.index, data=ap.labels_.flatten())\n",
    "\n",
    "cluster_size_limit = 1000\n",
    "counts = clustered_series_ap.value_counts()\n",
    "ticker_count = counts[(counts>1) & (counts<=cluster_size_limit)]\n",
    "print(\"Number of clusters:\", len(ticker_count))\n",
    "print(\"Number of Pairs:\", (ticker_count*(ticker_count-1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_with_clusters = []\n",
    "\n",
    "for cluster_id in np.unique(ap.labels_):\n",
    "    cluster_mask = ap.labels_ == cluster_id\n",
    "    cluster_symbols = X.index[cluster_mask]\n",
    "    \n",
    "    if len(cluster_symbols) > 1:\n",
    "        cluster_prices = price_matrix[cluster_symbols]\n",
    "        score_matrix, pvalue_matrix, pairs, _ = analyze_pairs(\n",
    "            cluster_prices,\n",
    "            pvalue_threshold=0.05\n",
    "        )\n",
    "        \n",
    "        if len(pairs) > 0:\n",
    "            for pair in pairs:\n",
    "                all_pairs_with_clusters.append({\n",
    "                    'pair': pair,\n",
    "                    'cluster': cluster_id\n",
    "                })\n",
    "            print(f\"\\nCluster {cluster_id} pairs:\")\n",
    "            for pair in pairs:\n",
    "                print(f\"{pair[0]} - {pair[1]}\")\n",
    "                \n",
    "            plot_cointegration_heatmap(pvalue_matrix, cluster_symbols)\n",
    "\n",
    "all_pairs = [item['pair'] for item in all_pairs_with_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = np.unique([stock for pair in top_pairs for stock in pair])\n",
    "X_data = pd.DataFrame(index=X.index, data=X).T  \n",
    "in_pairs_series = pd.Series(index=stocks, data=[ap.labels_[list(X.index).index(stock)] for stock in stocks])\n",
    "X_pairs = X_data.T.loc[stocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tsne = TSNE(learning_rate=30, perplexity=5, random_state=42, n_jobs=-1).fit_transform(X_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,12), facecolor='white')\n",
    "plt.clf()\n",
    "plt.gca().set_facecolor('#f8f9fa')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "for pair in top_pairs:\n",
    "    cluster = scores_df[scores_df['pair'] == pair]['cluster'].values[0]\n",
    "    loc1 = X_pairs.index.get_loc(pair[0])\n",
    "    loc2 = X_pairs.index.get_loc(pair[1])\n",
    "    x1, y1 = X_tsne[loc1, :]\n",
    "    x2, y2 = X_tsne[loc2, :]\n",
    "    plt.plot([x1, x2], [y1, y2], '-', alpha=0.4, linewidth=1.5, color='#4a90e2')\n",
    "\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                     s=300,\n",
    "                     alpha=0.7,\n",
    "                     c=in_pairs_series.values,\n",
    "                     cmap='tab20',\n",
    "                     edgecolor='white',\n",
    "                     linewidth=2)\n",
    "\n",
    "for x, y, name in zip(X_tsne[:,0], X_tsne[:,1], X_pairs.index):\n",
    "    plt.annotate(name,\n",
    "                (x,y),\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(0,10),\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=11,\n",
    "                fontweight='bold',\n",
    "                bbox=dict(facecolor='white', \n",
    "                         edgecolor='none',\n",
    "                         alpha=0.7,\n",
    "                         pad=1))\n",
    "\n",
    "plt.title('Stock Pairs Clustering Visualization (Same Cluster Only)', \n",
    "          fontsize=16, \n",
    "          pad=20,\n",
    "          fontweight='bold')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(series):\n",
    "    return (series - series.mean()) / np.std(series)\n",
    "\n",
    "def calculate_spread(data, symbol1, symbol2, start_date=None, end_date=None):\n",
    "    if start_date:\n",
    "        mask = (data.index >= start_date) & (data.index <= end_date)\n",
    "        data = data[mask]\n",
    "    \n",
    "    # Calculate ratio and z-score\n",
    "    ratios = data[symbol1] / data[symbol2]\n",
    "    zscore_ratios = zscore(ratios)\n",
    "    \n",
    "    return ratios, zscore_ratios\n",
    "\n",
    "def trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2, window1=5, window2=60):\n",
    "    ratios_train = S1_train / S2_train\n",
    "    ma2_train = ratios_train.rolling(window=window2, center=False).mean()\n",
    "    std_train = ratios_train.rolling(window=window2, center=False).std()\n",
    "    \n",
    "    ratios_test = S1_test / S2_test\n",
    "    trades = []\n",
    "    trade_id = 0\n",
    "    position = 0\n",
    "    entry_prices = None\n",
    "    entry_date = None\n",
    "    \n",
    "    for i in range(len(ratios_test)):\n",
    "        current_ratio = ratios_test.iloc[i]\n",
    "        current_date = ratios_test.index[i]\n",
    "        \n",
    "        ma2_test = ratios_test.iloc[:i+1].rolling(window=window2, center=False).mean().iloc[-1]\n",
    "        std_test = ratios_test.iloc[:i+1].rolling(window=window2, center=False).std().iloc[-1]\n",
    "        zscore = (current_ratio - ma2_test) / std_test\n",
    "        \n",
    "        if position == 0:\n",
    "            if zscore > 1.0:\n",
    "                entry_date = current_date\n",
    "                entry_prices = {\n",
    "                    symbol1: {\"price\": S1_test.iloc[i], \"type\": \"short\"},\n",
    "                    symbol2: {\"price\": S2_test.iloc[i], \"type\": \"long\"}\n",
    "                }\n",
    "                position = -1\n",
    "                \n",
    "            elif zscore < -1.0:\n",
    "                entry_date = current_date\n",
    "                entry_prices = {\n",
    "                    symbol1: {\"price\": S1_test.iloc[i], \"type\": \"long\"},\n",
    "                    symbol2: {\"price\": S2_test.iloc[i], \"type\": \"short\"}\n",
    "                }\n",
    "                position = 1\n",
    "                \n",
    "        elif abs(zscore) < 0.5 and position != 0:\n",
    "            for symbol in [symbol1, symbol2]:\n",
    "                trades.append({\n",
    "                    'trade_id': trade_id,\n",
    "                    'symbol': symbol,\n",
    "                    'entry_date': entry_date,\n",
    "                    'entry_price': entry_prices[symbol][\"price\"],\n",
    "                    'exit_date': current_date,\n",
    "                    'exit_price': S1_test.iloc[i] if symbol == symbol1 else S2_test.iloc[i],\n",
    "                    'position_type': entry_prices[symbol][\"type\"],\n",
    "                    'paired_symbol': symbol2 if symbol == symbol1 else symbol1,\n",
    "                    'exit_type': 'target'\n",
    "                })\n",
    "            position = 0\n",
    "            trade_id += 1\n",
    "            \n",
    "    return trades\n",
    "\n",
    "def backtest_pairs(price_matrix, pairs, train_end_date):\n",
    "    all_trades = []\n",
    "    \n",
    "    for symbol1, symbol2 in pairs:\n",
    "        training_mask = price_matrix.index < train_end_date\n",
    "        \n",
    "        S1_train = price_matrix[symbol1][training_mask]\n",
    "        S2_train = price_matrix[symbol2][training_mask]\n",
    "        S1_test = price_matrix[symbol1][~training_mask]\n",
    "        S2_test = price_matrix[symbol2][~training_mask]\n",
    "        \n",
    "        pair_trades = trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2)\n",
    "        all_trades.extend(pair_trades)\n",
    "    \n",
    "    trades_df = pd.DataFrame(all_trades)\n",
    "    trades_df.to_parquet('../data/results/apcluster_zscore_results.parquet')\n",
    "    \n",
    "    return trades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df = backtest_pairs(price_matrix, top_pairs, DATE_CONFIG['TRAIN_END'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug ð ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gefundene Top Pairs:\")\n",
    "for pair in top_pairs:\n",
    "    print(f\"{pair[0]} - {pair[1]}\")\n",
    "print(f\"\\nAnzahl der Top Pairs: {len(top_pairs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

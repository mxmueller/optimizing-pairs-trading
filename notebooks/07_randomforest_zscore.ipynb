{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "    \n",
    "    symbols = price_matrix.columns.tolist()\n",
    "    \n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "    \n",
    "    return price_matrix, symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(symbols):\n",
    "   return [(s1, s2) for i, s1 in enumerate(symbols) for s2 in symbols[i+1:]]\n",
    "\n",
    "def calculate_returns_and_spreads(price_matrix):\n",
    "    returns = price_matrix.pct_change().dropna()\n",
    "    \n",
    "    pairs = generate_pairs(price_matrix.columns)\n",
    "    \n",
    "    spreads = pd.DataFrame(index=returns.index)\n",
    "    for s1, s2 in pairs:\n",
    "        spreads[f'{s1}_{s2}_spread'] = returns[s1] - returns[s2]\n",
    "        \n",
    "    return returns, spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(returns, spreads, train_period, test_period, lookback=3):\n",
    "    ml_datasets = {}\n",
    "    \n",
    "    for spread_col in spreads.columns:\n",
    "        sym1, sym2 = spread_col.replace('_spread', '').split('_')\n",
    "        \n",
    "        # Basis-Features\n",
    "        df = pd.DataFrame({\n",
    "            f'{sym1}_return': returns[sym1],\n",
    "            f'{sym2}_return': returns[sym2]\n",
    "        })\n",
    "        \n",
    "        for t in range(1, lookback+1):\n",
    "            df[f'{sym1}_return_t-{t}'] = df[f'{sym1}_return'].shift(t)\n",
    "            df[f'{sym2}_return_t-{t}'] = df[f'{sym2}_return'].shift(t)\n",
    "        \n",
    "        # Target ist der Spread vom nächsten Tag\n",
    "        spread_next_day = (df[f'{sym1}_return'] - df[f'{sym2}_return']).shift(-1)\n",
    "        df['target'] = np.where(spread_next_day > 0, 1, 0)\n",
    "        \n",
    "        # Features sind nur die historischen Werte\n",
    "        feature_cols = [col for col in df.columns if 't-' in col]\n",
    "        features = df[feature_cols].copy()\n",
    "        \n",
    "        # NaN-Zeilen entfernen\n",
    "        clean_idx = features.dropna().index\n",
    "        features = features.loc[clean_idx]\n",
    "        target = df.loc[clean_idx, 'target']\n",
    "    \n",
    "        \n",
    "        train_mask = (features.index >= train_period['start']) & (features.index < train_period['end'])\n",
    "        test_mask = (features.index >= test_period['start']) & (features.index < test_period['end'])\n",
    "        \n",
    "        ml_datasets[f'{sym1}_{sym2}'] = {\n",
    "            'X_train': features[train_mask],\n",
    "            'X_test': features[test_mask],\n",
    "            'y_train': target[train_mask],\n",
    "            'y_test': target[test_mask]\n",
    "        }\n",
    "        \n",
    "    return ml_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_models(ml_datasets):\n",
    "    results = {}\n",
    "    \n",
    "    for pair, data in tqdm(ml_datasets.items(), desc=\"Training models\"):\n",
    "        # Keine extra Bereinigung mehr nötig, da schon in prepare_ml_data gemacht\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        rf.fit(data['X_train'], data['y_train'])\n",
    "        predictions = rf.predict(data['X_test'])\n",
    "        \n",
    "        results[pair] = {\n",
    "            'model': rf,\n",
    "            'accuracy': accuracy_score(data['y_test'], predictions),\n",
    "            'precision': precision_score(data['y_test'], predictions),\n",
    "            'recall': recall_score(data['y_test'], predictions),\n",
    "            'f1': f1_score(data['y_test'], predictions)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # Erste 10 Predictions vs. wahre Werte\n",
    "        #print(\"\\nErste 10 Vorhersagen vs. wahre Werte:\")\n",
    "        #for i in range(min(10, len(predictions))):\n",
    "        #    print(f\"Wahr: {data['y_test'].iloc[i]} | Vorhersage: {predictions[i]}\")\n",
    "            \n",
    "        # Feature Importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': data['X_train'].columns,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 2021-02-02 00:00:00 to 2025-01-01 00:00:00\n",
      "Total symbols: 94\n",
      "Total trading days: 985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 4371/4371 [09:02<00:00,  8.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Zuerst Daten laden\n",
    "price_matrix, symbols = load_and_prepare_data('./nasdaq_daily.parquet')\n",
    "\n",
    "# Dann Returns und Spreads berechnen\n",
    "returns, spreads = calculate_returns_and_spreads(price_matrix)\n",
    "\n",
    "# Perioden definieren\n",
    "train_period = get_training_period()\n",
    "test_period = get_test_period()\n",
    "\n",
    "# ML Daten vorbereiten\n",
    "ml_datasets = prepare_ml_data(returns, spreads, train_period, test_period)\n",
    "\n",
    "# Training und Evaluation\n",
    "model_results = train_evaluate_models(ml_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Pairs by F1-Score:\n",
      "           pair  accuracy  precision    recall        f1\n",
      "2793  FTNT_MELI  0.583333   0.565217  0.806202  0.664537\n",
      "1132  AXON_BIIB  0.539683   0.595506  0.706667  0.646341\n",
      "1150    AXON_EA  0.571429   0.615385  0.666667  0.640000\n",
      "1940  COST_ROST  0.583333   0.632653  0.645833  0.639175\n",
      "1443   BKNG_XEL  0.571429   0.584906  0.688889  0.632653\n",
      "1138  AXON_CHTR  0.579365   0.616438  0.642857  0.629371\n",
      "1722    CDW_PEP  0.579365   0.585526  0.674242  0.626761\n",
      "2032    CRWD_EA  0.567460   0.568750  0.694656  0.625430\n",
      "1210    AXON_ZS  0.563492   0.558282  0.705426  0.623288\n",
      "1424   BKNG_PEP  0.551587   0.596154  0.650350  0.622074\n",
      "1903   COST_HON  0.535714   0.592593  0.653061  0.621359\n",
      "1954   COST_XEL  0.539683   0.586420  0.659722  0.620915\n",
      "2294    CTAS_EA  0.543651   0.566265  0.686131  0.620462\n",
      "1920  COST_MNST  0.563492   0.622378  0.613793  0.618056\n",
      "287    ADP_BIIB  0.543651   0.560241  0.688889  0.617940\n",
      "3379   KLAC_XEL  0.579365   0.578231  0.658915  0.615942\n",
      "351    ADP_SBUX  0.559524   0.585526  0.649635  0.615917\n",
      "1188  AXON_PCAR  0.583333   0.595745  0.636364  0.615385\n",
      "4049   ORLY_PDD  0.551587   0.542169  0.708661  0.614334\n",
      "3268    KDP_PDD  0.535714   0.534483  0.720930  0.613861\n"
     ]
    }
   ],
   "source": [
    "# Top 20 Paare nach F1-Score sortieren und anzeigen\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'pair': pair,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1': metrics['f1']\n",
    "    }\n",
    "    for pair, metrics in model_results.items()\n",
    "])\n",
    "\n",
    "top_20 = results_df.sort_values('f1', ascending=False).head(20)\n",
    "print(\"\\nTop 20 Pairs by F1-Score:\")\n",
    "print(top_20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

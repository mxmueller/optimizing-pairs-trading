{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.analysis.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "    volume_matrix = df.pivot(index='date', columns='symbol', values='volume')\n",
    "    \n",
    "    symbols = price_matrix.columns.tolist()\n",
    "    \n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "    \n",
    "    return price_matrix, volume_matrix, symbols"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_pairs(cointegrated_pairs):\n",
    "    return cointegrated_pairs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import toml\n",
    "\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "price_matrix, volume_matrix, symbols = load_and_prepare_data(config['data']['raw_data_path'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_returns_and_spreads(price_matrix, cointegrated_pairs):\n",
    "    returns = price_matrix.pct_change().dropna()\n",
    "    \n",
    "    pairs = generate_pairs(cointegrated_pairs)\n",
    "    \n",
    "    spreads = pd.DataFrame(index=returns.index)\n",
    "    for s1, s2 in pairs:\n",
    "        spreads[f'{s1}_{s2}_spread'] = returns[s1] - returns[s2]\n",
    "        \n",
    "    return returns, spreads"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_ml_data(price_matrix, volume_matrix, spreads, train_period, test_period, lookback=5):\n",
    "   ml_datasets = {}\n",
    "   returns = price_matrix.pct_change().dropna()\n",
    "   \n",
    "   for spread_col in spreads.columns:\n",
    "       sym1, sym2 = spread_col.replace('_spread', '').split('_')\n",
    "       \n",
    "       df = pd.DataFrame({\n",
    "           # Volume Features\n",
    "           f'{sym1}_volume': volume_matrix[sym1],\n",
    "           f'{sym2}_volume': volume_matrix[sym2],\n",
    "           # Return Features \n",
    "           f'{sym1}_return': returns[sym1],\n",
    "           f'{sym2}_return': returns[sym2]\n",
    "       })\n",
    "       \n",
    "       # Volume History\n",
    "       for t in range(1, lookback+1):\n",
    "           df[f'{sym1}_volume_t-{t}'] = df[f'{sym1}_volume'].shift(t)\n",
    "           df[f'{sym2}_volume_t-{t}'] = df[f'{sym2}_volume'].shift(t)\n",
    "           \n",
    "       # Return History\n",
    "       for t in range(1, lookback+1):\n",
    "           df[f'{sym1}_return_t-{t}'] = df[f'{sym1}_return'].shift(t)\n",
    "           df[f'{sym2}_return_t-{t}'] = df[f'{sym2}_return'].shift(t)\n",
    "       \n",
    "       # Target bleibt gleich\n",
    "       spread_next_day = (returns[sym1] - returns[sym2]).shift(-1)\n",
    "       spread_next_day = spread_next_day.reindex(df.index)\n",
    "       df['target'] = np.where(spread_next_day > 0, 1, 0)\n",
    "       \n",
    "       feature_cols = [col for col in df.columns if 't-' in col]\n",
    "       features = df[feature_cols].copy()\n",
    "       \n",
    "       clean_idx = features.dropna().index\n",
    "       features = features.loc[clean_idx]\n",
    "       target = df.loc[clean_idx, 'target']\n",
    "   \n",
    "       train_mask = (features.index >= train_period['start']) & (features.index < train_period['end'])\n",
    "       test_mask = (features.index >= test_period['start']) & (features.index < test_period['end'])\n",
    "       \n",
    "       ml_datasets[f'{sym1}_{sym2}'] = {\n",
    "           'X_train': features[train_mask],\n",
    "           'X_test': features[test_mask],\n",
    "           'y_train': target[train_mask],\n",
    "           'y_test': target[test_mask]\n",
    "       }\n",
    "       \n",
    "   return ml_datasets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_evaluate_models(ml_datasets, coint_results):\n",
    "   results = {}\n",
    "   \n",
    "   for pair, data in tqdm(ml_datasets.items(), desc=\"Training models\"):\n",
    "       sym1, sym2 = pair.split('_')\n",
    "       \n",
    "       # Debug: Klassen-Balance\n",
    "       print(f\"\\nAnalyzing pair: {pair}\")\n",
    "       print(\"Class distribution (train):\", np.bincount(data['y_train']) / len(data['y_train']))\n",
    "       print(\"Class distribution (test):\", np.bincount(data['y_test']) / len(data['y_test']))\n",
    "       \n",
    "       p_value = coint_results[\n",
    "           ((coint_results['symbol1'] == sym1) & (coint_results['symbol2'] == sym2)) |\n",
    "           ((coint_results['symbol1'] == sym2) & (coint_results['symbol2'] == sym1))\n",
    "       ]['p_value'].iloc[0]\n",
    "       \n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(data['X_train'])\n",
    "       X_test_scaled = scaler.transform(data['X_test'])\n",
    "       \n",
    "       # Angepasste Parameter\n",
    "       svm = SVC(\n",
    "           kernel='rbf',\n",
    "           C=0.1,  # Reduzierter C-Wert\n",
    "           class_weight='balanced',\n",
    "           random_state=42\n",
    "       )\n",
    "       svm.fit(X_train_scaled, data['y_train'])\n",
    "       predictions = svm.predict(X_test_scaled)\n",
    "       \n",
    "       # Debug: Predictions Verteilung\n",
    "       print(\"\\nPrediction distribution:\", np.bincount(predictions) / len(predictions))\n",
    "       \n",
    "       f1 = f1_score(data['y_test'], predictions)\n",
    "       weighted_score = 1 * f1 + 0 * (1 - p_value)\n",
    "       \n",
    "       results[pair] = {\n",
    "           'model': svm,\n",
    "           'scaler': scaler,\n",
    "           'accuracy': accuracy_score(data['y_test'], predictions),\n",
    "           'precision': precision_score(data['y_test'], predictions),\n",
    "           'recall': recall_score(data['y_test'], predictions),\n",
    "           'f1': f1,\n",
    "           'p_value': p_value,\n",
    "           'weighted_score': weighted_score\n",
    "       }\n",
    "   \n",
    "   return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_matrix, pvalue_matrix, cointegrated_pairs, coint_results = analyze_pairs(price_matrix)\n",
    "\n",
    "returns, spreads = calculate_returns_and_spreads(price_matrix, cointegrated_pairs)\n",
    "\n",
    "train_period = get_training_period()\n",
    "test_period = get_test_period()\n",
    "\n",
    "ml_datasets = prepare_ml_data(price_matrix, volume_matrix, spreads, train_period, test_period)\n",
    "\n",
    "model_results = train_evaluate_models(ml_datasets, coint_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_df = pd.DataFrame([\n",
    "   {\n",
    "       'pair': pair,\n",
    "       'accuracy': metrics['accuracy'],\n",
    "       'precision': metrics['precision'],\n",
    "       'recall': metrics['recall'],\n",
    "       'f1': metrics['f1'],\n",
    "       'p_value': metrics['p_value'],\n",
    "       'weighted_score': metrics['weighted_score']\n",
    "   }\n",
    "   for pair, metrics in model_results.items()\n",
    "])\n",
    "\n",
    "top_20 = results_df.sort_values('weighted_score', ascending=False).head(20)\n",
    "print(top_20[['pair', 'f1', 'p_value', 'weighted_score', 'accuracy', 'precision', 'recall']])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

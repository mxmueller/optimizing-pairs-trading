{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.644489Z",
     "start_time": "2025-03-01T08:58:21.641055Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.statistics.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "tags": [
     "parameters"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.663991Z",
     "start_time": "2025-03-01T08:58:21.662032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p_threshold = 0.05\n",
    "min_pairs = 20\n",
    "\n",
    "window_shifts = 12\n",
    "shift_size = 1\n",
    "\n",
    "entry_threshold = 2.0\n",
    "exit_threshold = 0.5\n",
    "window1 = 5 \n",
    "window2 = 60\n",
    "\n",
    "learning_rate = 0.2\n",
    "max_depth = 2\n",
    "min_samples_leaf = 2\n",
    "min_samples_split = 2\n",
    "n_estimators = 300\n",
    "\n",
    "base_input_path = \"../../data/raw/\" \n",
    "input_filename = \"ftse_daily.parquet\" \n",
    "base_output_path = \"../../data/results/\" \n",
    "output_filename = \"GradientBoost_Z-Score_Sliding.parquet\""
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.702143Z",
     "start_time": "2025-03-01T08:58:21.699915Z"
    }
   },
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "\n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "\n",
    "    symbols = price_matrix.columns.tolist()\n",
    "\n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "\n",
    "    return df, price_matrix, symbols"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.767890Z",
     "start_time": "2025-03-01T08:58:21.736126Z"
    }
   },
   "source": [
    "input_data_path = f\"{base_input_path}{input_filename}\"\n",
    "df, price_matrix, symbols = load_and_prepare_data(input_data_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 2021-02-02 00:00:00 to 2025-01-01 00:00:00\n",
      "Total symbols: 98\n",
      "Total trading days: 987\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.808731Z",
     "start_time": "2025-03-01T08:58:21.805228Z"
    }
   },
   "source": [
    "returns_matrix = price_matrix.pct_change()\n",
    "returns_matrix = returns_matrix.dropna() "
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.846724Z",
     "start_time": "2025-03-01T08:58:21.843179Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_returns = pd.DataFrame(\n",
    "    scaler.fit_transform(returns_matrix),\n",
    "    index=returns_matrix.index,\n",
    "    columns=returns_matrix.columns\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:26.183493Z",
     "start_time": "2025-03-01T08:58:21.880810Z"
    }
   },
   "source": [
    "def calculate_rolling_correlation(returns_df, window=250):\n",
    "    pairs_corr = {}\n",
    "    symbols = returns_df.columns\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        for j in range(i+1, len(symbols)):\n",
    "            sym1, sym2 = symbols[i], symbols[j]\n",
    "            pair_key = f\"{sym1}_{sym2}\"\n",
    "            \n",
    "            rolling_corr = returns_df[[sym1, sym2]].rolling(window=window)\\\n",
    "                          .corr()\\\n",
    "                          .unstack()[sym2][sym1]\n",
    "                          \n",
    "            pairs_corr[pair_key] = rolling_corr\n",
    "            \n",
    "    return pd.DataFrame(pairs_corr)\n",
    "\n",
    "rolling_correlations = calculate_rolling_correlation(standardized_returns)\n",
    "rolling_correlations = rolling_correlations.dropna()"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 18\u001B[0m\n\u001B[1;32m     14\u001B[0m             pairs_corr[pair_key] \u001B[38;5;241m=\u001B[39m rolling_corr\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame(pairs_corr)\n\u001B[0;32m---> 18\u001B[0m rolling_correlations \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_rolling_correlation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstandardized_returns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m rolling_correlations \u001B[38;5;241m=\u001B[39m rolling_correlations\u001B[38;5;241m.\u001B[39mdropna()\n",
      "Cell \u001B[0;32mIn[38], line 11\u001B[0m, in \u001B[0;36mcalculate_rolling_correlation\u001B[0;34m(returns_df, window)\u001B[0m\n\u001B[1;32m      7\u001B[0m         sym1, sym2 \u001B[38;5;241m=\u001B[39m symbols[i], symbols[j]\n\u001B[1;32m      8\u001B[0m         pair_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msym1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msym2\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m         rolling_corr \u001B[38;5;241m=\u001B[39m \u001B[43mreturns_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43msym1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msym2\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrolling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m---> 11\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorr\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\\\n\u001B[1;32m     12\u001B[0m                       \u001B[38;5;241m.\u001B[39munstack()[sym2][sym1]\n\u001B[1;32m     14\u001B[0m         pairs_corr[pair_key] \u001B[38;5;241m=\u001B[39m rolling_corr\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame(pairs_corr)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/window/rolling.py:2861\u001B[0m, in \u001B[0;36mRolling.corr\u001B[0;34m(self, other, pairwise, ddof, numeric_only)\u001B[0m\n\u001B[1;32m   2735\u001B[0m \u001B[38;5;129m@doc\u001B[39m(\n\u001B[1;32m   2736\u001B[0m     template_header,\n\u001B[1;32m   2737\u001B[0m     create_section_header(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParameters\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2859\u001B[0m     numeric_only: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   2860\u001B[0m ):\n\u001B[0;32m-> 2861\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorr\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2862\u001B[0m \u001B[43m        \u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2863\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpairwise\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpairwise\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2864\u001B[0m \u001B[43m        \u001B[49m\u001B[43mddof\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mddof\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2865\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumeric_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2866\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/window/rolling.py:1850\u001B[0m, in \u001B[0;36mRollingAndExpandingMixin.corr\u001B[0;34m(self, other, pairwise, ddof, numeric_only)\u001B[0m\n\u001B[1;32m   1847\u001B[0m         result \u001B[38;5;241m=\u001B[39m numerator \u001B[38;5;241m/\u001B[39m denominator\n\u001B[1;32m   1848\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Series(result, index\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mindex, name\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mname, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m-> 1850\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply_pairwise\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1851\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selected_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpairwise\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorr_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumeric_only\u001B[49m\n\u001B[1;32m   1852\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/window/rolling.py:560\u001B[0m, in \u001B[0;36mBaseWindow._apply_pairwise\u001B[0;34m(self, target, other, pairwise, func, numeric_only)\u001B[0m\n\u001B[1;32m    557\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m other\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m numeric_only:\n\u001B[1;32m    558\u001B[0m     other \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_numeric_only(other)\n\u001B[0;32m--> 560\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mflex_binary_moment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpairwise\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpairwise\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/window/common.py:115\u001B[0m, in \u001B[0;36mflex_binary_moment\u001B[0;34m(arg1, arg2, f, pairwise)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    112\u001B[0m         result\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;241m=\u001B[39m MultiIndex\u001B[38;5;241m.\u001B[39mfrom_product(\n\u001B[1;32m    113\u001B[0m             [\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(arg2\u001B[38;5;241m.\u001B[39mcolumns)), \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(result_index))]\n\u001B[1;32m    114\u001B[0m         )\n\u001B[0;32m--> 115\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mswaplevel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m         result\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;241m=\u001B[39m MultiIndex\u001B[38;5;241m.\u001B[39mfrom_product(\n\u001B[1;32m    117\u001B[0m             [result_index] \u001B[38;5;241m+\u001B[39m [arg2\u001B[38;5;241m.\u001B[39mcolumns]\n\u001B[1;32m    118\u001B[0m         )\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# empty result\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:7379\u001B[0m, in \u001B[0;36mDataFrame.sort_index\u001B[0;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001B[0m\n\u001B[1;32m   7282\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msort_index\u001B[39m(\n\u001B[1;32m   7283\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   7284\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   7293\u001B[0m     key: IndexKeyFunc \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   7294\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   7295\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   7296\u001B[0m \u001B[38;5;124;03m    Sort object by labels (along an axis).\u001B[39;00m\n\u001B[1;32m   7297\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   7377\u001B[0m \u001B[38;5;124;03m    d  4\u001B[39;00m\n\u001B[1;32m   7378\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 7379\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort_index\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   7380\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7381\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mascending\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mascending\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7383\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkind\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkind\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mna_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7386\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort_remaining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort_remaining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   7389\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py:5309\u001B[0m, in \u001B[0;36mNDFrame.sort_index\u001B[0;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001B[0m\n\u001B[1;32m   5305\u001B[0m ascending \u001B[38;5;241m=\u001B[39m validate_ascending(ascending)\n\u001B[1;32m   5307\u001B[0m target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis(axis)\n\u001B[0;32m-> 5309\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[43mget_indexer_indexer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5310\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mascending\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkind\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_position\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort_remaining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\n\u001B[1;32m   5311\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m indexer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inplace:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/sorting.py:101\u001B[0m, in \u001B[0;36mget_indexer_indexer\u001B[0;34m(target, level, ascending, kind, na_position, sort_remaining, key)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m level \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     95\u001B[0m     _, indexer \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39msortlevel(\n\u001B[1;32m     96\u001B[0m         level,\n\u001B[1;32m     97\u001B[0m         ascending\u001B[38;5;241m=\u001B[39mascending,\n\u001B[1;32m     98\u001B[0m         sort_remaining\u001B[38;5;241m=\u001B[39msort_remaining,\n\u001B[1;32m     99\u001B[0m         na_position\u001B[38;5;241m=\u001B[39mna_position,\n\u001B[1;32m    100\u001B[0m     )\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (np\u001B[38;5;241m.\u001B[39mall(ascending) \u001B[38;5;129;01mand\u001B[39;00m \u001B[43mtarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_monotonic_increasing\u001B[49m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39many(ascending) \u001B[38;5;129;01mand\u001B[39;00m target\u001B[38;5;241m.\u001B[39mis_monotonic_decreasing\n\u001B[1;32m    103\u001B[0m ):\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;66;03m# Check monotonic-ness before sort an index (GH 11080)\u001B[39;00m\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(target, ABCMultiIndex):\n",
      "File \u001B[0;32mproperties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/multi.py:1692\u001B[0m, in \u001B[0;36mMultiIndex.is_monotonic_increasing\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1689\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01min\u001B[39;00m code \u001B[38;5;28;01mfor\u001B[39;00m code \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcodes):\n\u001B[1;32m   1690\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 1692\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(level\u001B[38;5;241m.\u001B[39mis_monotonic_increasing \u001B[38;5;28;01mfor\u001B[39;00m level \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlevels\u001B[49m):\n\u001B[1;32m   1693\u001B[0m     \u001B[38;5;66;03m# If each level is sorted, we can operate on the codes directly. GH27495\u001B[39;00m\n\u001B[1;32m   1694\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m libalgos\u001B[38;5;241m.\u001B[39mis_lexsorted(\n\u001B[1;32m   1695\u001B[0m         [x\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint64\u001B[39m\u001B[38;5;124m\"\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcodes]\n\u001B[1;32m   1696\u001B[0m     )\n\u001B[1;32m   1698\u001B[0m \u001B[38;5;66;03m# reversed() because lexsort() wants the most significant key last.\u001B[39;00m\n",
      "File \u001B[0;32mproperties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/multi.py:844\u001B[0m, in \u001B[0;36mMultiIndex.levels\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    839\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    841\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m    842\u001B[0m \u001B[38;5;66;03m# Levels Methods\u001B[39;00m\n\u001B[0;32m--> 844\u001B[0m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mlevels\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m FrozenList:\n\u001B[1;32m    846\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    847\u001B[0m \u001B[38;5;124;03m    Levels of the MultiIndex.\u001B[39;00m\n\u001B[1;32m    848\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    892\u001B[0m \u001B[38;5;124;03m    FrozenList([['mammal'], ['cat', 'dog', 'goat', 'human']])\u001B[39;00m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    894\u001B[0m     \u001B[38;5;66;03m# Use cache_readonly to ensure that self.get_locs doesn't repeatedly\u001B[39;00m\n\u001B[1;32m    895\u001B[0m     \u001B[38;5;66;03m# create new IndexEngine\u001B[39;00m\n\u001B[1;32m    896\u001B[0m     \u001B[38;5;66;03m# https://github.com/pandas-dev/pandas/issues/31648\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_rolling_betas(returns_df, window=250):\n",
    "    pairs_beta = {}\n",
    "    symbols = returns_df.columns\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        for j in range(i+1, len(symbols)):\n",
    "            sym1, sym2 = symbols[i], symbols[j]\n",
    "            pair_key = f\"{sym1}_{sym2}\"\n",
    "            \n",
    "            x = returns_df[sym2]\n",
    "            y = returns_df[sym1]\n",
    "            \n",
    "            rolling_cov = y.rolling(window=window).cov(x)\n",
    "            rolling_var = x.rolling(window=window).var()\n",
    "            rolling_beta = rolling_cov / rolling_var\n",
    "            \n",
    "            pairs_beta[pair_key] = rolling_beta\n",
    "            \n",
    "    return pd.DataFrame(pairs_beta)\n",
    "\n",
    "rolling_betas = calculate_rolling_betas(standardized_returns)\n",
    "rolling_betas = rolling_betas.dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_residuals_std(returns_df, window=250):\n",
    "    pairs_residuals = {}\n",
    "    symbols = returns_df.columns\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        for j in range(i+1, len(symbols)):\n",
    "            sym1, sym2 = symbols[i], symbols[j]\n",
    "            pair_key = f\"{sym1}_{sym2}\"\n",
    "            \n",
    "            x = returns_df[sym2].values\n",
    "            y = returns_df[sym1].values\n",
    "            \n",
    "            residuals = [np.nan] * len(returns_df)\n",
    "            for k in range(window, len(returns_df) + 1):\n",
    "                window_x = x[k-window:k]\n",
    "                window_y = y[k-window:k]\n",
    "                coeffs = np.polyfit(window_x, window_y, 1)\n",
    "                pred = np.polyval(coeffs, window_x)\n",
    "                window_residuals = window_y - pred\n",
    "                residuals[k-1] = np.std(window_residuals)\n",
    "                \n",
    "            pairs_residuals[pair_key] = residuals\n",
    "            \n",
    "    return pd.DataFrame(pairs_residuals, index=returns_df.index)\n",
    "\n",
    "rolling_residuals_std = calculate_residuals_std(standardized_returns)\n",
    "rolling_residuals_std = rolling_residuals_std.dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_spread_deviation(returns_df, window=250, holding_period=5, zscore_threshold=2):\n",
    "    pairs_spread_dev = {}\n",
    "    symbols = returns_df.columns\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        for j in range(i+1, len(symbols)):\n",
    "            sym1, sym2 = symbols[i], symbols[j]\n",
    "            pair_key = f\"{sym1}_{sym2}\"\n",
    "            \n",
    "            spread = returns_df[sym1] - returns_df[sym2]\n",
    "            rolling_mean = spread.rolling(window=window).mean()\n",
    "            rolling_std = spread.rolling(window=window).std()\n",
    "            zscore = (spread - rolling_mean) / rolling_std\n",
    "            \n",
    "            entry_signals = abs(zscore) > zscore_threshold\n",
    "            spread_changes = []\n",
    "            \n",
    "            for k in range(window, len(returns_df) - holding_period):\n",
    "                if entry_signals.iloc[k]:\n",
    "                    spread_t0 = spread.iloc[k]\n",
    "                    spread_t1 = spread.iloc[k + holding_period]\n",
    "                    spread_change = abs(spread_t1 - spread_t0)\n",
    "                    spread_changes.append(spread_change)\n",
    "            \n",
    "            if spread_changes:\n",
    "                pairs_spread_dev[pair_key] = np.mean(spread_changes)\n",
    "            else:\n",
    "                pairs_spread_dev[pair_key] = np.nan\n",
    "                \n",
    "    return pd.Series(pairs_spread_dev)\n",
    "\n",
    "average_spread_deviation = calculate_spread_deviation(standardized_returns)\n",
    "average_spread_deviation = average_spread_deviation.dropna()\n",
    "\n",
    "print(f\"Anzahl der berechneten Paare: {len(average_spread_deviation)}\")\n",
    "print(\"\\nTop 5 Paare mit niedrigster Spread-Abweichung:\")\n",
    "print(average_spread_deviation.nsmallest(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "feature_df = pd.DataFrame({\n",
    "   'correlation': rolling_correlations.iloc[-1],\n",
    "   'beta': rolling_betas.iloc[-1],\n",
    "   'residuals_std': rolling_residuals_std.iloc[-1]\n",
    "})\n",
    "\n",
    "feature_df['target'] = average_spread_deviation\n",
    "feature_df = feature_df.dropna()\n",
    "\n",
    "volume_matrix = df.pivot(index='date', columns='symbol', values='volume')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_volume_features(volume_df, pairs, windows=[50, 100]):\n",
    "    features = {}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        pair_features = {}\n",
    "        \n",
    "        for window in windows:\n",
    "            vol1 = volume_df[sym1].rolling(window=window)\n",
    "            vol2 = volume_df[sym2].rolling(window=window)\n",
    "            \n",
    "            pair_features[f'vol_avg_{window}'] = ((vol1.mean() + vol2.mean()) / 2).iloc[-1]\n",
    "            pair_features[f'vol_std_{window}'] = ((vol1.std() + vol2.std()) / 2).iloc[-1]\n",
    "            pair_features[f'vol_change_{window}'] = ((volume_df[sym1].pct_change() + volume_df[sym2].pct_change()) / 2).rolling(window=window).mean().iloc[-1]\n",
    "        \n",
    "        features[pair] = pair_features\n",
    "    \n",
    "    return pd.DataFrame.from_dict(features, orient='index')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "volume_features = calculate_volume_features(volume_matrix, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df[['correlation', 'beta', 'residuals_std', 'target']],\n",
    "    volume_features,\n",
    "], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_roc_features(price_df, volume_df, pairs, windows=[5, 20, 50]):\n",
    "    features = {}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        pair_features = {}\n",
    "        \n",
    "        for window in windows:\n",
    "            # Price ROC\n",
    "            price_roc_1 = price_df[sym1].pct_change(window)\n",
    "            price_roc_2 = price_df[sym2].pct_change(window)\n",
    "            \n",
    "            pair_features[f'ROC_SYM1_{window}d'] = price_roc_1.iloc[-1]\n",
    "            pair_features[f'ROC_SYM2_{window}d'] = price_roc_2.iloc[-1]\n",
    "            pair_features[f'ROC_diff_{window}d'] = price_roc_1.iloc[-1] - price_roc_2.iloc[-1]\n",
    "            \n",
    "            # Volume ROC\n",
    "            vol_roc_1 = volume_df[sym1].pct_change(window)\n",
    "            vol_roc_2 = volume_df[sym2].pct_change(window)\n",
    "            \n",
    "            pair_features[f'vol_ROC_SYM1_{window}d'] = vol_roc_1.iloc[-1]\n",
    "            pair_features[f'vol_ROC_SYM2_{window}d'] = vol_roc_2.iloc[-1]\n",
    "            pair_features[f'vol_ROC_diff_{window}d'] = vol_roc_1.iloc[-1] - vol_roc_2.iloc[-1]\n",
    "        \n",
    "        features[pair] = pair_features\n",
    "    \n",
    "    return pd.DataFrame.from_dict(features, orient='index')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "roc_features = calculate_roc_features(price_matrix, volume_matrix, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df,\n",
    "    roc_features\n",
    "], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_williams_r_features(price_df, pairs, windows=[10, 14, 28, 40]):\n",
    "   features = {}\n",
    "   \n",
    "   for pair in pairs:\n",
    "       sym1, sym2 = pair.split('_')\n",
    "       pair_features = {}\n",
    "       \n",
    "       for window in windows:\n",
    "           # Williams %R f端r Symbol 1\n",
    "           high1 = price_df[sym1].rolling(window=window).max()\n",
    "           low1 = price_df[sym1].rolling(window=window).min()\n",
    "           williams_r1 = ((high1 - price_df[sym1]) / (high1 - low1) * -100)\n",
    "           \n",
    "           # Williams %R f端r Symbol 2\n",
    "           high2 = price_df[sym2].rolling(window=window).max()\n",
    "           low2 = price_df[sym2].rolling(window=window).min()\n",
    "           williams_r2 = ((high2 - price_df[sym2]) / (high2 - low2) * -100)\n",
    "           \n",
    "           pair_features[f'WilliamsR_SYM1_{window}d'] = williams_r1.iloc[-1]\n",
    "           pair_features[f'WilliamsR_SYM2_{window}d'] = williams_r2.iloc[-1]\n",
    "           pair_features[f'WilliamsR_diff_{window}d'] = williams_r1.iloc[-1] - williams_r2.iloc[-1]\n",
    "       \n",
    "       features[pair] = pair_features\n",
    "   \n",
    "   return pd.DataFrame.from_dict(features, orient='index')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "williams_r_features = calculate_williams_r_features(price_matrix, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df,\n",
    "    williams_r_features\n",
    "], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_bollinger_bands_width(returns_df, pairs, windows=[5, 10, 20, 25, 50]):\n",
    "    features = {}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        pair_features = {}\n",
    "        \n",
    "        spread = returns_df[sym1] - returns_df[sym2]\n",
    "        \n",
    "        for window in windows:\n",
    "            sma = spread.rolling(window=window).mean()\n",
    "            std = spread.rolling(window=window).std()\n",
    "            \n",
    "            bb_width = (2 * std) / sma * 100\n",
    "            pair_features[f'BB_width_{window}d'] = bb_width.iloc[-1]\n",
    "        \n",
    "        features[pair] = pair_features\n",
    "    \n",
    "    return pd.DataFrame.from_dict(features, orient='index')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bollinger_bands_features = calculate_bollinger_bands_width(standardized_returns, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df,\n",
    "    bollinger_bands_features\n",
    "], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_macd_features(price_df, pairs, params=[\n",
    "    {'fast': 12, 'slow': 26, 'signal': 9},\n",
    "    {'fast': 5, 'slow': 13, 'signal': 5}\n",
    "]):\n",
    "    features = {}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        pair_features = {}\n",
    "        \n",
    "        for p in params:\n",
    "            period = f\"{p['fast']}_{p['slow']}_{p['signal']}\"\n",
    "            \n",
    "            for sym in [sym1, sym2]:\n",
    "                fast_ema = price_df[sym].ewm(span=p['fast']).mean()\n",
    "                slow_ema = price_df[sym].ewm(span=p['slow']).mean()\n",
    "                macd_line = fast_ema - slow_ema\n",
    "                signal_line = macd_line.ewm(span=p['signal']).mean()\n",
    "                histogram = macd_line - signal_line\n",
    "                \n",
    "                if sym == sym1:\n",
    "                    macd_line_1 = macd_line\n",
    "                    signal_line_1 = signal_line\n",
    "                    histogram_1 = histogram\n",
    "                else:\n",
    "                    macd_line_2 = macd_line\n",
    "                    signal_line_2 = signal_line\n",
    "                    histogram_2 = histogram\n",
    "            \n",
    "            pair_features[f'MACD_line_diff_{period}'] = (macd_line_1 - macd_line_2).iloc[-1]\n",
    "            pair_features[f'Signal_line_diff_{period}'] = (signal_line_1 - signal_line_2).iloc[-1]\n",
    "            pair_features[f'MACD_hist_diff_{period}'] = (histogram_1 - histogram_2).iloc[-1]\n",
    "            pair_features[f'MACD_cross_{period}_SYM1'] = 1 if macd_line_1.iloc[-1] > signal_line_1.iloc[-1] else 0\n",
    "            pair_features[f'MACD_cross_{period}_SYM2'] = 1 if macd_line_2.iloc[-1] > signal_line_2.iloc[-1] else 0\n",
    "        \n",
    "        features[pair] = pair_features\n",
    "    \n",
    "    return pd.DataFrame.from_dict(features, orient='index')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "macd_features = calculate_macd_features(price_matrix, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df,\n",
    "    macd_features\n",
    "], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search \n",
    "```Python\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], \n",
    "    'learning_rate': [0.01, 0.1, 0.2], \n",
    "    'max_depth': [2, 3, 4], \n",
    "    'min_samples_split': [2, 4], \n",
    "    'min_samples_leaf': [1, 2] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(random_state=42), \n",
    "    param_grid=param_grid, \n",
    "    scoring='r2', \n",
    "    cv=5, # Important !\n",
    "    n_jobs=-1, \n",
    "    verbose=2 \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "feature_columns = [\n",
    "    'correlation', 'beta', 'residuals_std',\n",
    "    'vol_change_50',\n",
    "    'ROC_SYM2_20d',\n",
    "    'vol_ROC_SYM1_50d',\n",
    "    'vol_ROC_SYM2_5d',\n",
    "    'ROC_SYM1_50d',\n",
    "    'vol_ROC_SYM2_20d',\n",
    "    'vol_ROC_SYM1_20d',\n",
    "    'WilliamsR_SYM1_40d',\n",
    "    'vol_ROC_SYM1_5d',\n",
    "    'ROC_SYM1_5d',\n",
    "    'vol_ROC_SYM2_50d',\n",
    "    'ROC_SYM2_50d',\n",
    "    'residuals_std',\n",
    "    'ROC_SYM1_20d',\n",
    "    'WilliamsR_SYM2_28d',\n",
    "    'ROC_SYM2_5d', \n",
    "    'WilliamsR_SYM1_28d',\n",
    "    'WilliamsR_SYM2_14d',\n",
    "    'beta', \n",
    "    'WilliamsR_SYM1_14d',\n",
    "    'vol_avg_100',\n",
    "    'MACD_line_diff_12_26_9',\n",
    "    'vol_avg_50',\n",
    "    'vol_ROC_diff_50d',\n",
    "    'WilliamsR_SYM2_40d',\n",
    "    'MACD_cross_12_26_9_SYM2',\n",
    "    'vol_change_100',\n",
    "    'MACD_hist_diff_12_26_9',\n",
    "    'vol_std_50'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = feature_df[feature_columns]\n",
    "y = feature_df['target']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "best_params_grid_search = {\n",
    "    'learning_rate': learning_rate, \n",
    "    'max_depth': max_depth, \n",
    "    'min_samples_leaf': min_samples_leaf, \n",
    "    'min_samples_split': min_samples_split, \n",
    "    'n_estimators': n_estimators\n",
    "}\n",
    "\n",
    "final_model_retrained = GradientBoostingRegressor(**best_params_grid_search, random_state=42) \n",
    "final_model_retrained.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_pred = final_model_retrained.predict(X_test)\n",
    "print(f'R2: {r2_score(y_test, y_pred):.3f}')\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred):.3f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': final_model_retrained.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "for idx, row in importance.head(30).iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': final_model_retrained.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance.head(30), x='importance', y='feature')\n",
    "plt.title('Top 10 Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predicted vs Actual\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk-Forward Evaluationfrom sklearn.impute import SimpleImputer  \n",
    "\n",
    "```\n",
    "walk_forward_dates = []\n",
    "current_train_start = DATE_CONFIG['TRAIN_START']\n",
    "current_train_end = DATE_CONFIG['TRAIN_END']\n",
    "step_size = timedelta(days=90)  \n",
    "while current_train_end <= DATE_CONFIG['TEST_END']:\n",
    "    walk_forward_dates.append({\n",
    "        'TRAIN_START': current_train_start,\n",
    "        'TRAIN_END': current_train_end\n",
    "    })\n",
    "    current_train_start += step_size\n",
    "    current_train_end += step_size\n",
    "\n",
    "walk_forward_results = []\n",
    "\n",
    "for wf_config in tqdm(walk_forward_dates, desc=\"Walk-Forward Iteration\"):\n",
    "    DATE_CONFIG['TRAIN_START'] = wf_config['TRAIN_START']\n",
    "    DATE_CONFIG['TRAIN_END'] = wf_config['TRAIN_END']\n",
    "\n",
    "    df, price_matrix, symbols = load_and_prepare_data(config['data']['raw_data_path'])\n",
    "    returns_matrix = price_matrix.pct_change().dropna()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized_returns = pd.DataFrame(\n",
    "        scaler.fit_transform(returns_matrix),\n",
    "        index=returns_matrix.index,\n",
    "        columns=returns_matrix.columns\n",
    "    )\n",
    "\n",
    "    # Rest der Feature-Berechnungen bleibt gleich\n",
    "    rolling_correlations = calculate_rolling_correlation(standardized_returns)\n",
    "    rolling_correlations = rolling_correlations.dropna()\n",
    "    rolling_betas = calculate_rolling_betas(standardized_returns)\n",
    "    rolling_betas = rolling_betas.dropna()\n",
    "    rolling_residuals_std = calculate_residuals_std(standardized_returns)\n",
    "    rolling_residuals_std = rolling_residuals_std.dropna()\n",
    "    average_spread_deviation = calculate_spread_deviation(standardized_returns)\n",
    "    average_spread_deviation = average_spread_deviation.dropna()\n",
    "\n",
    "    feature_df = pd.DataFrame({\n",
    "        'correlation': rolling_correlations.iloc[-1],\n",
    "        'beta': rolling_betas.iloc[-1],\n",
    "        'residuals_std': rolling_residuals_std.iloc[-1]\n",
    "    })\n",
    "    feature_df['target'] = average_spread_deviation\n",
    "    feature_df = feature_df.dropna()\n",
    "\n",
    "    volume_matrix = df.pivot(index='date', columns='symbol', values='volume')\n",
    "    volume_features = calculate_volume_features(volume_matrix, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df[['correlation', 'beta', 'residuals_std', 'target']], volume_features], axis=1)\n",
    "    roc_features = calculate_roc_features(price_matrix, volume_matrix, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df, roc_features], axis=1)\n",
    "    williams_r_features = calculate_williams_r_features(price_matrix, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df, williams_r_features], axis=1)\n",
    "    bollinger_bands_features = calculate_bollinger_bands_width(standardized_returns, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df, bollinger_bands_features], axis=1)\n",
    "    macd_features = calculate_macd_features(price_matrix, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df, macd_features], axis=1)\n",
    "\n",
    "    X = feature_df[feature_columns]\n",
    "    y = feature_df['target']\n",
    "\n",
    "    # Hier f端gen wir den Imputer ein\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    # Train-Test-Split mit imputierten Daten\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    best_params_grid_search = {'learning_rate': 0.2, 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2,\n",
    "                               'n_estimators': 300}\n",
    "    final_model_retrained = GradientBoostingRegressor(**best_params_grid_search, random_state=42)\n",
    "    final_model_retrained.fit(X_train, y_train)\n",
    "    y_pred = final_model_retrained.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        'TRAIN_START': str(DATE_CONFIG['TRAIN_START']),\n",
    "        'TRAIN_END': str(DATE_CONFIG['TRAIN_END']),\n",
    "        'R2': r2_score(y_test, y_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred)\n",
    "    }\n",
    "    walk_forward_results.append(results)\n",
    "\n",
    "walk_forward_df = pd.DataFrame(walk_forward_results)\n",
    "\n",
    "print(\"\\nWalk-Forward Validierungsergebnisse:\")\n",
    "print(walk_forward_df)\n",
    "\n",
    "average_metrics = walk_forward_df[['R2', 'MSE', 'MAE']].mean()\n",
    "print(\"\\nDurchschnittliche Metriken 端ber alle Walk-Forward-Schritte:\")\n",
    "print(average_metrics)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(walk_forward_df['TRAIN_END'], walk_forward_df['R2'], label='R2')\n",
    "plt.plot(walk_forward_df['TRAIN_END'], walk_forward_df['MSE'], label='MSE')\n",
    "plt.plot(walk_forward_df['TRAIN_END'], walk_forward_df['MAE'], label='MAE')\n",
    "plt.xlabel('Train End Date')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Walk-Forward Validierung Metriken')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2, window_number, window1=window1, window2=window2):\n",
    "    trades = []\n",
    "    trade_id = 0\n",
    "    active_trades = []\n",
    "    \n",
    "    S1_full = pd.concat([S1_train, S1_test])\n",
    "    S2_full = pd.concat([S2_train, S2_test])\n",
    "    \n",
    "    ratios_full = S1_full / S2_full\n",
    "    test_start = S1_test.index[0]\n",
    "    \n",
    "    for i in range(len(S1_test)):\n",
    "        current_idx = S1_train.shape[0] + i\n",
    "        ratio_history = ratios_full.iloc[:current_idx+1]\n",
    "        \n",
    "        if len(ratio_history) < window2:\n",
    "            continue\n",
    "            \n",
    "        ratio_history_past = ratios_full.iloc[:current_idx] \n",
    "        ma2 = ratio_history_past.rolling(window=window2, center=False).mean().iloc[-1] \n",
    "        std = ratio_history_past.rolling(window=window2, center=False).std().iloc[-1]   \n",
    "        \n",
    "        if std == 0:\n",
    "            continue\n",
    "            \n",
    "        current_ratio = ratios_full.iloc[current_idx]\n",
    "        current_date = S1_test.index[i]\n",
    "        zscore = (current_ratio - ma2) / std\n",
    "        \n",
    "        if zscore > entry_threshold:\n",
    "            trade_entry = {\n",
    "                'trade_id': trade_id,\n",
    "                'symbol1': symbol1,\n",
    "                'symbol2': symbol2,\n",
    "                'entry_date': current_date,\n",
    "                'entry_zscore': zscore,\n",
    "                'window': window_number,\n",
    "                'status': 'active',\n",
    "                'type': 'short',\n",
    "                'entry_prices': {\n",
    "                    symbol1: {'price': S1_test.iloc[i], 'type': 'short'},\n",
    "                    symbol2: {'price': S2_test.iloc[i], 'type': 'long'}\n",
    "                }\n",
    "            }\n",
    "            active_trades.append(trade_entry)\n",
    "            trade_id += 1\n",
    "            \n",
    "        elif zscore < -entry_threshold:\n",
    "            trade_entry = {\n",
    "                'trade_id': trade_id,\n",
    "                'symbol1': symbol1,\n",
    "                'symbol2': symbol2,\n",
    "                'entry_date': current_date,\n",
    "                'entry_zscore': zscore,\n",
    "                'window': window_number,\n",
    "                'status': 'active',\n",
    "                'type': 'long',\n",
    "                'entry_prices': {\n",
    "                    symbol1: {'price': S1_test.iloc[i], 'type': 'long'},\n",
    "                    symbol2: {'price': S2_test.iloc[i], 'type': 'short'}\n",
    "                }\n",
    "            }\n",
    "            active_trades.append(trade_entry)\n",
    "            trade_id += 1\n",
    "        \n",
    "        for trade in active_trades:\n",
    "            if trade['status'] == 'active':\n",
    "                if  (trade['type'] == 'short' and zscore < exit_threshold) or \\\n",
    "                    (trade['type'] == 'long' and zscore > -exit_threshold):\n",
    "                    \n",
    "                    trade['status'] = 'closed'\n",
    "                    trade['exit_date'] = current_date\n",
    "                    trade['exit_zscore'] = zscore\n",
    "                    \n",
    "                    for symbol in [symbol1, symbol2]:\n",
    "                        trades.append({\n",
    "                            'trade_id': trade['trade_id'],\n",
    "                            'symbol': symbol,\n",
    "                            'entry_date': trade['entry_date'],\n",
    "                            'entry_price': trade['entry_prices'][symbol]['price'],\n",
    "                            'exit_date': current_date,\n",
    "                            'exit_price': S1_test.iloc[i] if symbol == symbol1 else S2_test.iloc[i],\n",
    "                            'position_type': trade['entry_prices'][symbol]['type'],\n",
    "                            'paired_symbol': symbol2 if symbol == symbol1 else symbol1,\n",
    "                            'exit_type': 'target',\n",
    "                            'window': trade['window']\n",
    "                        })\n",
    "    \n",
    "    return trades, active_trades"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def backtest_pairs_sliding(price_matrix, initial_start_date, initial_end_date, base_output_path=base_output_path, output_filename=output_filename, window_shifts=window_shifts, p_threshold=p_threshold, shift_size=shift_size):\n",
    "    all_trades = []\n",
    "    ongoing_trades = []\n",
    "    \n",
    "    first_end = DATE_CONFIG['TRAIN_END']  \n",
    "    first_start = DATE_CONFIG['TRAIN_START'] \n",
    "    \n",
    "    print(f\"Price Matrix Zeitraum: {price_matrix.index.min()} bis {price_matrix.index.max()}\")\n",
    "    \n",
    "    for window_number in range(window_shifts):\n",
    "        # Calculate window dates\n",
    "        current_end = first_end + pd.DateOffset(months=window_number*shift_size)\n",
    "        current_start = first_start + pd.DateOffset(months=window_number*shift_size)\n",
    "        \n",
    "        print(f\"\\nAnalyse {window_number+1}/{window_shifts}\")\n",
    "        print(f\"Cluster-Fenster: {current_start} bis {current_end}\")\n",
    "        \n",
    "        # Load and prepare data for current window\n",
    "        cluster_data = price_matrix[(price_matrix.index >= current_start) & \n",
    "                                  (price_matrix.index <= current_end)].copy()\n",
    "        \n",
    "        if len(cluster_data) == 0:\n",
    "            print(f\"Keine Daten f端r Fenster {window_number+1}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Cluster Daten: {len(cluster_data)} Tage\")\n",
    "        \n",
    "        # Calculate returns and standardize\n",
    "        returns_matrix = cluster_data.pct_change()\n",
    "        returns_matrix = returns_matrix.dropna()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        standardized_returns = pd.DataFrame(\n",
    "            scaler.fit_transform(returns_matrix),\n",
    "            index=returns_matrix.index,\n",
    "            columns=returns_matrix.columns\n",
    "        )\n",
    "        \n",
    "        # Calculate features (same as before)\n",
    "        rolling_correlations = calculate_rolling_correlation(standardized_returns)\n",
    "        rolling_correlations = rolling_correlations.dropna()\n",
    "        rolling_betas = calculate_rolling_betas(standardized_returns)\n",
    "        rolling_betas = rolling_betas.dropna()\n",
    "        rolling_residuals_std = calculate_residuals_std(standardized_returns)\n",
    "        rolling_residuals_std = rolling_residuals_std.dropna()\n",
    "        average_spread_deviation = calculate_spread_deviation(standardized_returns)\n",
    "        average_spread_deviation = average_spread_deviation.dropna()\n",
    "\n",
    "        # Create and prepare feature dataframe\n",
    "        feature_df = pd.DataFrame({\n",
    "            'correlation': rolling_correlations.iloc[-1],\n",
    "            'beta': rolling_betas.iloc[-1],\n",
    "            'residuals_std': rolling_residuals_std.iloc[-1]\n",
    "        })\n",
    "        feature_df['target'] = average_spread_deviation\n",
    "        feature_df = feature_df.dropna()\n",
    "\n",
    "        # Additional features calculation (same as before)\n",
    "        volume_matrix = price_matrix.copy()\n",
    "        volume_features = calculate_volume_features(volume_matrix, rolling_correlations.columns)\n",
    "        roc_features = calculate_roc_features(cluster_data, volume_matrix, rolling_correlations.columns)\n",
    "        williams_r_features = calculate_williams_r_features(cluster_data, rolling_correlations.columns)\n",
    "        bollinger_bands_features = calculate_bollinger_bands_width(standardized_returns, rolling_correlations.columns)\n",
    "        macd_features = calculate_macd_features(cluster_data, rolling_correlations.columns)\n",
    "\n",
    "        # Combine all features\n",
    "        feature_df = pd.concat([\n",
    "            feature_df[['correlation', 'beta', 'residuals_std', 'target']],\n",
    "            volume_features,\n",
    "            roc_features,\n",
    "            williams_r_features,\n",
    "            bollinger_bands_features,\n",
    "            macd_features\n",
    "        ], axis=1)\n",
    "\n",
    "        # Prepare features for model\n",
    "        X = feature_df[feature_columns]\n",
    "        y = feature_df['target']\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_imputed = pd.DataFrame(\n",
    "            imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        best_params_grid_search = {\n",
    "            'learning_rate': 0.2,\n",
    "            'max_depth': 2,\n",
    "            'min_samples_leaf': 2,\n",
    "            'min_samples_split': 2,\n",
    "            'n_estimators': 300\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**best_params_grid_search, random_state=42)\n",
    "        model.fit(X_imputed, y)\n",
    "        \n",
    "        # Find cointegrated pairs\n",
    "        potential_pairs = []\n",
    "        for i in range(len(symbols)):\n",
    "            for j in range(i+1, len(symbols)):\n",
    "                symbol1, symbol2 = symbols[i], symbols[j]\n",
    "                series1 = cluster_data[symbol1]\n",
    "                series2 = cluster_data[symbol2]\n",
    "                \n",
    "                valid_idx = cluster_data.index[\n",
    "                    (~series1.isna()) & (~series2.isna())\n",
    "                ]\n",
    "                \n",
    "                if len(valid_idx) > 0:\n",
    "                    series1_clean = series1[valid_idx]\n",
    "                    series2_clean = series2[valid_idx]\n",
    "                    \n",
    "                    try:\n",
    "                        score, pvalue, _ = coint(series1_clean, series2_clean)\n",
    "                        if pvalue < p_threshold:\n",
    "                            pair_key = f\"{symbol1}_{symbol2}\"\n",
    "                            if pair_key in feature_df.index:\n",
    "                                potential_pairs.append({\n",
    "                                    'pair': (symbol1, symbol2),\n",
    "                                    'pvalue': pvalue,\n",
    "                                    'predicted_spread': model.predict([X_imputed.loc[pair_key]])[0]\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        \n",
    "        if not potential_pairs:\n",
    "            print(f\"Keine kointegrierten Paare gefunden f端r Fenster {window_number+1}\")\n",
    "            continue\n",
    "        \n",
    "        pairs_df = pd.DataFrame(potential_pairs)\n",
    "        pairs_df = pairs_df.sort_values('predicted_spread')\n",
    "        top_pairs = pairs_df['pair'].tolist()[:min_pairs]\n",
    "        \n",
    "        print(f\"Gefundene handelbare Paare f端r Fenster {window_number+1}: {len(top_pairs)}\")\n",
    "        \n",
    "        # Define trading period\n",
    "        trade_start = current_end\n",
    "        trade_end = trade_start + pd.DateOffset(months=1)\n",
    "        \n",
    "        print(f\"Trading-Zeitraum: {trade_start} bis {trade_end}\")\n",
    "        \n",
    "        trading_data = price_matrix[(price_matrix.index > trade_start) & \n",
    "                                  (price_matrix.index <= trade_end)].copy()\n",
    "        \n",
    "        print(f\"Trading Daten: {len(trading_data)} Tage\")\n",
    "        \n",
    "        if len(trading_data) == 0:\n",
    "            print(f\"Warnung: Kein Trading-Zeitraum verf端gbar nach {trade_start}\")\n",
    "            continue\n",
    "            \n",
    "        updated_ongoing_trades = []\n",
    "        closed_trade_ids = set()\n",
    "        \n",
    "        for open_trade in ongoing_trades:\n",
    "            symbol1, symbol2 = open_trade['symbol1'], open_trade['symbol2']\n",
    "            \n",
    "            if symbol1 in trading_data.columns and symbol2 in trading_data.columns:\n",
    "                still_active = True\n",
    "                \n",
    "                for idx, date in enumerate(trading_data.index):\n",
    "                    train_data = price_matrix[(price_matrix.index <= open_trade['entry_date'])]\n",
    "                    ratio_history = train_data[symbol1] / train_data[symbol2]\n",
    "                    \n",
    "                    if len(ratio_history) >= window2:\n",
    "                        ma = ratio_history.rolling(window=window2, center=False).mean().iloc[-1]\n",
    "                        std = ratio_history.rolling(window=window2, center=False).std().iloc[-1]\n",
    "                    else:\n",
    "                        ma = ratio_history.mean()\n",
    "                        std = ratio_history.std()\n",
    "                    \n",
    "                    if std == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    current_ratio = trading_data[symbol1].iloc[idx] / trading_data[symbol2].iloc[idx]\n",
    "                    current_zscore = (current_ratio - ma) / std\n",
    "                    \n",
    "                    if (open_trade['type'] == 'short' and current_zscore < 0.5) or \\\n",
    "                       (open_trade['type'] == 'long' and current_zscore > -0.5):\n",
    "                        \n",
    "                        open_trade['status'] = 'closed'\n",
    "                        open_trade['exit_date'] = date\n",
    "                        closed_trade_ids.add(open_trade['trade_id'])\n",
    "                        \n",
    "                        for symbol in [symbol1, symbol2]:\n",
    "                            all_trades.append({\n",
    "                                'trade_id': open_trade['trade_id'],\n",
    "                                'symbol': symbol,\n",
    "                                'entry_date': open_trade['entry_date'],\n",
    "                                'entry_price': open_trade['entry_prices'][symbol]['price'],\n",
    "                                'exit_date': date,\n",
    "                                'exit_price': trading_data.loc[date, symbol],\n",
    "                                'position_type': open_trade['entry_prices'][symbol]['type'],\n",
    "                                'paired_symbol': symbol2 if symbol == symbol1 else symbol1,\n",
    "                                'exit_type': 'target',\n",
    "                                'window': open_trade['window']\n",
    "                            })\n",
    "                        \n",
    "                        still_active = False\n",
    "                        break\n",
    "                \n",
    "                if still_active:\n",
    "                    updated_ongoing_trades.append(open_trade)\n",
    "        \n",
    "        ongoing_trades = updated_ongoing_trades\n",
    "        \n",
    "        # Execute trades\n",
    "        trade_count = 0\n",
    "        for pair in top_pairs:\n",
    "            symbol1, symbol2 = pair\n",
    "            \n",
    "            if symbol1 not in trading_data.columns or symbol2 not in trading_data.columns:\n",
    "                continue\n",
    "                \n",
    "            new_trades, active_new_trades = trade(\n",
    "                cluster_data[symbol1],\n",
    "                cluster_data[symbol2],\n",
    "                trading_data[symbol1],\n",
    "                trading_data[symbol2],\n",
    "                symbol1, symbol2,\n",
    "                window_number=window_number + 1\n",
    "            )\n",
    "            \n",
    "            if new_trades:\n",
    "                trade_count += len(new_trades) // 2\n",
    "                all_trades.extend(new_trades)\n",
    "            \n",
    "            ongoing_trades.extend(active_new_trades)\n",
    "        \n",
    "        print(f\"Neue geschlossene Trades in diesem Fenster: {trade_count}\")\n",
    "        print(f\"Aktuell offene Trades: {len(ongoing_trades)}\")\n",
    "    \n",
    "    trades_df = pd.DataFrame(all_trades)\n",
    "    \n",
    "    if len(trades_df) > 0:\n",
    "        full_output_path = f\"{base_output_path}{output_filename}\"\n",
    "        trades_df.to_parquet(full_output_path)\n",
    "        \n",
    "        print(\"\\nTrading Zusammenfassung:\")\n",
    "        print(f\"Gesamtanzahl Trades: {len(trades_df)}\")\n",
    "        print(f\"Unique Paare gehandelt: {len(trades_df[['symbol', 'paired_symbol']].drop_duplicates())}\")\n",
    "        print(f\"Zeitraum: {trades_df['entry_date'].min()} bis {trades_df['exit_date'].max()}\")\n",
    "        \n",
    "        print(\"\\nTrades pro Fenster:\")\n",
    "        print(trades_df['window'].value_counts().sort_index())\n",
    "    else:\n",
    "        print(\"Keine Trades generiert!\")\n",
    "        \n",
    "    return trades_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T09:31:32.749316Z",
     "start_time": "2025-03-01T08:58:30.988894Z"
    }
   },
   "source": [
    "trades_df = backtest_pairs_sliding(\n",
    "    price_matrix=price_matrix,\n",
    "    initial_start_date=DATE_CONFIG['TRAIN_START'],\n",
    "    initial_end_date=DATE_CONFIG['TRAIN_END'], \n",
    "    base_output_path=base_output_path,\n",
    "    output_filename=output_filename,\n",
    "    window_shifts=window_shifts,\n",
    "    p_threshold=p_threshold,\n",
    "    shift_size=shift_size\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price Matrix Zeitraum: 2021-02-02 00:00:00 bis 2024-12-31 00:00:00\n",
      "\n",
      "Analyse 1/12\n",
      "Cluster-Fenster: 2021-02-02 00:00:00 bis 2024-01-01 00:00:00\n",
      "Cluster Daten: 733 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 1: 20\n",
      "Trading-Zeitraum: 2024-01-01 00:00:00 bis 2024-02-01 00:00:00\n",
      "Trading Daten: 23 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 3\n",
      "Aktuell offene Trades: 101\n",
      "\n",
      "Analyse 2/12\n",
      "Cluster-Fenster: 2021-03-02 00:00:00 bis 2024-02-01 00:00:00\n",
      "Cluster Daten: 736 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 2: 20\n",
      "Trading-Zeitraum: 2024-02-01 00:00:00 bis 2024-03-01 00:00:00\n",
      "Trading Daten: 21 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 17\n",
      "Aktuell offene Trades: 98\n",
      "\n",
      "Analyse 3/12\n",
      "Cluster-Fenster: 2021-04-02 00:00:00 bis 2024-03-01 00:00:00\n",
      "Cluster Daten: 734 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 3: 20\n",
      "Trading-Zeitraum: 2024-03-01 00:00:00 bis 2024-04-01 00:00:00\n",
      "Trading Daten: 19 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 13\n",
      "Aktuell offene Trades: 184\n",
      "\n",
      "Analyse 4/12\n",
      "Cluster-Fenster: 2021-05-02 00:00:00 bis 2024-04-01 00:00:00\n",
      "Cluster Daten: 734 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 4: 20\n",
      "Trading-Zeitraum: 2024-04-01 00:00:00 bis 2024-05-01 00:00:00\n",
      "Trading Daten: 22 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 21\n",
      "Aktuell offene Trades: 234\n",
      "\n",
      "Analyse 5/12\n",
      "Cluster-Fenster: 2021-06-02 00:00:00 bis 2024-05-01 00:00:00\n",
      "Cluster Daten: 736 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 5: 20\n",
      "Trading-Zeitraum: 2024-05-01 00:00:00 bis 2024-06-01 00:00:00\n",
      "Trading Daten: 20 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 9\n",
      "Aktuell offene Trades: 226\n",
      "\n",
      "Analyse 6/12\n",
      "Cluster-Fenster: 2021-07-02 00:00:00 bis 2024-06-01 00:00:00\n",
      "Cluster Daten: 734 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 6: 20\n",
      "Trading-Zeitraum: 2024-06-01 00:00:00 bis 2024-07-01 00:00:00\n",
      "Trading Daten: 21 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 17\n",
      "Aktuell offene Trades: 258\n",
      "\n",
      "Analyse 7/12\n",
      "Cluster-Fenster: 2021-08-02 00:00:00 bis 2024-07-01 00:00:00\n",
      "Cluster Daten: 734 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 7: 20\n",
      "Trading-Zeitraum: 2024-07-01 00:00:00 bis 2024-08-01 00:00:00\n",
      "Trading Daten: 23 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 8\n",
      "Aktuell offene Trades: 290\n",
      "\n",
      "Analyse 8/12\n",
      "Cluster-Fenster: 2021-09-02 00:00:00 bis 2024-08-01 00:00:00\n",
      "Cluster Daten: 735 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 8: 20\n",
      "Trading-Zeitraum: 2024-08-01 00:00:00 bis 2024-09-01 00:00:00\n",
      "Trading Daten: 20 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 2\n",
      "Aktuell offene Trades: 346\n",
      "\n",
      "Analyse 9/12\n",
      "Cluster-Fenster: 2021-10-02 00:00:00 bis 2024-09-01 00:00:00\n",
      "Cluster Daten: 733 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 9: 20\n",
      "Trading-Zeitraum: 2024-09-01 00:00:00 bis 2024-10-01 00:00:00\n",
      "Trading Daten: 22 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 18\n",
      "Aktuell offene Trades: 395\n",
      "\n",
      "Analyse 10/12\n",
      "Cluster-Fenster: 2021-11-02 00:00:00 bis 2024-10-01 00:00:00\n",
      "Cluster Daten: 734 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 10: 20\n",
      "Trading-Zeitraum: 2024-10-01 00:00:00 bis 2024-11-01 00:00:00\n",
      "Trading Daten: 23 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 15\n",
      "Aktuell offene Trades: 422\n",
      "\n",
      "Analyse 11/12\n",
      "Cluster-Fenster: 2021-12-02 00:00:00 bis 2024-11-01 00:00:00\n",
      "Cluster Daten: 735 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 11: 20\n",
      "Trading-Zeitraum: 2024-11-01 00:00:00 bis 2024-12-01 00:00:00\n",
      "Trading Daten: 20 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 14\n",
      "Aktuell offene Trades: 509\n",
      "\n",
      "Analyse 12/12\n",
      "Cluster-Fenster: 2022-01-02 00:00:00 bis 2024-12-01 00:00:00\n",
      "Cluster Daten: 735 Tage\n",
      "Gefundene handelbare Paare f端r Fenster 12: 20\n",
      "Trading-Zeitraum: 2024-12-01 00:00:00 bis 2025-01-01 00:00:00\n",
      "Trading Daten: 20 Tage\n",
      "Neue geschlossene Trades in diesem Fenster: 1\n",
      "Aktuell offene Trades: 532\n",
      "\n",
      "Trading Zusammenfassung:\n",
      "Gesamtanzahl Trades: 1042\n",
      "Unique Paare gehandelt: 94\n",
      "Zeitraum: 2024-01-02 00:00:00 bis 2024-12-30 00:00:00\n",
      "\n",
      "Trades pro Fenster:\n",
      "window\n",
      "1     126\n",
      "2     112\n",
      "3      88\n",
      "4     126\n",
      "5      64\n",
      "6     156\n",
      "7     100\n",
      "8      72\n",
      "9      86\n",
      "10     52\n",
      "11     58\n",
      "12      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

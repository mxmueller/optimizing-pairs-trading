{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.644489Z",
     "start_time": "2025-03-01T08:58:21.641055Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.statistics.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.663991Z",
     "start_time": "2025-03-01T08:58:21.662032Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "p_threshold = 0.05\n",
    "min_pairs = 20\n",
    "\n",
    "window_shifts = 12\n",
    "shift_size = 1\n",
    "\n",
    "entry_threshold = 2.0\n",
    "exit_threshold = 0.5\n",
    "window1 = 5 \n",
    "window2 = 60\n",
    "\n",
    "learning_rate = 0.2\n",
    "max_depth = 2\n",
    "min_samples_leaf = 2\n",
    "min_samples_split = 2\n",
    "n_estimators = 300\n",
    "\n",
    "base_input_path = \"../../data/raw/\" \n",
    "input_filename = \"nasdaq_daily.parquet\" \n",
    "base_output_path = \"../../data/results/\" \n",
    "output_filename = \"GradientBoost_Z-Score_Sliding.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.702143Z",
     "start_time": "2025-03-01T08:58:21.699915Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "\n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "\n",
    "    symbols = price_matrix.columns.tolist()\n",
    "\n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "\n",
    "    return df, price_matrix, symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.767890Z",
     "start_time": "2025-03-01T08:58:21.736126Z"
    }
   },
   "outputs": [],
   "source": [
    "input_data_path = f\"{base_input_path}{input_filename}\"\n",
    "df, price_matrix, symbols = load_and_prepare_data(input_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.808731Z",
     "start_time": "2025-03-01T08:58:21.805228Z"
    }
   },
   "outputs": [],
   "source": [
    "returns_matrix = price_matrix.pct_change()\n",
    "returns_matrix = returns_matrix.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:21.846724Z",
     "start_time": "2025-03-01T08:58:21.843179Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_returns = pd.DataFrame(\n",
    "    scaler.fit_transform(returns_matrix),\n",
    "    index=returns_matrix.index,\n",
    "    columns=returns_matrix.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T08:58:26.183493Z",
     "start_time": "2025-03-01T08:58:21.880810Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_rolling_correlation(returns_df, window=250):\n",
    "    pairs_corr = {}\n",
    "    symbols = returns_df.columns\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        for j in range(i+1, len(symbols)):\n",
    "            sym1, sym2 = symbols[i], symbols[j]\n",
    "            pair_key = f\"{sym1}_{sym2}\"\n",
    "            \n",
    "            rolling_corr = returns_df[[sym1, sym2]].rolling(window=window)\\\n",
    "                          .corr()\\\n",
    "                          .unstack()[sym2][sym1]\n",
    "                          \n",
    "            pairs_corr[pair_key] = rolling_corr\n",
    "            \n",
    "    return pd.DataFrame(pairs_corr)\n",
    "\n",
    "rolling_correlations = calculate_rolling_correlation(standardized_returns)\n",
    "rolling_correlations = rolling_correlations.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_betas(returns_df, window=250):\n",
    "    pairs_beta = {}\n",
    "    symbols = returns_df.columns\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        for j in range(i+1, len(symbols)):\n",
    "            sym1, sym2 = symbols[i], symbols[j]\n",
    "            pair_key = f\"{sym1}_{sym2}\"\n",
    "            \n",
    "            x = returns_df[sym2]\n",
    "            y = returns_df[sym1]\n",
    "            \n",
    "            rolling_cov = y.rolling(window=window).cov(x)\n",
    "            rolling_var = x.rolling(window=window).var()\n",
    "            rolling_beta = rolling_cov / rolling_var\n",
    "            \n",
    "            pairs_beta[pair_key] = rolling_beta\n",
    "            \n",
    "    return pd.DataFrame(pairs_beta)\n",
    "\n",
    "rolling_betas = calculate_rolling_betas(standardized_returns)\n",
    "rolling_betas = rolling_betas.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_residuals_std(returns_df, window=250):\n",
    "    pairs_residuals = {}\n",
    "    symbols = returns_df.columns\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        for j in range(i+1, len(symbols)):\n",
    "            sym1, sym2 = symbols[i], symbols[j]\n",
    "            pair_key = f\"{sym1}_{sym2}\"\n",
    "            \n",
    "            x = returns_df[sym2].values\n",
    "            y = returns_df[sym1].values\n",
    "            \n",
    "            residuals = [np.nan] * len(returns_df)\n",
    "            for k in range(window, len(returns_df) + 1):\n",
    "                window_x = x[k-window:k]\n",
    "                window_y = y[k-window:k]\n",
    "                coeffs = np.polyfit(window_x, window_y, 1)\n",
    "                pred = np.polyval(coeffs, window_x)\n",
    "                window_residuals = window_y - pred\n",
    "                residuals[k-1] = np.std(window_residuals)\n",
    "                \n",
    "            pairs_residuals[pair_key] = residuals\n",
    "            \n",
    "    return pd.DataFrame(pairs_residuals, index=returns_df.index)\n",
    "\n",
    "rolling_residuals_std = calculate_residuals_std(standardized_returns)\n",
    "rolling_residuals_std = rolling_residuals_std.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spread_deviation(returns_df, window=250, holding_period=5, zscore_threshold=2):\n",
    "    pairs_spread_dev = {}\n",
    "    symbols = returns_df.columns\n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        for j in range(i+1, len(symbols)):\n",
    "            sym1, sym2 = symbols[i], symbols[j]\n",
    "            pair_key = f\"{sym1}_{sym2}\"\n",
    "            \n",
    "            spread = returns_df[sym1] - returns_df[sym2]\n",
    "            rolling_mean = spread.rolling(window=window).mean()\n",
    "            rolling_std = spread.rolling(window=window).std()\n",
    "            zscore = (spread - rolling_mean) / rolling_std\n",
    "            \n",
    "            entry_signals = abs(zscore) > zscore_threshold\n",
    "            spread_changes = []\n",
    "            \n",
    "            for k in range(window, len(returns_df) - holding_period):\n",
    "                if entry_signals.iloc[k]:\n",
    "                    spread_t0 = spread.iloc[k]\n",
    "                    spread_t1 = spread.iloc[k + holding_period]\n",
    "                    spread_change = abs(spread_t1 - spread_t0)\n",
    "                    spread_changes.append(spread_change)\n",
    "            \n",
    "            if spread_changes:\n",
    "                pairs_spread_dev[pair_key] = np.mean(spread_changes)\n",
    "            else:\n",
    "                pairs_spread_dev[pair_key] = np.nan\n",
    "                \n",
    "    return pd.Series(pairs_spread_dev)\n",
    "\n",
    "average_spread_deviation = calculate_spread_deviation(standardized_returns)\n",
    "average_spread_deviation = average_spread_deviation.dropna()\n",
    "\n",
    "print(f\"Anzahl der berechneten Paare: {len(average_spread_deviation)}\")\n",
    "print(\"\\nTop 5 Paare mit niedrigster Spread-Abweichung:\")\n",
    "print(average_spread_deviation.nsmallest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame({\n",
    "   'correlation': rolling_correlations.iloc[-1],\n",
    "   'beta': rolling_betas.iloc[-1],\n",
    "   'residuals_std': rolling_residuals_std.iloc[-1]\n",
    "})\n",
    "\n",
    "feature_df['target'] = average_spread_deviation\n",
    "feature_df = feature_df.dropna()\n",
    "\n",
    "volume_matrix = df.pivot(index='date', columns='symbol', values='volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_volume_features(volume_df, pairs, windows=[50, 100]):\n",
    "    features = {}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        pair_features = {}\n",
    "        \n",
    "        for window in windows:\n",
    "            vol1 = volume_df[sym1].rolling(window=window)\n",
    "            vol2 = volume_df[sym2].rolling(window=window)\n",
    "            \n",
    "            pair_features[f'vol_avg_{window}'] = ((vol1.mean() + vol2.mean()) / 2).iloc[-1]\n",
    "            pair_features[f'vol_std_{window}'] = ((vol1.std() + vol2.std()) / 2).iloc[-1]\n",
    "            pair_features[f'vol_change_{window}'] = ((volume_df[sym1].pct_change() + volume_df[sym2].pct_change()) / 2).rolling(window=window).mean().iloc[-1]\n",
    "        \n",
    "        features[pair] = pair_features\n",
    "    \n",
    "    return pd.DataFrame.from_dict(features, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_features = calculate_volume_features(volume_matrix, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df[['correlation', 'beta', 'residuals_std', 'target']],\n",
    "    volume_features,\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_features(price_df, volume_df, pairs, windows=[5, 20, 50]):\n",
    "    features = {}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        pair_features = {}\n",
    "        \n",
    "        for window in windows:\n",
    "            # Price ROC\n",
    "            price_roc_1 = price_df[sym1].pct_change(window)\n",
    "            price_roc_2 = price_df[sym2].pct_change(window)\n",
    "            \n",
    "            pair_features[f'ROC_SYM1_{window}d'] = price_roc_1.iloc[-1]\n",
    "            pair_features[f'ROC_SYM2_{window}d'] = price_roc_2.iloc[-1]\n",
    "            pair_features[f'ROC_diff_{window}d'] = price_roc_1.iloc[-1] - price_roc_2.iloc[-1]\n",
    "            \n",
    "            # Volume ROC\n",
    "            vol_roc_1 = volume_df[sym1].pct_change(window)\n",
    "            vol_roc_2 = volume_df[sym2].pct_change(window)\n",
    "            \n",
    "            pair_features[f'vol_ROC_SYM1_{window}d'] = vol_roc_1.iloc[-1]\n",
    "            pair_features[f'vol_ROC_SYM2_{window}d'] = vol_roc_2.iloc[-1]\n",
    "            pair_features[f'vol_ROC_diff_{window}d'] = vol_roc_1.iloc[-1] - vol_roc_2.iloc[-1]\n",
    "        \n",
    "        features[pair] = pair_features\n",
    "    \n",
    "    return pd.DataFrame.from_dict(features, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_features = calculate_roc_features(price_matrix, volume_matrix, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df,\n",
    "    roc_features\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_williams_r_features(price_df, pairs, windows=[10, 14, 28, 40]):\n",
    "   features = {}\n",
    "   \n",
    "   for pair in pairs:\n",
    "       sym1, sym2 = pair.split('_')\n",
    "       pair_features = {}\n",
    "       \n",
    "       for window in windows:\n",
    "           # Williams %R für Symbol 1\n",
    "           high1 = price_df[sym1].rolling(window=window).max()\n",
    "           low1 = price_df[sym1].rolling(window=window).min()\n",
    "           williams_r1 = ((high1 - price_df[sym1]) / (high1 - low1) * -100)\n",
    "           \n",
    "           # Williams %R für Symbol 2\n",
    "           high2 = price_df[sym2].rolling(window=window).max()\n",
    "           low2 = price_df[sym2].rolling(window=window).min()\n",
    "           williams_r2 = ((high2 - price_df[sym2]) / (high2 - low2) * -100)\n",
    "           \n",
    "           pair_features[f'WilliamsR_SYM1_{window}d'] = williams_r1.iloc[-1]\n",
    "           pair_features[f'WilliamsR_SYM2_{window}d'] = williams_r2.iloc[-1]\n",
    "           pair_features[f'WilliamsR_diff_{window}d'] = williams_r1.iloc[-1] - williams_r2.iloc[-1]\n",
    "       \n",
    "       features[pair] = pair_features\n",
    "   \n",
    "   return pd.DataFrame.from_dict(features, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "williams_r_features = calculate_williams_r_features(price_matrix, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df,\n",
    "    williams_r_features\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bollinger_bands_width(returns_df, pairs, windows=[5, 10, 20, 25, 50]):\n",
    "    features = {}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        pair_features = {}\n",
    "        \n",
    "        spread = returns_df[sym1] - returns_df[sym2]\n",
    "        \n",
    "        for window in windows:\n",
    "            sma = spread.rolling(window=window).mean()\n",
    "            std = spread.rolling(window=window).std()\n",
    "            \n",
    "            bb_width = (2 * std) / sma * 100\n",
    "            pair_features[f'BB_width_{window}d'] = bb_width.iloc[-1]\n",
    "        \n",
    "        features[pair] = pair_features\n",
    "    \n",
    "    return pd.DataFrame.from_dict(features, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollinger_bands_features = calculate_bollinger_bands_width(standardized_returns, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df,\n",
    "    bollinger_bands_features\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macd_features(price_df, pairs, params=[\n",
    "    {'fast': 12, 'slow': 26, 'signal': 9},\n",
    "    {'fast': 5, 'slow': 13, 'signal': 5}\n",
    "]):\n",
    "    features = {}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        sym1, sym2 = pair.split('_')\n",
    "        pair_features = {}\n",
    "        \n",
    "        for p in params:\n",
    "            period = f\"{p['fast']}_{p['slow']}_{p['signal']}\"\n",
    "            \n",
    "            for sym in [sym1, sym2]:\n",
    "                fast_ema = price_df[sym].ewm(span=p['fast']).mean()\n",
    "                slow_ema = price_df[sym].ewm(span=p['slow']).mean()\n",
    "                macd_line = fast_ema - slow_ema\n",
    "                signal_line = macd_line.ewm(span=p['signal']).mean()\n",
    "                histogram = macd_line - signal_line\n",
    "                \n",
    "                if sym == sym1:\n",
    "                    macd_line_1 = macd_line\n",
    "                    signal_line_1 = signal_line\n",
    "                    histogram_1 = histogram\n",
    "                else:\n",
    "                    macd_line_2 = macd_line\n",
    "                    signal_line_2 = signal_line\n",
    "                    histogram_2 = histogram\n",
    "            \n",
    "            pair_features[f'MACD_line_diff_{period}'] = (macd_line_1 - macd_line_2).iloc[-1]\n",
    "            pair_features[f'Signal_line_diff_{period}'] = (signal_line_1 - signal_line_2).iloc[-1]\n",
    "            pair_features[f'MACD_hist_diff_{period}'] = (histogram_1 - histogram_2).iloc[-1]\n",
    "            pair_features[f'MACD_cross_{period}_SYM1'] = 1 if macd_line_1.iloc[-1] > signal_line_1.iloc[-1] else 0\n",
    "            pair_features[f'MACD_cross_{period}_SYM2'] = 1 if macd_line_2.iloc[-1] > signal_line_2.iloc[-1] else 0\n",
    "        \n",
    "        features[pair] = pair_features\n",
    "    \n",
    "    return pd.DataFrame.from_dict(features, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_features = calculate_macd_features(price_matrix, rolling_correlations.columns)\n",
    "\n",
    "feature_df = pd.concat([\n",
    "    feature_df,\n",
    "    macd_features\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search \n",
    "```Python\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], \n",
    "    'learning_rate': [0.01, 0.1, 0.2], \n",
    "    'max_depth': [2, 3, 4], \n",
    "    'min_samples_split': [2, 4], \n",
    "    'min_samples_leaf': [1, 2] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(random_state=42), \n",
    "    param_grid=param_grid, \n",
    "    scoring='r2', \n",
    "    cv=5, # Important !\n",
    "    n_jobs=-1, \n",
    "    verbose=2 \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'correlation', 'beta', 'residuals_std',\n",
    "    'vol_change_50',\n",
    "    'ROC_SYM2_20d',\n",
    "    'vol_ROC_SYM1_50d',\n",
    "    'vol_ROC_SYM2_5d',\n",
    "    'ROC_SYM1_50d',\n",
    "    'vol_ROC_SYM2_20d',\n",
    "    'vol_ROC_SYM1_20d',\n",
    "    'WilliamsR_SYM1_40d',\n",
    "    'vol_ROC_SYM1_5d',\n",
    "    'ROC_SYM1_5d',\n",
    "    'vol_ROC_SYM2_50d',\n",
    "    'ROC_SYM2_50d',\n",
    "    'residuals_std',\n",
    "    'ROC_SYM1_20d',\n",
    "    'WilliamsR_SYM2_28d',\n",
    "    'ROC_SYM2_5d', \n",
    "    'WilliamsR_SYM1_28d',\n",
    "    'WilliamsR_SYM2_14d',\n",
    "    'beta', \n",
    "    'WilliamsR_SYM1_14d',\n",
    "    'vol_avg_100',\n",
    "    'MACD_line_diff_12_26_9',\n",
    "    'vol_avg_50',\n",
    "    'vol_ROC_diff_50d',\n",
    "    'WilliamsR_SYM2_40d',\n",
    "    'MACD_cross_12_26_9_SYM2',\n",
    "    'vol_change_100',\n",
    "    'MACD_hist_diff_12_26_9',\n",
    "    'vol_std_50'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = feature_df[feature_columns]\n",
    "y = feature_df['target']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "best_params_grid_search = {\n",
    "    'learning_rate': learning_rate, \n",
    "    'max_depth': max_depth, \n",
    "    'min_samples_leaf': min_samples_leaf, \n",
    "    'min_samples_split': min_samples_split, \n",
    "    'n_estimators': n_estimators\n",
    "}\n",
    "\n",
    "final_model_retrained = GradientBoostingRegressor(**best_params_grid_search, random_state=42) \n",
    "final_model_retrained.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_pred = final_model_retrained.predict(X_test)\n",
    "print(f'R2: {r2_score(y_test, y_pred):.3f}')\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': final_model_retrained.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "for idx, row in importance.head(30).iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': final_model_retrained.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance.head(30), x='importance', y='feature')\n",
    "plt.title('Top 10 Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk-Forward Evaluationfrom sklearn.impute import SimpleImputer  \n",
    "\n",
    "```\n",
    "walk_forward_dates = []\n",
    "current_train_start = DATE_CONFIG['TRAIN_START']\n",
    "current_train_end = DATE_CONFIG['TRAIN_END']\n",
    "step_size = timedelta(days=90)  \n",
    "while current_train_end <= DATE_CONFIG['TEST_END']:\n",
    "    walk_forward_dates.append({\n",
    "        'TRAIN_START': current_train_start,\n",
    "        'TRAIN_END': current_train_end\n",
    "    })\n",
    "    current_train_start += step_size\n",
    "    current_train_end += step_size\n",
    "\n",
    "walk_forward_results = []\n",
    "\n",
    "for wf_config in tqdm(walk_forward_dates, desc=\"Walk-Forward Iteration\"):\n",
    "    DATE_CONFIG['TRAIN_START'] = wf_config['TRAIN_START']\n",
    "    DATE_CONFIG['TRAIN_END'] = wf_config['TRAIN_END']\n",
    "\n",
    "    df, price_matrix, symbols = load_and_prepare_data(config['data']['raw_data_path'])\n",
    "    returns_matrix = price_matrix.pct_change().dropna()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized_returns = pd.DataFrame(\n",
    "        scaler.fit_transform(returns_matrix),\n",
    "        index=returns_matrix.index,\n",
    "        columns=returns_matrix.columns\n",
    "    )\n",
    "\n",
    "    # Rest der Feature-Berechnungen bleibt gleich\n",
    "    rolling_correlations = calculate_rolling_correlation(standardized_returns)\n",
    "    rolling_correlations = rolling_correlations.dropna()\n",
    "    rolling_betas = calculate_rolling_betas(standardized_returns)\n",
    "    rolling_betas = rolling_betas.dropna()\n",
    "    rolling_residuals_std = calculate_residuals_std(standardized_returns)\n",
    "    rolling_residuals_std = rolling_residuals_std.dropna()\n",
    "    average_spread_deviation = calculate_spread_deviation(standardized_returns)\n",
    "    average_spread_deviation = average_spread_deviation.dropna()\n",
    "\n",
    "    feature_df = pd.DataFrame({\n",
    "        'correlation': rolling_correlations.iloc[-1],\n",
    "        'beta': rolling_betas.iloc[-1],\n",
    "        'residuals_std': rolling_residuals_std.iloc[-1]\n",
    "    })\n",
    "    feature_df['target'] = average_spread_deviation\n",
    "    feature_df = feature_df.dropna()\n",
    "\n",
    "    volume_matrix = df.pivot(index='date', columns='symbol', values='volume')\n",
    "    volume_features = calculate_volume_features(volume_matrix, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df[['correlation', 'beta', 'residuals_std', 'target']], volume_features], axis=1)\n",
    "    roc_features = calculate_roc_features(price_matrix, volume_matrix, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df, roc_features], axis=1)\n",
    "    williams_r_features = calculate_williams_r_features(price_matrix, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df, williams_r_features], axis=1)\n",
    "    bollinger_bands_features = calculate_bollinger_bands_width(standardized_returns, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df, bollinger_bands_features], axis=1)\n",
    "    macd_features = calculate_macd_features(price_matrix, rolling_correlations.columns)\n",
    "    feature_df = pd.concat([feature_df, macd_features], axis=1)\n",
    "\n",
    "    X = feature_df[feature_columns]\n",
    "    y = feature_df['target']\n",
    "\n",
    "    # Hier fügen wir den Imputer ein\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    # Train-Test-Split mit imputierten Daten\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    best_params_grid_search = {'learning_rate': 0.2, 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2,\n",
    "                               'n_estimators': 300}\n",
    "    final_model_retrained = GradientBoostingRegressor(**best_params_grid_search, random_state=42)\n",
    "    final_model_retrained.fit(X_train, y_train)\n",
    "    y_pred = final_model_retrained.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        'TRAIN_START': str(DATE_CONFIG['TRAIN_START']),\n",
    "        'TRAIN_END': str(DATE_CONFIG['TRAIN_END']),\n",
    "        'R2': r2_score(y_test, y_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred)\n",
    "    }\n",
    "    walk_forward_results.append(results)\n",
    "\n",
    "walk_forward_df = pd.DataFrame(walk_forward_results)\n",
    "\n",
    "print(\"\\nWalk-Forward Validierungsergebnisse:\")\n",
    "print(walk_forward_df)\n",
    "\n",
    "average_metrics = walk_forward_df[['R2', 'MSE', 'MAE']].mean()\n",
    "print(\"\\nDurchschnittliche Metriken über alle Walk-Forward-Schritte:\")\n",
    "print(average_metrics)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(walk_forward_df['TRAIN_END'], walk_forward_df['R2'], label='R2')\n",
    "plt.plot(walk_forward_df['TRAIN_END'], walk_forward_df['MSE'], label='MSE')\n",
    "plt.plot(walk_forward_df['TRAIN_END'], walk_forward_df['MAE'], label='MAE')\n",
    "plt.xlabel('Train End Date')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Walk-Forward Validierung Metriken')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2, window_number, window1=window1, window2=window2):\n",
    "    trades = []\n",
    "    trade_id = 0\n",
    "    active_trades = []\n",
    "    \n",
    "    S1_full = pd.concat([S1_train, S1_test])\n",
    "    S2_full = pd.concat([S2_train, S2_test])\n",
    "    \n",
    "    ratios_full = S1_full / S2_full\n",
    "    test_start = S1_test.index[0]\n",
    "    \n",
    "    for i in range(len(S1_test)):\n",
    "        current_idx = S1_train.shape[0] + i\n",
    "        ratio_history = ratios_full.iloc[:current_idx+1]\n",
    "        \n",
    "        if len(ratio_history) < window2:\n",
    "            continue\n",
    "            \n",
    "        ratio_history_past = ratios_full.iloc[:current_idx] \n",
    "        ma2 = ratio_history_past.rolling(window=window2, center=False).mean().iloc[-1] \n",
    "        std = ratio_history_past.rolling(window=window2, center=False).std().iloc[-1]   \n",
    "        \n",
    "        if std == 0:\n",
    "            continue\n",
    "            \n",
    "        current_ratio = ratios_full.iloc[current_idx]\n",
    "        current_date = S1_test.index[i]\n",
    "        zscore = (current_ratio - ma2) / std\n",
    "        \n",
    "        if zscore > entry_threshold:\n",
    "            trade_entry = {\n",
    "                'trade_id': trade_id,\n",
    "                'symbol1': symbol1,\n",
    "                'symbol2': symbol2,\n",
    "                'entry_date': current_date,\n",
    "                'entry_zscore': zscore,\n",
    "                'window': window_number,\n",
    "                'status': 'active',\n",
    "                'type': 'short',\n",
    "                'entry_prices': {\n",
    "                    symbol1: {'price': S1_test.iloc[i], 'type': 'short'},\n",
    "                    symbol2: {'price': S2_test.iloc[i], 'type': 'long'}\n",
    "                }\n",
    "            }\n",
    "            active_trades.append(trade_entry)\n",
    "            trade_id += 1\n",
    "            \n",
    "        elif zscore < -entry_threshold:\n",
    "            trade_entry = {\n",
    "                'trade_id': trade_id,\n",
    "                'symbol1': symbol1,\n",
    "                'symbol2': symbol2,\n",
    "                'entry_date': current_date,\n",
    "                'entry_zscore': zscore,\n",
    "                'window': window_number,\n",
    "                'status': 'active',\n",
    "                'type': 'long',\n",
    "                'entry_prices': {\n",
    "                    symbol1: {'price': S1_test.iloc[i], 'type': 'long'},\n",
    "                    symbol2: {'price': S2_test.iloc[i], 'type': 'short'}\n",
    "                }\n",
    "            }\n",
    "            active_trades.append(trade_entry)\n",
    "            trade_id += 1\n",
    "        \n",
    "        for trade in active_trades:\n",
    "            if trade['status'] == 'active':\n",
    "                if  (trade['type'] == 'short' and zscore < exit_threshold) or \\\n",
    "                    (trade['type'] == 'long' and zscore > -exit_threshold):\n",
    "                    \n",
    "                    trade['status'] = 'closed'\n",
    "                    trade['exit_date'] = current_date\n",
    "                    trade['exit_zscore'] = zscore\n",
    "                    \n",
    "                    for symbol in [symbol1, symbol2]:\n",
    "                        trades.append({\n",
    "                            'trade_id': trade['trade_id'],\n",
    "                            'symbol': symbol,\n",
    "                            'entry_date': trade['entry_date'],\n",
    "                            'entry_price': trade['entry_prices'][symbol]['price'],\n",
    "                            'exit_date': current_date,\n",
    "                            'exit_price': S1_test.iloc[i] if symbol == symbol1 else S2_test.iloc[i],\n",
    "                            'position_type': trade['entry_prices'][symbol]['type'],\n",
    "                            'paired_symbol': symbol2 if symbol == symbol1 else symbol1,\n",
    "                            'exit_type': 'target',\n",
    "                            'window': trade['window']\n",
    "                        })\n",
    "    \n",
    "    return trades, active_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_pairs_sliding(price_matrix, initial_start_date, initial_end_date, base_output_path=base_output_path, output_filename=output_filename, window_shifts=window_shifts, p_threshold=p_threshold, shift_size=shift_size):\n",
    "    all_trades = []\n",
    "    ongoing_trades = []\n",
    "    \n",
    "    first_end = DATE_CONFIG['TRAIN_END']  \n",
    "    first_start = DATE_CONFIG['TRAIN_START'] \n",
    "    \n",
    "    print(f\"Price Matrix Zeitraum: {price_matrix.index.min()} bis {price_matrix.index.max()}\")\n",
    "    \n",
    "    for window_number in range(window_shifts):\n",
    "        # Calculate window dates\n",
    "        current_end = first_end + pd.DateOffset(months=window_number*shift_size)\n",
    "        current_start = first_start + pd.DateOffset(months=window_number*shift_size)\n",
    "        \n",
    "        print(f\"\\nAnalyse {window_number+1}/{window_shifts}\")\n",
    "        print(f\"Cluster-Fenster: {current_start} bis {current_end}\")\n",
    "        \n",
    "        # Load and prepare data for current window\n",
    "        cluster_data = price_matrix[(price_matrix.index >= current_start) & \n",
    "                                  (price_matrix.index <= current_end)].copy()\n",
    "        \n",
    "        if len(cluster_data) == 0:\n",
    "            print(f\"Keine Daten für Fenster {window_number+1}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Cluster Daten: {len(cluster_data)} Tage\")\n",
    "        \n",
    "        # Calculate returns and standardize\n",
    "        returns_matrix = cluster_data.pct_change()\n",
    "        returns_matrix = returns_matrix.dropna()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        standardized_returns = pd.DataFrame(\n",
    "            scaler.fit_transform(returns_matrix),\n",
    "            index=returns_matrix.index,\n",
    "            columns=returns_matrix.columns\n",
    "        )\n",
    "        \n",
    "        # Calculate features (same as before)\n",
    "        rolling_correlations = calculate_rolling_correlation(standardized_returns)\n",
    "        rolling_correlations = rolling_correlations.dropna()\n",
    "        rolling_betas = calculate_rolling_betas(standardized_returns)\n",
    "        rolling_betas = rolling_betas.dropna()\n",
    "        rolling_residuals_std = calculate_residuals_std(standardized_returns)\n",
    "        rolling_residuals_std = rolling_residuals_std.dropna()\n",
    "        average_spread_deviation = calculate_spread_deviation(standardized_returns)\n",
    "        average_spread_deviation = average_spread_deviation.dropna()\n",
    "\n",
    "        # Create and prepare feature dataframe\n",
    "        feature_df = pd.DataFrame({\n",
    "            'correlation': rolling_correlations.iloc[-1],\n",
    "            'beta': rolling_betas.iloc[-1],\n",
    "            'residuals_std': rolling_residuals_std.iloc[-1]\n",
    "        })\n",
    "        feature_df['target'] = average_spread_deviation\n",
    "        feature_df = feature_df.dropna()\n",
    "\n",
    "        # Additional features calculation (same as before)\n",
    "        volume_matrix = price_matrix.copy()\n",
    "        volume_features = calculate_volume_features(volume_matrix, rolling_correlations.columns)\n",
    "        roc_features = calculate_roc_features(cluster_data, volume_matrix, rolling_correlations.columns)\n",
    "        williams_r_features = calculate_williams_r_features(cluster_data, rolling_correlations.columns)\n",
    "        bollinger_bands_features = calculate_bollinger_bands_width(standardized_returns, rolling_correlations.columns)\n",
    "        macd_features = calculate_macd_features(cluster_data, rolling_correlations.columns)\n",
    "\n",
    "        # Combine all features\n",
    "        feature_df = pd.concat([\n",
    "            feature_df[['correlation', 'beta', 'residuals_std', 'target']],\n",
    "            volume_features,\n",
    "            roc_features,\n",
    "            williams_r_features,\n",
    "            bollinger_bands_features,\n",
    "            macd_features\n",
    "        ], axis=1)\n",
    "\n",
    "        # Prepare features for model\n",
    "        X = feature_df[feature_columns]\n",
    "        y = feature_df['target']\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_imputed = pd.DataFrame(\n",
    "            imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        best_params_grid_search = {\n",
    "            'learning_rate': 0.2,\n",
    "            'max_depth': 2,\n",
    "            'min_samples_leaf': 2,\n",
    "            'min_samples_split': 2,\n",
    "            'n_estimators': 300\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**best_params_grid_search, random_state=42)\n",
    "        model.fit(X_imputed, y)\n",
    "        \n",
    "        # Find cointegrated pairs\n",
    "        potential_pairs = []\n",
    "        for i in range(len(symbols)):\n",
    "            for j in range(i+1, len(symbols)):\n",
    "                symbol1, symbol2 = symbols[i], symbols[j]\n",
    "                series1 = cluster_data[symbol1]\n",
    "                series2 = cluster_data[symbol2]\n",
    "                \n",
    "                valid_idx = cluster_data.index[\n",
    "                    (~series1.isna()) & (~series2.isna())\n",
    "                ]\n",
    "                \n",
    "                if len(valid_idx) > 0:\n",
    "                    series1_clean = series1[valid_idx]\n",
    "                    series2_clean = series2[valid_idx]\n",
    "                    \n",
    "                    try:\n",
    "                        score, pvalue, _ = coint(series1_clean, series2_clean)\n",
    "                        if pvalue < p_threshold:\n",
    "                            pair_key = f\"{symbol1}_{symbol2}\"\n",
    "                            if pair_key in feature_df.index:\n",
    "                                potential_pairs.append({\n",
    "                                    'pair': (symbol1, symbol2),\n",
    "                                    'pvalue': pvalue,\n",
    "                                    'predicted_spread': model.predict([X_imputed.loc[pair_key]])[0]\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        \n",
    "        if not potential_pairs:\n",
    "            print(f\"Keine kointegrierten Paare gefunden für Fenster {window_number+1}\")\n",
    "            continue\n",
    "        \n",
    "        pairs_df = pd.DataFrame(potential_pairs)\n",
    "        pairs_df = pairs_df.sort_values('predicted_spread')\n",
    "        top_pairs = pairs_df['pair'].tolist()[:min_pairs]\n",
    "        \n",
    "        print(f\"Gefundene handelbare Paare für Fenster {window_number+1}: {len(top_pairs)}\")\n",
    "        \n",
    "        # Define trading period\n",
    "        trade_start = current_end\n",
    "        trade_end = trade_start + pd.DateOffset(months=1)\n",
    "        \n",
    "        print(f\"Trading-Zeitraum: {trade_start} bis {trade_end}\")\n",
    "        \n",
    "        trading_data = price_matrix[(price_matrix.index > trade_start) & \n",
    "                                  (price_matrix.index <= trade_end)].copy()\n",
    "        \n",
    "        print(f\"Trading Daten: {len(trading_data)} Tage\")\n",
    "        \n",
    "        if len(trading_data) == 0:\n",
    "            print(f\"Warnung: Kein Trading-Zeitraum verfügbar nach {trade_start}\")\n",
    "            continue\n",
    "            \n",
    "        updated_ongoing_trades = []\n",
    "        closed_trade_ids = set()\n",
    "        \n",
    "        for open_trade in ongoing_trades:\n",
    "            symbol1, symbol2 = open_trade['symbol1'], open_trade['symbol2']\n",
    "            \n",
    "            if symbol1 in trading_data.columns and symbol2 in trading_data.columns:\n",
    "                still_active = True\n",
    "                \n",
    "                for idx, date in enumerate(trading_data.index):\n",
    "                    train_data = price_matrix[(price_matrix.index <= open_trade['entry_date'])]\n",
    "                    ratio_history = train_data[symbol1] / train_data[symbol2]\n",
    "                    \n",
    "                    if len(ratio_history) >= window2:\n",
    "                        ma = ratio_history.rolling(window=window2, center=False).mean().iloc[-1]\n",
    "                        std = ratio_history.rolling(window=window2, center=False).std().iloc[-1]\n",
    "                    else:\n",
    "                        ma = ratio_history.mean()\n",
    "                        std = ratio_history.std()\n",
    "                    \n",
    "                    if std == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    current_ratio = trading_data[symbol1].iloc[idx] / trading_data[symbol2].iloc[idx]\n",
    "                    current_zscore = (current_ratio - ma) / std\n",
    "                    \n",
    "                    if (open_trade['type'] == 'short' and current_zscore < 0.5) or \\\n",
    "                       (open_trade['type'] == 'long' and current_zscore > -0.5):\n",
    "                        \n",
    "                        open_trade['status'] = 'closed'\n",
    "                        open_trade['exit_date'] = date\n",
    "                        closed_trade_ids.add(open_trade['trade_id'])\n",
    "                        \n",
    "                        for symbol in [symbol1, symbol2]:\n",
    "                            all_trades.append({\n",
    "                                'trade_id': open_trade['trade_id'],\n",
    "                                'symbol': symbol,\n",
    "                                'entry_date': open_trade['entry_date'],\n",
    "                                'entry_price': open_trade['entry_prices'][symbol]['price'],\n",
    "                                'exit_date': date,\n",
    "                                'exit_price': trading_data.loc[date, symbol],\n",
    "                                'position_type': open_trade['entry_prices'][symbol]['type'],\n",
    "                                'paired_symbol': symbol2 if symbol == symbol1 else symbol1,\n",
    "                                'exit_type': 'target',\n",
    "                                'window': open_trade['window']\n",
    "                            })\n",
    "                        \n",
    "                        still_active = False\n",
    "                        break\n",
    "                \n",
    "                if still_active:\n",
    "                    updated_ongoing_trades.append(open_trade)\n",
    "        \n",
    "        ongoing_trades = updated_ongoing_trades\n",
    "        \n",
    "        # Execute trades\n",
    "        trade_count = 0\n",
    "        for pair in top_pairs:\n",
    "            symbol1, symbol2 = pair\n",
    "            \n",
    "            if symbol1 not in trading_data.columns or symbol2 not in trading_data.columns:\n",
    "                continue\n",
    "                \n",
    "            new_trades, active_new_trades = trade(\n",
    "                cluster_data[symbol1],\n",
    "                cluster_data[symbol2],\n",
    "                trading_data[symbol1],\n",
    "                trading_data[symbol2],\n",
    "                symbol1, symbol2,\n",
    "                window_number=window_number + 1\n",
    "            )\n",
    "            \n",
    "            if new_trades:\n",
    "                trade_count += len(new_trades) // 2\n",
    "                all_trades.extend(new_trades)\n",
    "            \n",
    "            ongoing_trades.extend(active_new_trades)\n",
    "        \n",
    "        print(f\"Neue geschlossene Trades in diesem Fenster: {trade_count}\")\n",
    "        print(f\"Aktuell offene Trades: {len(ongoing_trades)}\")\n",
    "    \n",
    "    trades_df = pd.DataFrame(all_trades)\n",
    "    \n",
    "    if len(trades_df) > 0:\n",
    "        full_output_path = f\"{base_output_path}{output_filename}\"\n",
    "        trades_df.to_parquet(full_output_path)\n",
    "        \n",
    "        print(\"\\nTrading Zusammenfassung:\")\n",
    "        print(f\"Gesamtanzahl Trades: {len(trades_df)}\")\n",
    "        print(f\"Unique Paare gehandelt: {len(trades_df[['symbol', 'paired_symbol']].drop_duplicates())}\")\n",
    "        print(f\"Zeitraum: {trades_df['entry_date'].min()} bis {trades_df['exit_date'].max()}\")\n",
    "        \n",
    "        print(\"\\nTrades pro Fenster:\")\n",
    "        print(trades_df['window'].value_counts().sort_index())\n",
    "    else:\n",
    "        print(\"Keine Trades generiert!\")\n",
    "        \n",
    "    return trades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T09:31:32.749316Z",
     "start_time": "2025-03-01T08:58:30.988894Z"
    }
   },
   "outputs": [],
   "source": [
    "trades_df = backtest_pairs_sliding(\n",
    "    price_matrix=price_matrix,\n",
    "    initial_start_date=DATE_CONFIG['TRAIN_START'],\n",
    "    initial_end_date=DATE_CONFIG['TRAIN_END'], \n",
    "    base_output_path=base_output_path,\n",
    "    output_filename=output_filename,\n",
    "    window_shifts=window_shifts,\n",
    "    p_threshold=p_threshold,\n",
    "    shift_size=shift_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

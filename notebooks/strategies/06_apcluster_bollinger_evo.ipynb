{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install kneed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.analysis.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Basic configuration\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "    \n",
    "    symbols = price_matrix.columns.tolist()\n",
    "    \n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "    \n",
    "    return price_matrix, symbols"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loade Price Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import toml\n",
    "\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "price_matrix, symbols = load_and_prepare_data(config['data']['raw_data_path'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\text{Rendite} = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{P_t - P_{t-1}}{P_{t-1}} \\times \\text{Trading days per year} $$\n",
    "\n",
    "\n",
    "$$ \\text{Volatilität} = \\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} \\left( \\frac{P_t - P_{t-1}}{P_{t-1}} - \\mu \\right)^2} \\times \\sqrt{\\text{Trading days per year}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_metrics(price_matrix):\n",
    "    returns = price_matrix.pct_change().mean() * DATE_CONFIG['TRADING_DAYS_PER_YEAR']\n",
    "    metrics = pd.DataFrame(returns, columns=['returns'])\n",
    "    metrics['volatility'] = price_matrix.pct_change().std() * np.sqrt(DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "    return metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "metrics = calculate_metrics(price_matrix)\n",
    "print(\"Erste 5 Zeilen der Metriken:\")\n",
    "print(metrics.head())\n",
    "print(\"\\nBeschreibung der Metriken:\")\n",
    "print(metrics.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale Transform\n",
    "The StandardScaler transforms our features (returns and volatility) to have zero mean and unit variance, which eliminates the scale difference between our variables and prevents higher magnitude features from dominating. This standardization is crucial for many machine learning algorithms as it ensures that all features contribute equally to the model and helps prevent numerical instabilities during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def scale_metrics(metrics):\n",
    "    scaler = StandardScaler()\n",
    "    scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(metrics),\n",
    "        columns=metrics.columns,\n",
    "        index=metrics.index\n",
    "    )\n",
    "    return scaled"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X = scale_metrics(metrics)\n",
    "print(\"Erste 5 Zeilen der skalierten Daten:\")\n",
    "print(X.head())\n",
    "print(\"\\nBeschreibung der skalierten Daten:\")\n",
    "print(X.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affinity Propagation Clustering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ap = AffinityPropagation()\n",
    "ap.fit(X)\n",
    "labels1 = ap.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(X.iloc[:,0], X.iloc[:,1], c=labels1, cmap='rainbow')\n",
    "ax.set_title('Affinity Propagation Clustering Results')\n",
    "ax.set_xlabel('Mean Return')\n",
    "ax.set_ylabel('Volatility')\n",
    "plt.colorbar(scatter)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_top_pairs(X, ap, price_matrix, min_pairs=20, p_threshold=0.05):\n",
    "    scores = []\n",
    "    \n",
    "    for cluster_id in np.unique(ap.labels_):\n",
    "        cluster_mask = ap.labels_ == cluster_id\n",
    "        cluster_symbols = X.index[cluster_mask]\n",
    "        center = X.iloc[ap.cluster_centers_indices_[cluster_id]]\n",
    "        \n",
    "        for i in range(len(cluster_symbols)):\n",
    "            for j in range(i+1, len(cluster_symbols)):\n",
    "                symbol1, symbol2 = cluster_symbols[i], cluster_symbols[j]\n",
    "                \n",
    "                dist1 = np.linalg.norm(X.loc[symbol1] - center)\n",
    "                dist2 = np.linalg.norm(X.loc[symbol2] - center)\n",
    "                center_dist = (dist1 + dist2) / 2\n",
    "                \n",
    "                profile_diff = np.linalg.norm(X.loc[symbol1] - X.loc[symbol2])\n",
    "                \n",
    "                series1 = price_matrix[symbol1]\n",
    "                series2 = price_matrix[symbol2]\n",
    "                score, pvalue, _ = coint(series1, series2)\n",
    "                \n",
    "                if pvalue < p_threshold:\n",
    "                    scores.append({\n",
    "                        'pair': (symbol1, symbol2),\n",
    "                        'center_dist': center_dist,\n",
    "                        'profile_diff': profile_diff,\n",
    "                        'pvalue': pvalue,\n",
    "                        'cluster': cluster_id\n",
    "                    })\n",
    "\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "\n",
    "    scores_df['center_dist_norm'] = (scores_df['center_dist'] - scores_df['center_dist'].min()) / \\\n",
    "                                   (scores_df['center_dist'].max() - scores_df['center_dist'].min())\n",
    "    scores_df['profile_diff_norm'] = (scores_df['profile_diff'] - scores_df['profile_diff'].min()) / \\\n",
    "                                    (scores_df['profile_diff'].max() - scores_df['profile_diff'].min())\n",
    "\n",
    "    scores_df['combined_score'] = 0.6 * scores_df['center_dist_norm'] + \\\n",
    "                                 0.4 * scores_df['profile_diff_norm']\n",
    "    \n",
    "    scores_df = scores_df.sort_values('combined_score')\n",
    "    \n",
    "    while len(scores_df) < min_pairs and p_threshold < 0.1:\n",
    "        p_threshold += 0.05\n",
    "        scores = []\n",
    "        for cluster_id in np.unique(ap.labels_):\n",
    "            cluster_mask = ap.labels_ == cluster_id\n",
    "            cluster_symbols = X.index[cluster_mask]\n",
    "            center = X.iloc[ap.cluster_centers_indices_[cluster_id]]\n",
    "            \n",
    "            for i in range(len(cluster_symbols)):\n",
    "                for j in range(i+1, len(cluster_symbols)):\n",
    "                    symbol1, symbol2 = cluster_symbols[i], cluster_symbols[j]\n",
    "                    dist1 = np.linalg.norm(X.loc[symbol1] - center)\n",
    "                    dist2 = np.linalg.norm(X.loc[symbol2] - center)\n",
    "                    center_dist = (dist1 + dist2) / 2\n",
    "                    profile_diff = np.linalg.norm(X.loc[symbol1] - X.loc[symbol2])\n",
    "                    score, pvalue, _ = coint(series1, series2)\n",
    "                    \n",
    "                    if pvalue < p_threshold:\n",
    "                        scores.append({\n",
    "                            'pair': (symbol1, symbol2),\n",
    "                            'center_dist': center_dist,\n",
    "                            'profile_diff': profile_diff,\n",
    "                            'pvalue': pvalue,\n",
    "                            'cluster': cluster_id\n",
    "                        })\n",
    "        \n",
    "        scores_df = pd.DataFrame(scores)\n",
    "        if len(scores) > 0:\n",
    "            scores_df['center_dist_norm'] = (scores_df['center_dist'] - scores_df['center_dist'].min()) / \\\n",
    "                                          (scores_df['center_dist'].max() - scores_df['center_dist'].min())\n",
    "            scores_df['profile_diff_norm'] = (scores_df['profile_diff'] - scores_df['profile_diff'].min()) / \\\n",
    "                                           (scores_df['profile_diff'].max() - scores_df['profile_diff'].min())\n",
    "            scores_df['combined_score'] = 0.6 * scores_df['center_dist_norm'] + \\\n",
    "                                        0.4 * scores_df['profile_diff_norm']\n",
    "            scores_df = scores_df.sort_values('combined_score')\n",
    "    \n",
    "    print(f\"Found {len(scores_df)} pairs with p-value < {p_threshold}\")\n",
    "    return scores_df['pair'].tolist()[:min_pairs], scores_df[:min_pairs]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "top_pairs, scores_df = get_top_pairs(X, ap, price_matrix, min_pairs=20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from itertools import cycle\n",
    "\n",
    "#Extract the cluster centers and labels\n",
    "cci = ap.cluster_centers_indices_\n",
    "labels2 = ap.labels_\n",
    "\n",
    "#Print their number\n",
    "clusters = len(cci)\n",
    "print('The number of clusters is:',clusters)\n",
    "\n",
    "#Plot the results\n",
    "X_ap = np.asarray(X)\n",
    "plt.close('all')\n",
    "plt.figure(1)\n",
    "plt.clf\n",
    "fig=plt.figure(figsize=(15,10))\n",
    "colors = cycle('cmykrgbcmykrgbcmykrgbcmykrgb')\n",
    "for k, col in zip(range(clusters),colors):\n",
    "    cluster_members = labels2 == k\n",
    "    cluster_center = X_ap[cci[k]]\n",
    "    plt.plot(X_ap[cluster_members, 0], X_ap[cluster_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=12)\n",
    "    for x in X_ap[cluster_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.title('Affinity Propagation Clustering Results with Connections')\n",
    "plt.xlabel('Mean Return')\n",
    "plt.ylabel('Volatility')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "clustered_series_ap = pd.Series(index=X.index, data=ap.labels_.flatten())\n",
    "\n",
    "cluster_size_limit = 1000\n",
    "counts = clustered_series_ap.value_counts()\n",
    "ticker_count = counts[(counts>1) & (counts<=cluster_size_limit)]\n",
    "print(\"Number of clusters:\", len(ticker_count))\n",
    "print(\"Number of Pairs:\", (ticker_count*(ticker_count-1)).sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_pairs_with_clusters = []\n",
    "\n",
    "for cluster_id in np.unique(ap.labels_):\n",
    "    cluster_mask = ap.labels_ == cluster_id\n",
    "    cluster_symbols = X.index[cluster_mask]\n",
    "    \n",
    "    if len(cluster_symbols) > 1:\n",
    "        cluster_prices = price_matrix[cluster_symbols]\n",
    "        score_matrix, pvalue_matrix, pairs, _ = analyze_pairs(\n",
    "            cluster_prices,\n",
    "            pvalue_threshold=0.05\n",
    "        )\n",
    "        \n",
    "        if len(pairs) > 0:\n",
    "            for pair in pairs:\n",
    "                all_pairs_with_clusters.append({\n",
    "                    'pair': pair,\n",
    "                    'cluster': cluster_id\n",
    "                })\n",
    "            print(f\"\\nCluster {cluster_id} pairs:\")\n",
    "            for pair in pairs:\n",
    "                print(f\"{pair[0]} - {pair[1]}\")\n",
    "                \n",
    "            plot_cointegration_heatmap(pvalue_matrix, cluster_symbols)\n",
    "\n",
    "all_pairs = [item['pair'] for item in all_pairs_with_clusters]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "stocks = np.unique([stock for pair in top_pairs for stock in pair])\n",
    "X_data = pd.DataFrame(index=X.index, data=X).T  \n",
    "in_pairs_series = pd.Series(index=stocks, data=[ap.labels_[list(X.index).index(stock)] for stock in stocks])\n",
    "X_pairs = X_data.T.loc[stocks]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "X_tsne = TSNE(learning_rate=30, perplexity=5, random_state=42, n_jobs=-1).fit_transform(X_pairs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize=(20,12), facecolor='white')\n",
    "plt.clf()\n",
    "plt.gca().set_facecolor('#f8f9fa')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "for pair in top_pairs:\n",
    "    cluster = scores_df[scores_df['pair'] == pair]['cluster'].values[0]\n",
    "    loc1 = X_pairs.index.get_loc(pair[0])\n",
    "    loc2 = X_pairs.index.get_loc(pair[1])\n",
    "    x1, y1 = X_tsne[loc1, :]\n",
    "    x2, y2 = X_tsne[loc2, :]\n",
    "    plt.plot([x1, x2], [y1, y2], '-', alpha=0.4, linewidth=1.5, color='#4a90e2')\n",
    "\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                     s=300,\n",
    "                     alpha=0.7,\n",
    "                     c=in_pairs_series.values,\n",
    "                     cmap='tab20',\n",
    "                     edgecolor='white',\n",
    "                     linewidth=2)\n",
    "\n",
    "for x, y, name in zip(X_tsne[:,0], X_tsne[:,1], X_pairs.index):\n",
    "    plt.annotate(name,\n",
    "                (x,y),\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(0,10),\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=11,\n",
    "                fontweight='bold',\n",
    "                bbox=dict(facecolor='white', \n",
    "                         edgecolor='none',\n",
    "                         alpha=0.7,\n",
    "                         pad=1))\n",
    "\n",
    "plt.title('Stock Pairs Clustering Visualization (Same Cluster Only)', \n",
    "          fontsize=16, \n",
    "          pad=20,\n",
    "          fontweight='bold')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2, cluster_timestamp, window=50, hr_window=25, hr_recalc=3, std_dev=1.5):\n",
    "    trades = []\n",
    "    trade_id = 0\n",
    "    active_trades = []  # Liste statt einzelner Position\n",
    "    \n",
    "    S1_full = pd.concat([S1_train, S1_test])\n",
    "    S2_full = pd.concat([S2_train, S2_test])\n",
    "    \n",
    "    days_since_recalc = 0\n",
    "    current_hedge_ratio = None\n",
    "    \n",
    "    for i in range(len(S1_test)):\n",
    "        current_idx = S1_train.shape[0] + i\n",
    "        current_date = S1_test.index[i]\n",
    "        \n",
    "        if days_since_recalc >= hr_recalc or current_hedge_ratio is None:\n",
    "            if current_idx >= hr_window:\n",
    "                hr_data_1 = S1_full.iloc[current_idx-hr_window:current_idx]\n",
    "                hr_data_2 = S2_full.iloc[current_idx-hr_window:current_idx]\n",
    "                model = sm.OLS(hr_data_1, hr_data_2)\n",
    "                current_hedge_ratio = model.fit().params[0]\n",
    "                days_since_recalc = 0\n",
    "        \n",
    "        days_since_recalc += 1\n",
    "        \n",
    "        if current_hedge_ratio is None:\n",
    "            continue\n",
    "            \n",
    "        spread_full = S1_full - (S2_full * current_hedge_ratio)\n",
    "        spread_window = spread_full.iloc[:current_idx+1]\n",
    "        \n",
    "        if len(spread_window) < window:\n",
    "            continue\n",
    "            \n",
    "        rolling_mean = spread_window.rolling(window=window).mean().iloc[-1]\n",
    "        rolling_std = spread_window.rolling(window=window).std().iloc[-1]\n",
    "        \n",
    "        if pd.isna(rolling_std) or rolling_std == 0:\n",
    "            continue\n",
    "            \n",
    "        upper_band = rolling_mean + (rolling_std * std_dev)\n",
    "        lower_band = rolling_mean - (rolling_std * std_dev)\n",
    "        \n",
    "        current_spread = spread_full.iloc[current_idx]\n",
    "\n",
    "        # Neue Short Position eröffnen wenn Spread über Upper Band\n",
    "        if current_spread > upper_band:\n",
    "            trade_entry = {\n",
    "                'trade_id': trade_id,\n",
    "                'entry_date': current_date,\n",
    "                'type': 'short',\n",
    "                'status': 'active',\n",
    "                'entry_prices': {\n",
    "                    symbol1: {'price': S1_test.iloc[i], 'type': 'short'},\n",
    "                    symbol2: {'price': S2_test.iloc[i], 'type': 'long'}\n",
    "                },\n",
    "                'hedge_ratio': current_hedge_ratio\n",
    "            }\n",
    "            active_trades.append(trade_entry)\n",
    "            trade_id += 1\n",
    "            \n",
    "        # Neue Long Position eröffnen wenn Spread unter Lower Band    \n",
    "        elif current_spread < lower_band:\n",
    "            trade_entry = {\n",
    "                'trade_id': trade_id,\n",
    "                'entry_date': current_date,\n",
    "                'type': 'long', \n",
    "                'status': 'active',\n",
    "                'entry_prices': {\n",
    "                    symbol1: {'price': S1_test.iloc[i], 'type': 'long'},\n",
    "                    symbol2: {'price': S2_test.iloc[i], 'type': 'short'}\n",
    "                },\n",
    "                'hedge_ratio': current_hedge_ratio\n",
    "            }\n",
    "            active_trades.append(trade_entry)\n",
    "            trade_id += 1\n",
    "\n",
    "        # Prüfe alle aktiven Trades auf Schließung\n",
    "        for trade in active_trades:\n",
    "            if trade['status'] == 'active':\n",
    "                if (trade['type'] == 'short' and current_spread < rolling_mean) or \\\n",
    "                   (trade['type'] == 'long' and current_spread > rolling_mean):\n",
    "                    \n",
    "                    trade['status'] = 'closed'\n",
    "                    \n",
    "                    for symbol in [symbol1, symbol2]:\n",
    "                        trades.append({\n",
    "                            'trade_id': trade['trade_id'],\n",
    "                            'symbol': symbol,\n",
    "                            'entry_date': trade['entry_date'],\n",
    "                            'entry_price': trade['entry_prices'][symbol]['price'],\n",
    "                            'exit_date': current_date,\n",
    "                            'exit_price': S1_test.iloc[i] if symbol == symbol1 else S2_test.iloc[i],\n",
    "                            'position_type': trade['entry_prices'][symbol]['type'],\n",
    "                            'paired_symbol': symbol2 if symbol == symbol1 else symbol1,\n",
    "                            'exit_type': 'target',\n",
    "                            'cluster_timestamp': cluster_timestamp,\n",
    "                            'hedge_ratio': trade['hedge_ratio']\n",
    "                        })\n",
    "\n",
    "\n",
    "    return trades, [t for t in active_trades if t['status'] == 'active']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def zscore(series):\n",
    "    return (series - series.mean()) / np.std(series)\n",
    "\n",
    "def calculate_spread(data, symbol1, symbol2, start_date=None, end_date=None):\n",
    "    if start_date:\n",
    "        mask = (data.index >= start_date) & (data.index <= end_date)\n",
    "        data = data[mask]\n",
    "    \n",
    "    ratios = data[symbol1] / data[symbol2]\n",
    "    zscore_ratios = zscore(ratios)\n",
    "    \n",
    "    return ratios, zscore_ratios"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def backtest_pairs_sliding(price_matrix, initial_start_date, initial_end_date, window_shifts=10, p_threshold=0.05):\n",
    "    all_trades = []\n",
    "    shift_size = 1\n",
    "    \n",
    "    print(f\"Price Matrix Zeitraum: {price_matrix.index.min()} bis {price_matrix.index.max()}\")\n",
    "    \n",
    "    for shift in range(window_shifts):\n",
    "        current_start = initial_start_date + pd.DateOffset(months=shift*shift_size)\n",
    "        current_end = initial_end_date + pd.DateOffset(months=shift*shift_size)\n",
    "        \n",
    "        print(f\"\\nAnalyse {shift+1}/{window_shifts}\")\n",
    "        print(f\"Cluster-Fenster: {current_start} bis {current_end}\")\n",
    "        \n",
    "        cluster_data = price_matrix[(price_matrix.index >= current_start) & \n",
    "                                  (price_matrix.index <= current_end)].copy()\n",
    "        \n",
    "        if len(cluster_data) == 0:\n",
    "            print(f\"Keine Daten für Fenster {shift+1}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Cluster Daten: {len(cluster_data)} Tage\")\n",
    "            \n",
    "        metrics = calculate_metrics(cluster_data)\n",
    "        X = scale_metrics(metrics)\n",
    "        \n",
    "        ap = AffinityPropagation()\n",
    "        ap.fit(X)\n",
    "        \n",
    "        potential_pairs = []\n",
    "        \n",
    "        for cluster_id in np.unique(ap.labels_):\n",
    "            cluster_mask = ap.labels_ == cluster_id\n",
    "            cluster_symbols = X.index[cluster_mask]\n",
    "            center = X.iloc[ap.cluster_centers_indices_[cluster_id]]\n",
    "            \n",
    "            for i in range(len(cluster_symbols)):\n",
    "                for j in range(i+1, len(cluster_symbols)):\n",
    "                    symbol1, symbol2 = cluster_symbols[i], cluster_symbols[j]\n",
    "                    \n",
    "                    dist1 = np.linalg.norm(X.loc[symbol1] - center)\n",
    "                    dist2 = np.linalg.norm(X.loc[symbol2] - center)\n",
    "                    center_dist = (dist1 + dist2) / 2\n",
    "                    profile_diff = np.linalg.norm(X.loc[symbol1] - X.loc[symbol2])\n",
    "                    \n",
    "                    # Get clean data for cointegration test\n",
    "                    s1 = cluster_data[symbol1].dropna()\n",
    "                    s2 = cluster_data[symbol2].dropna()\n",
    "                    \n",
    "                    if len(s1) > 0 and len(s2) > 0:\n",
    "                        try:\n",
    "                            score, pvalue, _ = coint(s1, s2)\n",
    "                            \n",
    "                            if pvalue < p_threshold:\n",
    "                                potential_pairs.append({\n",
    "                                    'pair': (symbol1, symbol2),\n",
    "                                    'center_dist': center_dist,\n",
    "                                    'profile_diff': profile_diff,\n",
    "                                    'pvalue': pvalue,\n",
    "                                    'cluster': cluster_id\n",
    "                                })\n",
    "                        except:\n",
    "                            continue\n",
    "        \n",
    "        if not potential_pairs:\n",
    "            print(f\"Keine cointegrierten Paare gefunden für Fenster {shift+1}\")\n",
    "            continue\n",
    "            \n",
    "        pairs_df = pd.DataFrame(potential_pairs)\n",
    "        \n",
    "        # Normalize scores\n",
    "        pairs_df['center_dist_norm'] = (pairs_df['center_dist'] - pairs_df['center_dist'].min()) / \\\n",
    "                                     (pairs_df['center_dist'].max() - pairs_df['center_dist'].min())\n",
    "        pairs_df['profile_diff_norm'] = (pairs_df['profile_diff'] - pairs_df['profile_diff'].min()) / \\\n",
    "                                      (pairs_df['profile_diff'].max() - pairs_df['profile_diff'].min())\n",
    "        \n",
    "        pairs_df['combined_score'] = 0.6 * pairs_df['center_dist_norm'] + \\\n",
    "                                   0.4 * pairs_df['profile_diff_norm']\n",
    "        \n",
    "        pairs_df = pairs_df.sort_values('combined_score')\n",
    "        top_pairs = pairs_df['pair'].tolist()[:20]\n",
    "        \n",
    "        print(f\"Gefundene cointegrierte Paare für Fenster {shift+1}: {len(top_pairs)}\")\n",
    "        \n",
    "        trade_start = current_end\n",
    "        trade_end = trade_start + pd.DateOffset(months=1)\n",
    "        \n",
    "        print(f\"Trading-Zeitraum: {trade_start} bis {trade_end}\")\n",
    "        \n",
    "        trading_data = price_matrix[(price_matrix.index > trade_start) & \n",
    "                                  (price_matrix.index <= trade_end)].copy()\n",
    "        \n",
    "        print(f\"Trading Daten: {len(trading_data)} Tage\")\n",
    "        \n",
    "        if len(trading_data) == 0:\n",
    "            print(f\"Warnung: Kein Trading-Zeitraum verfügbar nach {trade_start}\")\n",
    "            continue\n",
    "            \n",
    "        trade_count = 0\n",
    "        for pair in top_pairs:\n",
    "            symbol1, symbol2 = pair\n",
    "            \n",
    "            # Get clean segments for trading\n",
    "            train_s1 = cluster_data[symbol1].dropna()\n",
    "            train_s2 = cluster_data[symbol2].dropna()\n",
    "            test_s1 = trading_data[symbol1].dropna()\n",
    "            test_s2 = trading_data[symbol2].dropna()\n",
    "            \n",
    "            if len(train_s1) == 0 or len(train_s2) == 0 or len(test_s1) == 0 or len(test_s2) == 0:\n",
    "                continue\n",
    "                \n",
    "            trades, _ = trade(\n",
    "                train_s1, \n",
    "                train_s2,\n",
    "                test_s1, \n",
    "                test_s2,\n",
    "                symbol1, symbol2,\n",
    "                cluster_timestamp=current_end\n",
    "            )\n",
    "            \n",
    "            if trades:\n",
    "                trade_count += len(trades)\n",
    "            all_trades.extend(trades)\n",
    "            \n",
    "        print(f\"Trades in diesem Fenster: {trade_count}\")\n",
    "    \n",
    "    trades_df = pd.DataFrame(all_trades)\n",
    "    \n",
    "    if len(trades_df) > 0:\n",
    "        trades_df.to_parquet('../../data/results/Cluster_Bollinger_Sliding.parquet')\n",
    "        \n",
    "        print(\"\\nTrading Zusammenfassung:\")\n",
    "        print(f\"Gesamtanzahl Trades: {len(trades_df)}\")\n",
    "        print(f\"Unique Paare gehandelt: {len(trades_df[['symbol', 'paired_symbol']].drop_duplicates())}\")\n",
    "        print(f\"Zeitraum: {trades_df['entry_date'].min()} bis {trades_df['exit_date'].max()}\")\n",
    "        \n",
    "        unique_timestamps = trades_df['cluster_timestamp'].unique()\n",
    "        timestamp_to_window = {ts: idx for idx, ts in enumerate(sorted(unique_timestamps))}\n",
    "        trades_df['window'] = trades_df['cluster_timestamp'].map(timestamp_to_window)\n",
    "        \n",
    "        print(\"\\nTrades pro Fenster:\")\n",
    "        print(trades_df['window'].value_counts().sort_index())\n",
    "    else:\n",
    "        print(\"Keine Trades generiert!\")\n",
    "        \n",
    "    return trades_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T14:04:52.088111Z",
     "start_time": "2025-02-14T14:03:59.032589Z"
    }
   },
   "source": [
    "trades_df = backtest_pairs_sliding(\n",
    "    price_matrix=price_matrix,\n",
    "    initial_start_date=DATE_CONFIG['TRAIN_START'],\n",
    "    initial_end_date=DATE_CONFIG['TRAIN_END'], \n",
    "    window_shifts=12\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price Matrix Zeitraum: 2021-02-02 00:00:00 bis 2024-12-31 00:00:00\n",
      "\n",
      "Analyse 1/12\n",
      "Cluster-Fenster: 2021-02-02 00:00:00 bis 2024-01-01 00:00:00\n",
      "Cluster Daten: 733 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 1: 20\n",
      "Trading-Zeitraum: 2024-01-01 00:00:00 bis 2024-02-01 00:00:00\n",
      "Trading Daten: 22 Tage\n",
      "Trades in diesem Fenster: 30\n",
      "\n",
      "Analyse 2/12\n",
      "Cluster-Fenster: 2021-03-02 00:00:00 bis 2024-02-01 00:00:00\n",
      "Cluster Daten: 736 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 2: 20\n",
      "Trading-Zeitraum: 2024-02-01 00:00:00 bis 2024-03-01 00:00:00\n",
      "Trading Daten: 20 Tage\n",
      "Trades in diesem Fenster: 36\n",
      "\n",
      "Analyse 3/12\n",
      "Cluster-Fenster: 2021-04-02 00:00:00 bis 2024-03-01 00:00:00\n",
      "Cluster Daten: 733 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 3: 20\n",
      "Trading-Zeitraum: 2024-03-01 00:00:00 bis 2024-04-01 00:00:00\n",
      "Trading Daten: 20 Tage\n",
      "Trades in diesem Fenster: 28\n",
      "\n",
      "Analyse 4/12\n",
      "Cluster-Fenster: 2021-05-02 00:00:00 bis 2024-04-01 00:00:00\n",
      "Cluster Daten: 733 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 4: 20\n",
      "Trading-Zeitraum: 2024-04-01 00:00:00 bis 2024-05-01 00:00:00\n",
      "Trading Daten: 22 Tage\n",
      "Trades in diesem Fenster: 64\n",
      "\n",
      "Analyse 5/12\n",
      "Cluster-Fenster: 2021-06-02 00:00:00 bis 2024-05-01 00:00:00\n",
      "Cluster Daten: 734 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 5: 20\n",
      "Trading-Zeitraum: 2024-05-01 00:00:00 bis 2024-06-01 00:00:00\n",
      "Trading Daten: 21 Tage\n",
      "Trades in diesem Fenster: 46\n",
      "\n",
      "Analyse 6/12\n",
      "Cluster-Fenster: 2021-07-02 00:00:00 bis 2024-06-01 00:00:00\n",
      "Cluster Daten: 733 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 6: 20\n",
      "Trading-Zeitraum: 2024-06-01 00:00:00 bis 2024-07-01 00:00:00\n",
      "Trading Daten: 20 Tage\n",
      "Trades in diesem Fenster: 26\n",
      "\n",
      "Analyse 7/12\n",
      "Cluster-Fenster: 2021-08-02 00:00:00 bis 2024-07-01 00:00:00\n",
      "Cluster Daten: 733 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 7: 20\n",
      "Trading-Zeitraum: 2024-07-01 00:00:00 bis 2024-08-01 00:00:00\n",
      "Trading Daten: 22 Tage\n",
      "Trades in diesem Fenster: 66\n",
      "\n",
      "Analyse 8/12\n",
      "Cluster-Fenster: 2021-09-02 00:00:00 bis 2024-08-01 00:00:00\n",
      "Cluster Daten: 732 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 8: 20\n",
      "Trading-Zeitraum: 2024-08-01 00:00:00 bis 2024-09-01 00:00:00\n",
      "Trading Daten: 21 Tage\n",
      "Trades in diesem Fenster: 20\n",
      "\n",
      "Analyse 9/12\n",
      "Cluster-Fenster: 2021-10-02 00:00:00 bis 2024-09-01 00:00:00\n",
      "Cluster Daten: 732 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 9: 20\n",
      "Trading-Zeitraum: 2024-09-01 00:00:00 bis 2024-10-01 00:00:00\n",
      "Trading Daten: 21 Tage\n",
      "Trades in diesem Fenster: 46\n",
      "\n",
      "Analyse 10/12\n",
      "Cluster-Fenster: 2021-11-02 00:00:00 bis 2024-10-01 00:00:00\n",
      "Cluster Daten: 732 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 10: 20\n",
      "Trading-Zeitraum: 2024-10-01 00:00:00 bis 2024-11-01 00:00:00\n",
      "Trading Daten: 23 Tage\n",
      "Trades in diesem Fenster: 52\n",
      "\n",
      "Analyse 11/12\n",
      "Cluster-Fenster: 2021-12-02 00:00:00 bis 2024-11-01 00:00:00\n",
      "Cluster Daten: 734 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 11: 20\n",
      "Trading-Zeitraum: 2024-11-01 00:00:00 bis 2024-12-01 00:00:00\n",
      "Trading Daten: 19 Tage\n",
      "Trades in diesem Fenster: 30\n",
      "\n",
      "Analyse 12/12\n",
      "Cluster-Fenster: 2022-01-02 00:00:00 bis 2024-12-01 00:00:00\n",
      "Cluster Daten: 732 Tage\n",
      "Gefundene cointegrierte Paare für Fenster 12: 20\n",
      "Trading-Zeitraum: 2024-12-01 00:00:00 bis 2025-01-01 00:00:00\n",
      "Trading Daten: 21 Tage\n",
      "Trades in diesem Fenster: 32\n",
      "\n",
      "Trading Zusammenfassung:\n",
      "Gesamtanzahl Trades: 476\n",
      "Unique Paare gehandelt: 88\n",
      "Zeitraum: 2024-01-02 00:00:00 bis 2024-12-31 00:00:00\n",
      "\n",
      "Trades pro Fenster:\n",
      "window\n",
      "0     30\n",
      "1     36\n",
      "2     28\n",
      "3     64\n",
      "4     46\n",
      "5     26\n",
      "6     66\n",
      "7     20\n",
      "8     46\n",
      "9     52\n",
      "10    30\n",
      "11    32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.analysis.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "    \n",
    "    symbols = price_matrix.columns.tolist()\n",
    "    \n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "    \n",
    "    return price_matrix, symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(cointegrated_pairs):\n",
    "    return cointegrated_pairs\n",
    "\n",
    "def calculate_returns_and_spreads(price_matrix, cointegrated_pairs):\n",
    "    returns = price_matrix.pct_change().dropna()\n",
    "    \n",
    "    pairs = generate_pairs(cointegrated_pairs)\n",
    "    \n",
    "    spreads = pd.DataFrame(index=returns.index)\n",
    "    for s1, s2 in pairs:\n",
    "        spreads[f'{s1}_{s2}_spread'] = returns[s1] - returns[s2]\n",
    "        \n",
    "    return returns, spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(returns, spreads, train_period, test_period, lookback=3):\n",
    "    ml_datasets = {}\n",
    "    \n",
    "    for spread_col in spreads.columns:\n",
    "        sym1, sym2 = spread_col.replace('_spread', '').split('_')\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            f'{sym1}_return': returns[sym1],\n",
    "            f'{sym2}_return': returns[sym2]\n",
    "        })\n",
    "        \n",
    "        for t in range(1, lookback+1):\n",
    "            df[f'{sym1}_return_t-{t}'] = df[f'{sym1}_return'].shift(t)\n",
    "            df[f'{sym2}_return_t-{t}'] = df[f'{sym2}_return'].shift(t)\n",
    "        \n",
    "        spread_next_day = (df[f'{sym1}_return'] - df[f'{sym2}_return']).shift(-1)\n",
    "        df['target'] = np.where(spread_next_day > 0, 1, 0)\n",
    "        \n",
    "        feature_cols = [col for col in df.columns if 't-' in col]\n",
    "        features = df[feature_cols].copy()\n",
    "        \n",
    "        clean_idx = features.dropna().index\n",
    "        features = features.loc[clean_idx]\n",
    "        target = df.loc[clean_idx, 'target']\n",
    "    \n",
    "        \n",
    "        train_mask = (features.index >= train_period['start']) & (features.index < train_period['end'])\n",
    "        test_mask = (features.index >= test_period['start']) & (features.index < test_period['end'])\n",
    "        \n",
    "        ml_datasets[f'{sym1}_{sym2}'] = {\n",
    "            'X_train': features[train_mask],\n",
    "            'X_test': features[test_mask],\n",
    "            'y_train': target[train_mask],\n",
    "            'y_test': target[test_mask]\n",
    "        }\n",
    "        \n",
    "    return ml_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_models(ml_datasets, coint_results):\n",
    "   results = {}\n",
    "   \n",
    "   for pair, data in tqdm(ml_datasets.items(), desc=\"Training models\"):\n",
    "       sym1, sym2 = pair.split('_')\n",
    "       \n",
    "       p_value = coint_results[\n",
    "           ((coint_results['symbol1'] == sym1) & (coint_results['symbol2'] == sym2)) |\n",
    "           ((coint_results['symbol1'] == sym2) & (coint_results['symbol2'] == sym1))\n",
    "       ]['p_value'].iloc[0]\n",
    "       \n",
    "       rf = RandomForestClassifier(\n",
    "           n_estimators=100,\n",
    "           random_state=42\n",
    "       )\n",
    "       \n",
    "       rf.fit(data['X_train'], data['y_train'])\n",
    "       predictions = rf.predict(data['X_test'])\n",
    "       \n",
    "       f1 = f1_score(data['y_test'], predictions)\n",
    "       \n",
    "       weighted_score = 1 * f1 + 0 * (1 - p_value)\n",
    "       \n",
    "       results[pair] = {\n",
    "           'model': rf,\n",
    "           'accuracy': accuracy_score(data['y_test'], predictions),\n",
    "           'precision': precision_score(data['y_test'], predictions),\n",
    "           'recall': recall_score(data['y_test'], predictions),\n",
    "           'f1': f1,\n",
    "           'p_value': p_value,\n",
    "           'weighted_score': weighted_score\n",
    "       }\n",
    "       \n",
    "       feature_importance = pd.DataFrame({\n",
    "           'feature': data['X_train'].columns,\n",
    "           'importance': rf.feature_importances_\n",
    "       }).sort_values('importance', ascending=False)\n",
    "   \n",
    "   return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 2021-02-02 00:00:00 to 2025-01-01 00:00:00\n",
      "Total symbols: 100\n",
      "Total trading days: 1022\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "price_matrix, symbols = load_and_prepare_data(config['data']['raw_data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing pairs: 100%|██████████| 4950/4950 [01:30<00:00, 54.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete!\n",
      "Found 2015 cointegrated pairs\n",
      "Total pairs analyzed: 4950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 2015/2015 [04:30<00:00,  7.46it/s]\n"
     ]
    }
   ],
   "source": [
    "score_matrix, pvalue_matrix, cointegrated_pairs, coint_results = analyze_pairs(price_matrix)\n",
    "\n",
    "# plot_cointegration_heatmap(pvalue_matrix, symbols)\n",
    "\n",
    "returns, spreads = calculate_returns_and_spreads(price_matrix, cointegrated_pairs)\n",
    "\n",
    "train_period = get_training_period()\n",
    "test_period = get_test_period()\n",
    "\n",
    "ml_datasets = prepare_ml_data(returns, spreads, train_period, test_period)\n",
    "\n",
    "model_results = train_evaluate_models(ml_datasets, coint_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pair        f1   p_value  weighted_score  accuracy  precision  \\\n",
      "82    AFIWZ_VHRYD  0.634483  0.002792        0.634483  0.595420   0.601307   \n",
      "1548  MFKBJ_UECLL  0.622951  0.005567        0.622951  0.561069   0.562130   \n",
      "1863  STEPT_VHRYD  0.614334  0.001187        0.614334  0.568702   0.592105   \n",
      "1911  UJLJN_WVCYQ  0.607143  0.000872        0.607143  0.580153   0.611511   \n",
      "533   FAGMW_KQFSM  0.604651  0.007477        0.604651  0.545802   0.532164   \n",
      "28    ADDRO_WGELK  0.602007  0.009007        0.602007  0.545802   0.592105   \n",
      "242   AYWVH_QGMXV  0.601266  0.001515        0.601266  0.519084   0.510753   \n",
      "1495  LURZP_VHRYD  0.600000  0.001386        0.600000  0.572519   0.575342   \n",
      "1372  KXPSW_VAUSI  0.600000  0.011910        0.600000  0.557252   0.557692   \n",
      "38    AFIWZ_FVLHU  0.598007  0.012823        0.598007  0.538168   0.566038   \n",
      "1198  KFZNC_KQFSM  0.598007  0.010082        0.598007  0.538168   0.555556   \n",
      "1743  QIVTR_QJJFG  0.597938  0.008174        0.597938  0.553435   0.587838   \n",
      "1921  UJLJN_ZQLND  0.597865  0.039991        0.597865  0.568702   0.579310   \n",
      "936   GKRHI_VHRYD  0.596364  0.024027        0.596364  0.576336   0.557823   \n",
      "729   GCAOZ_LFOSF  0.594059  0.009635        0.594059  0.530534   0.584416   \n",
      "1151  JAJTW_MXUEC  0.592834  0.001369        0.592834  0.522901   0.548193   \n",
      "1843  SIPNK_UJLJN  0.590747  0.003354        0.590747  0.561069   0.546053   \n",
      "1884  TOUNA_UECLL  0.589831  0.008562        0.589831  0.538168   0.508772   \n",
      "1665  OSEHZ_VHRYD  0.587031  0.001403        0.587031  0.538168   0.544304   \n",
      "521   FAGMW_GDXGP  0.587031  0.002624        0.587031  0.538168   0.511905   \n",
      "\n",
      "        recall  \n",
      "82    0.671533  \n",
      "1548  0.698529  \n",
      "1863  0.638298  \n",
      "1911  0.602837  \n",
      "533   0.700000  \n",
      "28    0.612245  \n",
      "242   0.730769  \n",
      "1495  0.626866  \n",
      "1372  0.649254  \n",
      "38    0.633803  \n",
      "1198  0.647482  \n",
      "1743  0.608392  \n",
      "1921  0.617647  \n",
      "936   0.640625  \n",
      "729   0.604027  \n",
      "1151  0.645390  \n",
      "1843  0.643411  \n",
      "1884  0.701613  \n",
      "1665  0.637037  \n",
      "521   0.688000  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame([\n",
    "   {\n",
    "       'pair': pair,\n",
    "       'accuracy': metrics['accuracy'],\n",
    "       'precision': metrics['precision'],\n",
    "       'recall': metrics['recall'],\n",
    "       'f1': metrics['f1'],\n",
    "       'p_value': metrics['p_value'],\n",
    "       'weighted_score': metrics['weighted_score']\n",
    "   }\n",
    "   for pair, metrics in model_results.items()\n",
    "])\n",
    "\n",
    "top_20 = results_df.sort_values('weighted_score', ascending=False).head(20)\n",
    "print(top_20[['pair', 'f1', 'p_value', 'weighted_score', 'accuracy', 'precision', 'recall']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(series):\n",
    "    return (series - series.mean()) / np.std(series)\n",
    "\n",
    "def calculate_spread(data, symbol1, symbol2, start_date=None, end_date=None):\n",
    "    if start_date:\n",
    "        mask = (data.index >= start_date) & (data.index <= end_date)\n",
    "        data = data[mask]\n",
    "    \n",
    "    ratios = data[symbol1] / data[symbol2]\n",
    "    zscore_ratios = zscore(ratios)\n",
    "    \n",
    "    return ratios, zscore_ratios\n",
    "\n",
    "def trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2, window=50, std_dev=1.5):\n",
    "    model = sm.OLS(S1_train, S2_train)\n",
    "    hedge_ratio = model.fit().params[0]\n",
    "    \n",
    "    spread_test = S1_test - (S2_test * hedge_ratio)\n",
    "    \n",
    "    trades = []\n",
    "    trade_id = 0\n",
    "    position = 0\n",
    "    entry_prices = None\n",
    "    entry_date = None\n",
    "    \n",
    "    prev_spread = None\n",
    "    \n",
    "    for i in range(len(spread_test)):\n",
    "        current_date = spread_test.index[i]\n",
    "        current_spread = spread_test.iloc[i]\n",
    "        \n",
    "        rolling_mean = spread_test.iloc[:i+1].rolling(window=window, center=False).mean().iloc[-1]\n",
    "        rolling_std = spread_test.iloc[:i+1].rolling(window=window, center=False).std().iloc[-1]\n",
    "        \n",
    "        upper_band = rolling_mean + (rolling_std * std_dev)\n",
    "        lower_band = rolling_mean - (rolling_std * std_dev)\n",
    "        \n",
    "        if prev_spread is not None:\n",
    "            if position == 0:\n",
    "                if prev_spread > lower_band and current_spread < lower_band:\n",
    "                    entry_date = current_date\n",
    "                    entry_prices = {\n",
    "                        symbol1: {\"price\": S1_test.iloc[i], \"type\": \"long\"},\n",
    "                        symbol2: {\"price\": S2_test.iloc[i], \"type\": \"short\"}\n",
    "                    }\n",
    "                    position = 1\n",
    "                    \n",
    "                elif prev_spread < upper_band and current_spread > upper_band:\n",
    "                    entry_date = current_date\n",
    "                    entry_prices = {\n",
    "                        symbol1: {\"price\": S1_test.iloc[i], \"type\": \"short\"},\n",
    "                        symbol2: {\"price\": S2_test.iloc[i], \"type\": \"long\"}\n",
    "                    }\n",
    "                    position = -1\n",
    "                    \n",
    "            elif ((position == 1 and prev_spread < upper_band and current_spread > upper_band) or \n",
    "                  (position == -1 and prev_spread > lower_band and current_spread < lower_band)):\n",
    "                \n",
    "                for symbol in [symbol1, symbol2]:\n",
    "                    trades.append({\n",
    "                        'trade_id': trade_id,\n",
    "                        'symbol': symbol,\n",
    "                        'entry_date': entry_date,\n",
    "                        'entry_price': entry_prices[symbol][\"price\"],\n",
    "                        'exit_date': current_date,\n",
    "                        'exit_price': S1_test.iloc[i] if symbol == symbol1 else S2_test.iloc[i],\n",
    "                        'position_type': entry_prices[symbol][\"type\"],\n",
    "                        'paired_symbol': symbol2 if symbol == symbol1 else symbol1,\n",
    "                        'exit_type': 'target'\n",
    "                    })\n",
    "                position = 0\n",
    "                trade_id += 1\n",
    "                \n",
    "        prev_spread = current_spread\n",
    "            \n",
    "    return trades\n",
    "\n",
    "def backtest_pairs(price_matrix, pairs, train_end_date):\n",
    "    all_trades = []\n",
    "    \n",
    "    for symbol1, symbol2 in pairs:\n",
    "        training_mask = price_matrix.index < train_end_date\n",
    "        \n",
    "        S1_train = price_matrix[symbol1][training_mask]\n",
    "        S2_train = price_matrix[symbol2][training_mask]\n",
    "        S1_test = price_matrix[symbol1][~training_mask]\n",
    "        S2_test = price_matrix[symbol2][~training_mask]\n",
    "        \n",
    "        pair_trades = trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2)\n",
    "        all_trades.extend(pair_trades)\n",
    "    \n",
    "    trades_df = pd.DataFrame(all_trades)\n",
    "    trades_df.to_parquet('../../data/results/Random-Forest_Bollinger.parquet')\n",
    "    \n",
    "    return trades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtest Results:\n",
      "Total number of trades: 86\n"
     ]
    }
   ],
   "source": [
    "top_pairs = [tuple(pair.split('_')) for pair in top_20['pair']]\n",
    "\n",
    "trades_df = backtest_pairs(price_matrix, top_pairs, train_period['end'])\n",
    "\n",
    "print(\"\\nBacktest Results:\")\n",
    "print(f\"Total number of trades: {len(trades_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

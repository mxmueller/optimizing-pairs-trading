{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcointegration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.analysis.cointegration import find_cointegrated_pairs, analyze_pairs, plot_cointegration_heatmap\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATE_CONFIG = {\n",
    "    'TRAIN_START': pd.Timestamp('2021-02-02'),\n",
    "    'TRAIN_END': pd.Timestamp('2024-01-01'),\n",
    "    'TEST_END': pd.Timestamp('2025-01-01'),\n",
    "    'TRADING_DAYS_PER_YEAR': 252  \n",
    "}\n",
    "\n",
    "def get_training_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_START'],\n",
    "        'end': DATE_CONFIG['TRAIN_END']\n",
    "    }\n",
    "\n",
    "def get_test_period():\n",
    "    return {\n",
    "        'start': DATE_CONFIG['TRAIN_END'],\n",
    "        'end': DATE_CONFIG['TEST_END']\n",
    "    }\n",
    "\n",
    "def get_training_days():\n",
    "    years = (DATE_CONFIG['TRAIN_END'] - DATE_CONFIG['TRAIN_START']).days / 365\n",
    "    return int(years * DATE_CONFIG['TRADING_DAYS_PER_YEAR'])\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    mask = (df['date'] >= DATE_CONFIG['TRAIN_START']) & \\\n",
    "           (df['date'] <= DATE_CONFIG['TEST_END'])\n",
    "    df = df[mask]\n",
    "    \n",
    "    price_matrix = df.pivot(index='date', columns='symbol', values='close')\n",
    "    \n",
    "    symbols = price_matrix.columns.tolist()\n",
    "    \n",
    "    print(f\"Loaded data from {DATE_CONFIG['TRAIN_START']} to {DATE_CONFIG['TEST_END']}\")\n",
    "    print(f\"Total symbols: {len(symbols)}\")\n",
    "    print(f\"Total trading days: {len(price_matrix)}\")\n",
    "    \n",
    "    return price_matrix, symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(cointegrated_pairs):\n",
    "    return cointegrated_pairs\n",
    "\n",
    "def calculate_returns_and_spreads(price_matrix, cointegrated_pairs):\n",
    "    returns = price_matrix.pct_change().dropna()\n",
    "    \n",
    "    pairs = generate_pairs(cointegrated_pairs)\n",
    "    \n",
    "    spreads = pd.DataFrame(index=returns.index)\n",
    "    for s1, s2 in pairs:\n",
    "        spreads[f'{s1}_{s2}_spread'] = returns[s1] - returns[s2]\n",
    "        \n",
    "    return returns, spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(returns, spreads, train_period, test_period, lookback=3):\n",
    "    ml_datasets = {}\n",
    "    \n",
    "    for spread_col in spreads.columns:\n",
    "        sym1, sym2 = spread_col.replace('_spread', '').split('_')\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            f'{sym1}_return': returns[sym1],\n",
    "            f'{sym2}_return': returns[sym2]\n",
    "        })\n",
    "        \n",
    "        for t in range(1, lookback+1):\n",
    "            df[f'{sym1}_return_t-{t}'] = df[f'{sym1}_return'].shift(t)\n",
    "            df[f'{sym2}_return_t-{t}'] = df[f'{sym2}_return'].shift(t)\n",
    "        \n",
    "        spread_next_day = (df[f'{sym1}_return'] - df[f'{sym2}_return']).shift(-1)\n",
    "        df['target'] = np.where(spread_next_day > 0, 1, 0)\n",
    "        \n",
    "        feature_cols = [col for col in df.columns if 't-' in col]\n",
    "        features = df[feature_cols].copy()\n",
    "        \n",
    "        clean_idx = features.dropna().index\n",
    "        features = features.loc[clean_idx]\n",
    "        target = df.loc[clean_idx, 'target']\n",
    "    \n",
    "        \n",
    "        train_mask = (features.index >= train_period['start']) & (features.index < train_period['end'])\n",
    "        test_mask = (features.index >= test_period['start']) & (features.index < test_period['end'])\n",
    "        \n",
    "        ml_datasets[f'{sym1}_{sym2}'] = {\n",
    "            'X_train': features[train_mask],\n",
    "            'X_test': features[test_mask],\n",
    "            'y_train': target[train_mask],\n",
    "            'y_test': target[test_mask]\n",
    "        }\n",
    "        \n",
    "    return ml_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_models(ml_datasets, coint_results):\n",
    "   results = {}\n",
    "   \n",
    "   for pair, data in tqdm(ml_datasets.items(), desc=\"Training models\"):\n",
    "       sym1, sym2 = pair.split('_')\n",
    "       \n",
    "       p_value = coint_results[\n",
    "           ((coint_results['symbol1'] == sym1) & (coint_results['symbol2'] == sym2)) |\n",
    "           ((coint_results['symbol1'] == sym2) & (coint_results['symbol2'] == sym1))\n",
    "       ]['p_value'].iloc[0]\n",
    "       \n",
    "       rf = RandomForestClassifier(\n",
    "           n_estimators=100,\n",
    "           random_state=42\n",
    "       )\n",
    "       \n",
    "       rf.fit(data['X_train'], data['y_train'])\n",
    "       predictions = rf.predict(data['X_test'])\n",
    "       \n",
    "       f1 = f1_score(data['y_test'], predictions)\n",
    "       \n",
    "       weighted_score = 0.6 * f1 + 0.4 * (1 - p_value)\n",
    "       \n",
    "       results[pair] = {\n",
    "           'model': rf,\n",
    "           'accuracy': accuracy_score(data['y_test'], predictions),\n",
    "           'precision': precision_score(data['y_test'], predictions),\n",
    "           'recall': recall_score(data['y_test'], predictions),\n",
    "           'f1': f1,\n",
    "           'p_value': p_value,\n",
    "           'weighted_score': weighted_score\n",
    "       }\n",
    "       \n",
    "       feature_importance = pd.DataFrame({\n",
    "           'feature': data['X_train'].columns,\n",
    "           'importance': rf.feature_importances_\n",
    "       }).sort_values('importance', ascending=False)\n",
    "   \n",
    "   return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "\n",
    "with open(\"../config.toml\", \"r\") as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "price_matrix, symbols = load_and_prepare_data(config['data']['raw_data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 2021-02-02 00:00:00 to 2025-01-01 00:00:00\n",
      "Total symbols: 94\n",
      "Total trading days: 985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing pairs: 100%|██████████| 4371/4371 [00:52<00:00, 83.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete!\n",
      "Found 45 cointegrated pairs\n",
      "Total pairs analyzed: 4371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 45/45 [00:05<00:00,  8.07it/s]\n"
     ]
    }
   ],
   "source": [
    "score_matrix, pvalue_matrix, cointegrated_pairs, coint_results = analyze_pairs(price_matrix)\n",
    "\n",
    "# plot_cointegration_heatmap(pvalue_matrix, symbols)\n",
    "\n",
    "returns, spreads = calculate_returns_and_spreads(price_matrix, cointegrated_pairs)\n",
    "\n",
    "train_period = get_training_period()\n",
    "test_period = get_test_period()\n",
    "\n",
    "ml_datasets = prepare_ml_data(returns, spreads, train_period, test_period)\n",
    "\n",
    "model_results = train_evaluate_models(ml_datasets, coint_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         pair        f1   p_value  weighted_score  accuracy  precision  \\\n",
      "30   ISRG_TTD  0.559387  0.003670        0.734164  0.543651   0.536765   \n",
      "32   KHC_MSFT  0.549020  0.009724        0.725522  0.543651   0.522388   \n",
      "4    ADSK_HON  0.547445  0.009983        0.724474  0.507937   0.576923   \n",
      "31   KDP_MNST  0.541176  0.000629        0.724454  0.535714   0.547619   \n",
      "27     EA_TTD  0.544715  0.006853        0.724088  0.555556   0.531746   \n",
      "5   AMAT_NXPI  0.539007  0.000091        0.723368  0.484127   0.503311   \n",
      "15   CSX_TTWO  0.539062  0.009163        0.719772  0.531746   0.543307   \n",
      "13    CSX_MDB  0.529183  0.009500        0.713710  0.519841   0.544000   \n",
      "28   GOOG_TTD  0.526718  0.007768        0.712923  0.507937   0.489362   \n",
      "37  LRCX_NXPI  0.521739  0.001418        0.712476  0.476190   0.483221   \n",
      "25    EA_NFLX  0.515873  0.002453        0.708543  0.515873   0.442177   \n",
      "8   CDNS_PCAR  0.517110  0.006162        0.707802  0.496032   0.496350   \n",
      "38   MAR_PANW  0.508475  0.000692        0.704808  0.539683   0.566038   \n",
      "39   MCHP_XEL  0.508197  0.005534        0.702704  0.523810   0.525424   \n",
      "17     CSX_ZS  0.504425  0.006336        0.700120  0.555556   0.518182   \n",
      "9     CDW_XEL  0.498141  0.001578        0.698254  0.464286   0.458904   \n",
      "29  GOOGL_TTD  0.500000  0.007324        0.697070  0.492063   0.477612   \n",
      "20     EA_HON  0.492188  0.004449        0.693533  0.484127   0.500000   \n",
      "44   TTD_VRSK  0.494024  0.007306        0.693492  0.496032   0.521008   \n",
      "3    ADI_PANW  0.493827  0.009833        0.692363  0.511905   0.495868   \n",
      "\n",
      "      recall  \n",
      "30  0.584000  \n",
      "32  0.578512  \n",
      "4   0.520833  \n",
      "31  0.534884  \n",
      "27  0.558333  \n",
      "5   0.580153  \n",
      "15  0.534884  \n",
      "13  0.515152  \n",
      "28  0.570248  \n",
      "37  0.566929  \n",
      "25  0.619048  \n",
      "8   0.539683  \n",
      "38  0.461538  \n",
      "39  0.492063  \n",
      "17  0.491379  \n",
      "9   0.544715  \n",
      "29  0.524590  \n",
      "20  0.484615  \n",
      "44  0.469697  \n",
      "3   0.491803  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame([\n",
    "   {\n",
    "       'pair': pair,\n",
    "       'accuracy': metrics['accuracy'],\n",
    "       'precision': metrics['precision'],\n",
    "       'recall': metrics['recall'],\n",
    "       'f1': metrics['f1'],\n",
    "       'p_value': metrics['p_value'],\n",
    "       'weighted_score': metrics['weighted_score']\n",
    "   }\n",
    "   for pair, metrics in model_results.items()\n",
    "])\n",
    "\n",
    "top_20 = results_df.sort_values('weighted_score', ascending=False).head(20)\n",
    "print(top_20[['pair', 'f1', 'p_value', 'weighted_score', 'accuracy', 'precision', 'recall']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(series):\n",
    "    return (series - series.mean()) / np.std(series)\n",
    "\n",
    "def calculate_spread(data, symbol1, symbol2, start_date=None, end_date=None):\n",
    "    if start_date:\n",
    "        mask = (data.index >= start_date) & (data.index <= end_date)\n",
    "        data = data[mask]\n",
    "    \n",
    "    ratios = data[symbol1] / data[symbol2]\n",
    "    zscore_ratios = zscore(ratios)\n",
    "    \n",
    "    return ratios, zscore_ratios\n",
    "\n",
    "def trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2, window=50, std_dev=1.5):\n",
    "    model = sm.OLS(S1_train, S2_train)\n",
    "    hedge_ratio = model.fit().params[0]\n",
    "    \n",
    "    spread_test = S1_test - (S2_test * hedge_ratio)\n",
    "    \n",
    "    trades = []\n",
    "    trade_id = 0\n",
    "    position = 0\n",
    "    entry_prices = None\n",
    "    entry_date = None\n",
    "    \n",
    "    prev_spread = None\n",
    "    \n",
    "    for i in range(len(spread_test)):\n",
    "        current_date = spread_test.index[i]\n",
    "        current_spread = spread_test.iloc[i]\n",
    "        \n",
    "        rolling_mean = spread_test.iloc[:i+1].rolling(window=window, center=False).mean().iloc[-1]\n",
    "        rolling_std = spread_test.iloc[:i+1].rolling(window=window, center=False).std().iloc[-1]\n",
    "        \n",
    "        upper_band = rolling_mean + (rolling_std * std_dev)\n",
    "        lower_band = rolling_mean - (rolling_std * std_dev)\n",
    "        \n",
    "        if prev_spread is not None:\n",
    "            if position == 0:\n",
    "                if prev_spread > lower_band and current_spread < lower_band:\n",
    "                    entry_date = current_date\n",
    "                    entry_prices = {\n",
    "                        symbol1: {\"price\": S1_test.iloc[i], \"type\": \"long\"},\n",
    "                        symbol2: {\"price\": S2_test.iloc[i], \"type\": \"short\"}\n",
    "                    }\n",
    "                    position = 1\n",
    "                    \n",
    "                elif prev_spread < upper_band and current_spread > upper_band:\n",
    "                    entry_date = current_date\n",
    "                    entry_prices = {\n",
    "                        symbol1: {\"price\": S1_test.iloc[i], \"type\": \"short\"},\n",
    "                        symbol2: {\"price\": S2_test.iloc[i], \"type\": \"long\"}\n",
    "                    }\n",
    "                    position = -1\n",
    "                    \n",
    "            elif ((position == 1 and prev_spread < upper_band and current_spread > upper_band) or \n",
    "                  (position == -1 and prev_spread > lower_band and current_spread < lower_band)):\n",
    "                \n",
    "                for symbol in [symbol1, symbol2]:\n",
    "                    trades.append({\n",
    "                        'trade_id': trade_id,\n",
    "                        'symbol': symbol,\n",
    "                        'entry_date': entry_date,\n",
    "                        'entry_price': entry_prices[symbol][\"price\"],\n",
    "                        'exit_date': current_date,\n",
    "                        'exit_price': S1_test.iloc[i] if symbol == symbol1 else S2_test.iloc[i],\n",
    "                        'position_type': entry_prices[symbol][\"type\"],\n",
    "                        'paired_symbol': symbol2 if symbol == symbol1 else symbol1,\n",
    "                        'exit_type': 'target'\n",
    "                    })\n",
    "                position = 0\n",
    "                trade_id += 1\n",
    "                \n",
    "        prev_spread = current_spread\n",
    "            \n",
    "    return trades\n",
    "\n",
    "def backtest_pairs(price_matrix, pairs, train_end_date):\n",
    "    all_trades = []\n",
    "    \n",
    "    for symbol1, symbol2 in pairs:\n",
    "        training_mask = price_matrix.index < train_end_date\n",
    "        \n",
    "        S1_train = price_matrix[symbol1][training_mask]\n",
    "        S2_train = price_matrix[symbol2][training_mask]\n",
    "        S1_test = price_matrix[symbol1][~training_mask]\n",
    "        S2_test = price_matrix[symbol2][~training_mask]\n",
    "        \n",
    "        pair_trades = trade(S1_train, S2_train, S1_test, S2_test, symbol1, symbol2)\n",
    "        all_trades.extend(pair_trades)\n",
    "    \n",
    "    trades_df = pd.DataFrame(all_trades)\n",
    "    trades_df.to_parquet('../../data/results/Random-Forest_Bollinger.parquet')\n",
    "    \n",
    "    return trades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtest Results:\n",
      "Total number of trades: 294\n"
     ]
    }
   ],
   "source": [
    "top_pairs = [tuple(pair.split('_')) for pair in top_20['pair']]\n",
    "\n",
    "trades_df = backtest_pairs(price_matrix, top_pairs, train_period['end'])\n",
    "\n",
    "print(\"\\nBacktest Results:\")\n",
    "print(f\"Total number of trades: {len(trades_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
